[
  {
    "objectID": "talk/2021-05-04-data-viz-accessibility/index.html",
    "href": "talk/2021-05-04-data-viz-accessibility/index.html",
    "title": "Revealing Room for Improvement in Accessibility within a Social Media Data Visualization Learning Community",
    "section": "",
    "text": "Presented with Liz Hare, PhD from Dog Genetics, LLC"
  },
  {
    "objectID": "talk/2021-05-04-data-viz-accessibility/index.html#abstract",
    "href": "talk/2021-05-04-data-viz-accessibility/index.html#abstract",
    "title": "Revealing Room for Improvement in Accessibility within a Social Media Data Visualization Learning Community",
    "section": "Abstract",
    "text": "Abstract\nWe all aim to use data to tell a compelling story, and many of us enjoy sharing how we got there by open-sourcing our code, but we don’t always share our story with everyone. Even kind, supportive, and open communities like the #TidyTuesday R learning community on Twitter has a ways to go before the content shared can be accessible to everyone.Lived experiences of blind R users tell us that most data visualizations shared for TidyTuesday are inaccessible to screen reading technology because they lack alternative text (i.e. alt text) descriptions. Our goal was to bring this hidden lack of accessibility to the surface by examining the alternative text accompanying data visualizations shared as part of the TidyTuesday social project.We scraped the alternative text from 6,443 TidyTuesday images posted on Twitter between April 2, 2018 and January 31, 2021. The first image attached to each tweet was considered the primary image and was scraped for alternative text. Manual web inspection revealed the CSS class and HTML element corresponding to the primary image, as well as the attribute containing the alternative text. We used this information and the ROpenSci {RSelenium} package to scrape the alternative text. Our preliminary analysis found that only 2.4% of the images contained a text description entered by the tweet author compared to 84% which were described by default as ‘Image.’This small group of intentional alternative text descriptions had a median word count of 18 (range: 1-170), and a median character count of 83 (range: 8-788). As a reference point, Twitter allows 240 characters in a single tweet and 1,000 characters for image descriptions. This analysis was made possible thanks to a dataset of historical TidyTuesday tweet data collected using the ROpenSci {rtweet} package, and openly available in the TidyTuesday GitHub repository.We will present during Session 0 on May 4, 2021: Crowdcast Link"
  },
  {
    "objectID": "talk/2021-03-23-amia-informatics-summit/index.html",
    "href": "talk/2021-03-23-amia-informatics-summit/index.html",
    "title": "The Impact of Sickle Cell Status on Adverse Delivery Outcomes Using Electronic Health Record Data",
    "section": "",
    "text": "This study investigates the effect of sickle cell trait and sickle cell disease, on adverse pregnancy outcomes at Penn Medicine. The risk of a Cesarean section (C-section), preterm birth, stillbirth, pain crisis, blood transfusion, and hemorrhage during delivery were all found to be significantly correlated with race/ethnicity, sickle cell disease, the number of pain crises before delivery, and the number of blood transfusions before delivery. Multiple birth was also found to significantly increase the risk of these same outcomes.\n\nOral Presentations S17: March 23, 2021 11:30am-1pm ET\n\n\n\nBanner for the 2021 AMIA Informatics Summit (March 22-25)"
  },
  {
    "objectID": "talk/2021-03-23-amia-informatics-summit/index.html#abstract",
    "href": "talk/2021-03-23-amia-informatics-summit/index.html#abstract",
    "title": "The Impact of Sickle Cell Status on Adverse Delivery Outcomes Using Electronic Health Record Data",
    "section": "",
    "text": "This study investigates the effect of sickle cell trait and sickle cell disease, on adverse pregnancy outcomes at Penn Medicine. The risk of a Cesarean section (C-section), preterm birth, stillbirth, pain crisis, blood transfusion, and hemorrhage during delivery were all found to be significantly correlated with race/ethnicity, sickle cell disease, the number of pain crises before delivery, and the number of blood transfusions before delivery. Multiple birth was also found to significantly increase the risk of these same outcomes.\n\nOral Presentations S17: March 23, 2021 11:30am-1pm ET\n\n\n\nBanner for the 2021 AMIA Informatics Summit (March 22-25)"
  },
  {
    "objectID": "talk/2021-07-07-presentable-user2021/index.html",
    "href": "talk/2021-07-07-presentable-user2021/index.html",
    "title": "Professional, Polished, Presentable: Making Great Slides with xaringan",
    "section": "",
    "text": "Co-instructed with Garrick Aden-Buie"
  },
  {
    "objectID": "talk/2021-07-07-presentable-user2021/index.html#abstract",
    "href": "talk/2021-07-07-presentable-user2021/index.html#abstract",
    "title": "Professional, Polished, Presentable: Making Great Slides with xaringan",
    "section": "Abstract",
    "text": "Abstract\nThe xaringan package brings professional, impressive, and visually appealing slides to the powerful R Markdown ecosystem. Through our hands-on tutorial, you will learn how to design highly effective slides that support presentations for teaching and reporting alike. Over three hours, you will learn how to create an accessible baseline design that matches your institution or organization’s style guide. Together we’ll explore the basics of CSS—the design language of the internet—and how we can leverage CSS to produce elegant slides for effective communication."
  },
  {
    "objectID": "talk/2021-03-16-xaringan-deploy-demo/index.html",
    "href": "talk/2021-03-16-xaringan-deploy-demo/index.html",
    "title": "Writing Presentations in R",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "talk/2020-12-17-introduccion-xaringan/index.html",
    "href": "talk/2020-12-17-introduccion-xaringan/index.html",
    "title": "Introducción al Paquete xaringan",
    "section": "",
    "text": "El taller tiene por objetivo introducir a las participantes al paquete xaringan de R como una herramienta para crear diapositivas de presentación impresionantes que se pueden implementar en la web para compartir fácilmente.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talk/2020-11-03-xaringan-basics-and-beyond/index.html",
    "href": "talk/2020-11-03-xaringan-basics-and-beyond/index.html",
    "title": "Sharing Your Work with xaringan: The Basics and Beyond",
    "section": "",
    "text": "This four-hour hands-on workshop will be a gentle introduction to the xaringan package as a tool to create impressive presentation slides that can be deployed to the web for easy sharing.\nDay 1 (Nov. 3, 3-5pm BST) will cover the nuts and bolts of creating presentation slides using xaringan and deploying them in HTML format for easy sharing with others.\nDay 2 (Nov. 5, 3-5pm BST) will cover how to take your slides to the next level with the xaringanExtra package and how to customize slides with CSS.\nThis workshop is designed for R users already familiar with R Markdown and GitHub."
  },
  {
    "objectID": "talk/2020-11-03-xaringan-basics-and-beyond/index.html#description",
    "href": "talk/2020-11-03-xaringan-basics-and-beyond/index.html#description",
    "title": "Sharing Your Work with xaringan: The Basics and Beyond",
    "section": "",
    "text": "This four-hour hands-on workshop will be a gentle introduction to the xaringan package as a tool to create impressive presentation slides that can be deployed to the web for easy sharing.\nDay 1 (Nov. 3, 3-5pm BST) will cover the nuts and bolts of creating presentation slides using xaringan and deploying them in HTML format for easy sharing with others.\nDay 2 (Nov. 5, 3-5pm BST) will cover how to take your slides to the next level with the xaringanExtra package and how to customize slides with CSS.\nThis workshop is designed for R users already familiar with R Markdown and GitHub."
  },
  {
    "objectID": "talk/2023-10-11-epis4equity/index.html",
    "href": "talk/2023-10-11-epis4equity/index.html",
    "title": "Place-based Interventions to Promote Structural Change & Equity",
    "section": "",
    "text": "Deeply Rooted\nIGNITE: A Randomized Controlled Trial of Concentrated Investment in Black Neighborhoods to Address Structural Racism as a Fundamental Cause of Poor Health\nSPARROw\nNature and Wellbeing Project"
  },
  {
    "objectID": "talk/2023-10-11-epis4equity/index.html#projects-highlighted",
    "href": "talk/2023-10-11-epis4equity/index.html#projects-highlighted",
    "title": "Place-based Interventions to Promote Structural Change & Equity",
    "section": "",
    "text": "Deeply Rooted\nIGNITE: A Randomized Controlled Trial of Concentrated Investment in Black Neighborhoods to Address Structural Racism as a Fundamental Cause of Poor Health\nSPARROw\nNature and Wellbeing Project"
  },
  {
    "objectID": "talk/2022-09-29-ccd-sips/index.html",
    "href": "talk/2022-09-29-ccd-sips/index.html",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "",
    "text": "A gentle introduction to web-scraping, geocoding and map-making in an end-to-end analysis. The workshop will cover how to scrape the bar and restaurant data from the Center City Sips website and convert the addresses to latitude and longitude, as well as how to plot the geographic data onto a map and customize the visualization with {leaflet}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n  \n\n\n\n\n Back to top"
  },
  {
    "objectID": "project/2021-07-07-presentable-user2021/index.html",
    "href": "project/2021-07-07-presentable-user2021/index.html",
    "title": "Professional, Polished, Presentable",
    "section": "",
    "text": "The xaringan package brings professional, impressive, and visually appealing slides to the powerful R Markdown ecosystem. Through our hands-on tutorial, you will learn how to design highly effective slides that support presentations for teaching and reporting alike. Over three hours, you will learn how to create an accessible baseline design that matches your institution or organization’s style guide. Together we’ll explore the basics of CSS—the design language of the internet—and how we can leverage CSS to produce elegant slides for effective communication."
  },
  {
    "objectID": "project/2021-07-07-presentable-user2021/index.html#abstract",
    "href": "project/2021-07-07-presentable-user2021/index.html#abstract",
    "title": "Professional, Polished, Presentable",
    "section": "",
    "text": "The xaringan package brings professional, impressive, and visually appealing slides to the powerful R Markdown ecosystem. Through our hands-on tutorial, you will learn how to design highly effective slides that support presentations for teaching and reporting alike. Over three hours, you will learn how to create an accessible baseline design that matches your institution or organization’s style guide. Together we’ll explore the basics of CSS—the design language of the internet—and how we can leverage CSS to produce elegant slides for effective communication."
  },
  {
    "objectID": "project/2020-03-25-data-hack-opioid/index.html",
    "href": "project/2020-03-25-data-hack-opioid/index.html",
    "title": "Visualizing the Inside Journey of Recovery from Opioid Use Disorder",
    "section": "",
    "text": "Referral process pipeline (PDF)"
  },
  {
    "objectID": "project/2020-03-25-data-hack-opioid/index.html#data-hack-hosts",
    "href": "project/2020-03-25-data-hack-opioid/index.html#data-hack-hosts",
    "title": "Visualizing the Inside Journey of Recovery from Opioid Use Disorder",
    "section": "Data Hack hosts",
    "text": "Data Hack hosts\n\nCode for Philly\nDataPhilly\nR-Ladies Philly\nPhilly Data Jawn\nCity of Philadelphia"
  },
  {
    "objectID": "project/2020-03-25-data-hack-opioid/index.html#partner-organizationsinterviewees",
    "href": "project/2020-03-25-data-hack-opioid/index.html#partner-organizationsinterviewees",
    "title": "Visualizing the Inside Journey of Recovery from Opioid Use Disorder",
    "section": "Partner organizations/interviewees",
    "text": "Partner organizations/interviewees\n\nPrevention Point Prevention: drop-in, medical, mobile clinic, and re-entry case managers.\nHealth Federation of Philadelphia: Opioid Response Program Coordinator and intern.\nPenn Medicine Center for Opioid Recovery and Engagement: Certified recovery specialists, emergency medicine physician, and individual in recovery."
  },
  {
    "objectID": "project/2020-12-09-xaringan/index.html",
    "href": "project/2020-12-09-xaringan/index.html",
    "title": "nhsr CSS Theme for xaringan",
    "section": "",
    "text": "In preparation for a xaringan workshop I created for the NHS-R 2020 Conference, I designed a custom CSS theme with input from the NHS-R Community that followed the NHS identity guidelines. This theme was also contributed to the nhsrtheme package developed by Tom Jemmett."
  },
  {
    "objectID": "project/2020-12-09-xaringan/index.html#theme",
    "href": "project/2020-12-09-xaringan/index.html#theme",
    "title": "nhsr CSS Theme for xaringan",
    "section": "",
    "text": "In preparation for a xaringan workshop I created for the NHS-R 2020 Conference, I designed a custom CSS theme with input from the NHS-R Community that followed the NHS identity guidelines. This theme was also contributed to the nhsrtheme package developed by Tom Jemmett."
  },
  {
    "objectID": "project/2021-05-04-tidy-tuesday-alt-text/index.html",
    "href": "project/2021-05-04-tidy-tuesday-alt-text/index.html",
    "title": "TidyTuesdayAltText",
    "section": "",
    "text": "The original data were collected and made available by Tom Mock (@thomas_mock) using {rtweet}. These data are available in the TidyTuesday repository.\nThese tweets were processed and scraped for alternative text by Silvia Canelón (@spcanelon)\n\nData were filtered to remove tweets without attached media (e.g. images)\nData were supplemented with reply tweets collected using {rtweet}. This was done to identify whether the original tweet or a reply tweet contained an external link (e.g. data source, repository with source code)\nAlternative (alt) text was scraped from tweet images using {RSelenium}. The first image attached to each tweet was considered the primary image and only the primary image from each tweet was scraped for alternative text. The following attributes were used to build the scraper:\n\n\nCSS selector: .css-1dbjc4n.r-1p0dtai.r-1mlwlqe.r-1d2f490.r-11wrixw\nElement attribute: aria-label\n\n\n\n\nFigure 1: Example of web inspection being used to identify the CSS selector utilized for alt-text web scraping\n\n\nThis data package does not include data that could directly identify the tweet author in order to respect any author’s decision to delete a tweet or make their account private after the data was originally collected.1\nTo obtain the tweet text, author screen name, and many other tweet attributes, you can “rehydrate” the TweetIds (or “status” ids)2) using the {rtweet} package.3"
  },
  {
    "objectID": "project/2021-05-04-tidy-tuesday-alt-text/index.html#about-the-data",
    "href": "project/2021-05-04-tidy-tuesday-alt-text/index.html#about-the-data",
    "title": "TidyTuesdayAltText",
    "section": "",
    "text": "The original data were collected and made available by Tom Mock (@thomas_mock) using {rtweet}. These data are available in the TidyTuesday repository.\nThese tweets were processed and scraped for alternative text by Silvia Canelón (@spcanelon)\n\nData were filtered to remove tweets without attached media (e.g. images)\nData were supplemented with reply tweets collected using {rtweet}. This was done to identify whether the original tweet or a reply tweet contained an external link (e.g. data source, repository with source code)\nAlternative (alt) text was scraped from tweet images using {RSelenium}. The first image attached to each tweet was considered the primary image and only the primary image from each tweet was scraped for alternative text. The following attributes were used to build the scraper:\n\n\nCSS selector: .css-1dbjc4n.r-1p0dtai.r-1mlwlqe.r-1d2f490.r-11wrixw\nElement attribute: aria-label\n\n\n\n\nFigure 1: Example of web inspection being used to identify the CSS selector utilized for alt-text web scraping\n\n\nThis data package does not include data that could directly identify the tweet author in order to respect any author’s decision to delete a tweet or make their account private after the data was originally collected.1\nTo obtain the tweet text, author screen name, and many other tweet attributes, you can “rehydrate” the TweetIds (or “status” ids)2) using the {rtweet} package.3"
  },
  {
    "objectID": "project/2021-05-04-tidy-tuesday-alt-text/index.html#tidytuesday-databases-on-notion",
    "href": "project/2021-05-04-tidy-tuesday-alt-text/index.html#tidytuesday-databases-on-notion",
    "title": "TidyTuesdayAltText",
    "section": "TidyTuesday databases on Notion",
    "text": "TidyTuesday databases on Notion\nI use the data available in the TidyTuesday repository to populate some searchable TidyTuesday databases at tiny.cc/notion-dataviz with data visualizations tagged by the dataset of the week, hashtags, mentions, etc.\n\n\n\nFigure 2: Screenshot of the 2021 TidyTuesday database on Notion, taken on June 1, 2021\n\n\n\n\n\nFigure 3: Screenshot of the tweet sharing the TidyTuesday database on Notion"
  },
  {
    "objectID": "project/2021-05-04-tidy-tuesday-alt-text/index.html#footnotes",
    "href": "project/2021-05-04-tidy-tuesday-alt-text/index.html#footnotes",
    "title": "TidyTuesdayAltText",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDeveloper Policy – Twitter Developers | Twitter Developer↩︎\nTweet object | Twitter Developer↩︎\nGet tweets data for given statuses (status IDs). — lookup_tweets • rOpenSci: rtweet↩︎"
  },
  {
    "objectID": "blog/2022-05-31-ccd-sips/index.html",
    "href": "blog/2022-05-31-ccd-sips/index.html",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "",
    "text": "Philly’s Center City District posted a list of restaurants and bars participating in Philly’s 2022 CCD Sips. CCD Sips is a series of summer Wednesday evenings (4:30-7pm) filled with happy hour specials, between June 1st and August 31st.\nI prefer to take in this information as a map instead of a list, so I scraped some information from the website and made one! You can click or tap on the circle map markers to see information about each restaurant/bar along with a direct link to their posted happy hour specials.\nCheck out the link at the top of this post for a larger version of the interactive map below. And jump down to the tutorial if you’d like to learn how I used R to build the interactive map!"
  },
  {
    "objectID": "blog/2022-05-31-ccd-sips/index.html#tutorial-start",
    "href": "blog/2022-05-31-ccd-sips/index.html#tutorial-start",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "Tutorial start",
    "text": "Tutorial start\nAside from the tidyverse and here packages, I used a handful of R packages to bring this map project together.\n\n\n\n\n\n\n\n\nPackage\nPurpose\nVersion\n\n\n\n\nrobotstxt\nCheck website for scraping permissions\n0.7.13\n\n\nrvest\nScrape the information off of the website\n1.0.1\n\n\nggmap\nGeocode the restaurant addresses\n3.0.0\n\n\nleaflet\nBuild the interactive map\n2.0.4.1\n\n\nleaflet.extras\nAdd extra functionality to map\n1.0.0"
  },
  {
    "objectID": "blog/2022-05-31-ccd-sips/index.html#scraping-the-data",
    "href": "blog/2022-05-31-ccd-sips/index.html#scraping-the-data",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "Scraping the data",
    "text": "Scraping the data\n\nChecking site permissions\nCheck the site’s terms of service using the robotstxt package, which downloads and parses the site’s robots.txt file.\nWhat I wanted to look for was whether any pages are not allowed to be crawled by bots/scrapers. In my case there weren’t any, indicated by Allow: /.\nget_robotstxt(\"https://centercityphila.org/explore-center-city/ccd-sips/sips-list-view\")\n\n\nOutput\n\n[robots.txt]\n--------------------------------------\n\n# robots.txt overwrite by: on_suspect_content\n\nUser-agent: *\nAllow: /\n\n\n\n[events]\n--------------------------------------\n\nrequested:   https://centercityphila.org/explore-center-city/ccd-sips/sips-list-view/robots.txt \ndownloaded:  https://centercityphila.org/explore-center-city/ccd-sips/sips-list-view/robots.txt \n\n$on_not_found\n$on_not_found$status_code\n[1] 404\n\n\n$on_file_type_mismatch\n$on_file_type_mismatch$content_type\n[1] \"text/html; charset=utf-8\"\n\n\n$on_suspect_content\n$on_suspect_content$parsable\n[1] FALSE\n\n$on_suspect_content$content_suspect\n[1] TRUE\n\n\n[attributes]\n--------------------------------------\n\nproblems, cached, request, class\n\n\n\nHarvesting data from the first page\nThen I used the rvest package to scrape the information from the tables of restaurants/bars participating in CCD Sips.\nI’ve learned that ideally you would only scrape each page once, so I checked my approach with the first page before I wrote a function to scrape the remaining pages.\n# define the page\nurl &lt;- \"https://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1\"\n\n# read the page html\nhtml1 &lt;- read_html(url)\n\n# extract table info\ntable1 &lt;- \n  html1 |&gt; \n  html_node(\"table\") |&gt; \n  html_table()\ntable1 |&gt; head(3) |&gt; kableExtra::kable()\n\n\n\n\n\nName\n\n\nAddress\n\n\nPhone\n\n\nCCD SIPS Specials\n\n\n\n\n\n\n1225 Raw Sushi and Sake Lounge\n\n\n1225 Sansom St, Philadelphia, PA 19102\n\n\n215.238.1903\n\n\nCCD SIPS Specials\n\n\n\n\n1518 Bar and Grill\n\n\n1518 Sansom St, Philadelphia, PA 19102\n\n\n267.639.6851\n\n\nCCD SIPS Specials\n\n\n\n\nAir Grille Garden at Dilworth Park\n\n\n1 S 15th St, Philadelphia, PA 19102\n\n\n215.587.2761\n\n\nCCD SIPS Specials\n\n\n\n\n\n# extract hyperlinks to specific restaurant/bar specials\nlinks &lt;- \n  html1 |&gt; \n  html_elements(\".o-table__tag.ccd-text-link\") |&gt; \n  html_attr(\"href\") |&gt; \n  as_tibble()\nlinks |&gt; head(3) |&gt; kableExtra::kable()\n\n\n\n\n\nvalue\n\n\n\n\n\n\n#1225-raw-sushi-and-sake-lounge\n\n\n\n\n#1518-bar-and-grill\n\n\n\n\n#air-grill-garden-dilworth-park\n\n\n\n\n\n# add full hyperlinks to the table info\ntable1Mod &lt;-\n  bind_cols(table1, links) |&gt; \n  mutate(Specials = paste0(url, value)) |&gt; \n  select(-c(`CCD SIPS Specials`, value))\ntable1Mod |&gt; head(3) |&gt; kableExtra::kable()\n\n\n\n\n\nName\n\n\nAddress\n\n\nPhone\n\n\nSpecials\n\n\n\n\n\n\n1225 Raw Sushi and Sake Lounge\n\n\n1225 Sansom St, Philadelphia, PA 19102\n\n\n215.238.1903\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1225-raw-sushi-and-sake-lounge\n\n\n\n\n1518 Bar and Grill\n\n\n1518 Sansom St, Philadelphia, PA 19102\n\n\n267.639.6851\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1518-bar-and-grill\n\n\n\n\nAir Grille Garden at Dilworth Park\n\n\n1 S 15th St, Philadelphia, PA 19102\n\n\n215.587.2761\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#air-grill-garden-dilworth-park\n\n\n\n\n\n\n\nHarvesting data from the remaining pages\nOnce I could confirm that the above approach harvested the information I needed, I adapted the code into a function that I could apply to pages 2-3 of the site.\ngetTables &lt;- function(pageNumber) {\n  Sys.sleep(2)\n  \n  url &lt;- paste0(\"https://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=\", pageNumber)\n  \n  html &lt;- read_html(url)\n  \n  table &lt;- \n    html |&gt; \n    html_node(\"table\") |&gt;\n    html_table()\n  \n  links &lt;- \n    html |&gt; \n    html_elements(\".o-table__tag.ccd-text-link\") |&gt; \n    html_attr(\"href\") |&gt; \n    as_tibble()\n  \n  tableSpecials &lt;&lt;-\n    bind_cols(table, links) |&gt; \n    mutate(Specials = paste0(url, value)) |&gt; \n    select(-c(`CCD SIPS Specials`, value))\n}\nI used my getTable() function and the purrr::map_df() function to harvest the table of restaurants/bars from pages 2 and 3. Then I combined all the data frames together and saved the complete data frame as an .Rds object so that I wouldn’t have to scrape the data again.\n# get remaining tables\ntable2 &lt;- map_df(2:3, getTables) \n\n# combine all tables\ntable &lt;- bind_rows(table1Mod, table2)\ntable |&gt; head(3) |&gt; kableExtra::kable()\n\n\n\n\n\nName\n\n\nAddress\n\n\nPhone\n\n\nSpecials\n\n\n\n\n\n\n1028 Yamitsuki Sushi & Ramen\n\n\n1028 Arch Street, Philadelphia, PA 19107\n\n\n215.629.3888\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1028-yamitsuki-sushi-ramen\n\n\n\n\n1225 Raw Sushi and Sake Lounge\n\n\n1225 Sansom St, Philadelphia, PA 19102\n\n\n215.238.1903\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1225-raw-sushi-and-sake-lounge\n\n\n\n\n1518 Bar and Grill\n\n\n1518 Sansom St, Philadelphia, PA 19102\n\n\n267.639.6851\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1518-bar-and-grill\n\n\n\n\n\n# save full table to file\nwrite_rds(\n  table,\n  file = here(\"content/blog/2022-05-31-ccd-sips/specialsScraped.Rds\")\n  )"
  },
  {
    "objectID": "blog/2022-05-31-ccd-sips/index.html#geocoding-addresses",
    "href": "blog/2022-05-31-ccd-sips/index.html#geocoding-addresses",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "Geocoding addresses",
    "text": "Geocoding addresses\nThe next step was to use geocoding to convert the restaurant/bar addresses to geographical coordinates (longitude and latitude) that I could map. I used the ggmap package and the Google Geocoding API service because this was a small project (59 addresses/requests) which wouldn’t make a dent in the free credit available on the platform.\nThe last time I geocoded addresses was for an almost identical project in 2019 and I had issues using the same API key from back then, so I made a new one. I restricted my new key to the Geocoding and Geolocation APIs.\n# register my API key\n# ggmap::register_google(key = \"[your key]\")\n\n# geocode addresses\nspecials_ggmap &lt;- \n  table |&gt; \n  mutate_geocode(Address)\n\n# rename new variables\nspecials &lt;- \n  specials_ggmap |&gt; \n  rename(Longitude = lon,\n         Latitude = lat) \nspecials |&gt; head(3) |&gt; kableExtra::kable()\n\n\n\n\n\nName\n\n\nAddress\n\n\nPhone\n\n\nSpecials\n\n\nLongitude\n\n\nLatitude\n\n\n\n\n\n\n1028 Yamitsuki Sushi & Ramen\n\n\n1028 Arch Street, Philadelphia, PA 19107\n\n\n215.629.3888\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1028-yamitsuki-sushi-ramen\n\n\n-75.15746\n\n\n39.95354\n\n\n\n\n1225 Raw Sushi and Sake Lounge\n\n\n1225 Sansom St, Philadelphia, PA 19102\n\n\n215.238.1903\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1225-raw-sushi-and-sake-lounge\n\n\n-75.16149\n\n\n39.95004\n\n\n\n\n1518 Bar and Grill\n\n\n1518 Sansom St, Philadelphia, PA 19102\n\n\n267.639.6851\n\n\nhttps://centercityphila.org/explore-center-city/ccd-sips/sips-list-view?page=1#1518-bar-and-grill\n\n\n-75.16665\n\n\n39.95020\n\n\n\n\n\nI made sure to save the new data frame with geographical coordinates as an .Rds object so I wouldn’t have to geocode the data again! This would be particularly important if I was working on a large project.\n# save table with geocoded addresses to file\nwrite_rds(\n  specials,\n  file = here(\"content/blog/2022-05-31-ccd-sips/specialsGeocoded.Rds\"))"
  },
  {
    "objectID": "blog/2022-05-31-ccd-sips/index.html#building-the-map",
    "href": "blog/2022-05-31-ccd-sips/index.html#building-the-map",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "Building the map",
    "text": "Building the map\nTo build the map, I used the leaflet package. Some of the resources I found helpful, in addition to the package documentation:\n\nScrape website data with the new R package rvest (+ a postscript on interacting with web pages with RSelenium) · Hollie at ZevRoss – how to style pop-ups\nLeaflet Map Markers in R · Jindra Lacko – how to customize marker icons\nA guide to basic Leaflet accessibility · Leaflet – accessibility considerations. Though it’s unclear to me how these features built into the Leaflet library translate over to the leaflet R package. For example, I couldn’t find an option for adding alt-text or a title to each marker, but maybe I wasn’t looking in the right place within the documentation.\n\n\nCustomizing map markers\n# style pop-ups for the map with inline css styling\n\n# marker for the restaurants/bars\npopInfoCircles &lt;- paste(\"&lt;h2 style='font-family: Red Hat Text, sans-serif; font-size: 1.6em; color:#43464C;'&gt;\", \"&lt;a style='color: #00857A;' href=\", specials$Specials, \"&gt;\", specials$Name, \"&lt;/a&gt;&lt;/h2&gt;\",\"&lt;p style='font-family: Red Hat Text, sans-serif; font-weight: normal; font-size: 1.5em; color:#9197A6;'&gt;\", specials$Address, \"&lt;/p&gt;\")\n\n# marker for the center of the map\npopInfoMarker&lt;-paste(\"&lt;h1 style='padding-top: 0.5em; margin-top: 1em; margin-bottom: 0.5em; font-family: Red Hat Text, sans-serif; font-size: 1.8em; color:#43464C;'&gt;\", \"&lt;a style='color: #00857A;' href='https://centercityphila.org/explore-center-city/ccdsips'&gt;\", \"Center City District Sips 2022\", \"&lt;/a&gt;&lt;/h1&gt;&lt;p style='color:#9197A6; font-family: Red Hat Text, sans-serif; font-size: 1.5em; padding-bottom: 1em;'&gt;\", \"Philadelphia, PA\", \"&lt;/p&gt;\")\n\n# custom icon for the center of the map\nawesome &lt;-\n  makeAwesomeIcon(\n    icon = \"map-pin\",\n    iconColor = \"#FFFFFF\",\n    markerColor = \"darkblue\",\n    library = \"fa\"\n  )\n\n\nPlotting the restaurants/bars\nleaflet(data = specials, \n        width = \"100%\", \n        height = \"850px\",\n        # https://stackoverflow.com/a/42170340\n        options = tileOptions(minZoom = 15,\n                              maxZoom = 19)) |&gt;\n  # add map markers ----\n  addCircles(\n    lat = ~ specials$Latitude, \n    lng = ~ specials$Longitude, \n    fillColor = \"#009E91\", #olivedrab goldenrod\n    fillOpacity = 0.6, \n    stroke = F,\n    radius = 12, \n    popup = popInfoCircles,\n    label = ~ Name,\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      ))\n\n\n\n\n\n\n\nAdding the map background\nleaflet(data = specials, \n        width = \"100%\", \n        height = \"850px\",\n        # https://stackoverflow.com/a/42170340\n        options = tileOptions(minZoom = 15,\n                              maxZoom = 19)) |&gt;\n  # add map markers ----\n  addCircles(\n    lat = ~ specials$Latitude, \n    lng = ~ specials$Longitude, \n    fillColor = \"#009E91\", #olivedrab goldenrod\n    fillOpacity = 0.6, \n    stroke = F,\n    radius = 12, \n    popup = popInfoCircles,\n    label = ~ Name,\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      )) |&gt;\n  # add map tiles in the background ----\n  addProviderTiles(providers$CartoDB.Positron)\n\n\n\n\n\n\n\nSetting the map view\nleaflet(data = specials, \n        width = \"100%\", \n        height = \"850px\",\n        # https://stackoverflow.com/a/42170340\n        options = tileOptions(minZoom = 15,\n                              maxZoom = 19)) |&gt;\n  # add map markers ----\n  addCircles(\n    lat = ~ specials$Latitude, \n    lng = ~ specials$Longitude, \n    fillColor = \"#009E91\", #olivedrab goldenrod\n    fillOpacity = 0.6, \n    stroke = F,\n    radius = 12, \n    popup = popInfoCircles,\n    label = ~ Name,\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      )) |&gt;\n  # add map tiles in the background ----\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  # set the map view\n  setView(mean(specials$Longitude), \n          mean(specials$Latitude), \n          zoom = 16)\n\n\n\n\n\n\n\nAdding a marker at the center\nleaflet(data = specials, \n        width = \"100%\", \n        height = \"850px\",\n        # https://stackoverflow.com/a/42170340\n        options = tileOptions(minZoom = 15,\n                              maxZoom = 19)) |&gt;\n  # add map markers ----\n  addCircles(\n    lat = ~ specials$Latitude, \n    lng = ~ specials$Longitude, \n    fillColor = \"#009E91\", #olivedrab goldenrod\n    fillOpacity = 0.6,\n    stroke = F,\n    radius = 12, \n    popup = popInfoCircles,\n    label = ~ Name,\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      )) |&gt;\n  # add map tiles in the background ----\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  # set the map view\n  setView(mean(specials$Longitude), \n          mean(specials$Latitude), \n          zoom = 16) |&gt;\n  # add marker at the center ----\n  addAwesomeMarkers(\n    icon = awesome,\n    lng = mean(specials$Longitude), \n    lat = mean(specials$Latitude), \n    label = \"Center City District Sips 2022\",\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      ),\n    popup = popInfoMarker,\n    popupOptions = popupOptions(maxWidth = 250))\n\n\n\n\n\n\n\nAdding fullscreen control\nleaflet(data = specials, \n        width = \"100%\", \n        height = \"850px\",\n        # https://stackoverflow.com/a/42170340\n        options = tileOptions(minZoom = 15,\n                              maxZoom = 19)) |&gt;\n  # add map markers ----\n  addCircles(\n    lat = ~ specials$Latitude, \n    lng = ~ specials$Longitude, \n    fillColor = \"#009E91\", #olivedrab goldenrod\n    fillOpacity = 0.6, \n    stroke = F,\n    radius = 12, \n    popup = popInfoCircles,\n    label = ~ Name,\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      )) |&gt;\n  # add map tiles in the background ----\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  # set the map view\n  setView(mean(specials$Longitude), \n          mean(specials$Latitude), \n          zoom = 16) |&gt;\n  # add marker at the center ----\n  addAwesomeMarkers(\n    icon = awesome,\n    lng = mean(specials$Longitude), \n    lat = mean(specials$Latitude), \n    label = \"Center City District Sips 2022\",\n    labelOptions = labelOptions(\n      style = list(\n        \"font-family\" = \"Red Hat Text, sans-serif\",\n        \"font-size\" = \"1.2em\")\n      ),\n    popup = popInfoMarker,\n    popupOptions = popupOptions(maxWidth = 250)) |&gt; \n  # add fullscreen control button ----\n  leaflet.extras::addFullscreenControl()"
  },
  {
    "objectID": "blog/2022-05-31-ccd-sips/index.html#creating-the-map-with-quarto",
    "href": "blog/2022-05-31-ccd-sips/index.html#creating-the-map-with-quarto",
    "title": "Philly Center City District Sips 2022: An Interactive Map",
    "section": "Creating the map with Quarto",
    "text": "Creating the map with Quarto\nThe first time around, I created a standalone map by first running an R script with the necessary code, and then exporting the HTML output as a webpage. This worked well enough, except that I realized:\n\nThe title of the map webpage (the name that is displayed on a browser tab) was just “map” because the name of the HTML file was map.html. I wanted something more descriptive.\nThe map wasn’t mobile-responsive. In other words, the map markers and text looked too small when viewed on a mobile device.\n\n\nChanging the webpage title\nThe webpage title was a quick one to fix thanks to a Stack Overflow response to a question about turning off the title in an R Markdown document. The pagetitle YAML option lets you set the HTML’s title tag independently of the document title:\npagetitle: \"Philly CCD Sips 2022 Map\"\n\n\nFixing the mobile-responsiveness\nThe mobile-responsiveness issue could be solved by adding metadata to the map HTML, but I would need to be able to blend HTML with R code. I have been practicing using Quarto and figured I could make a standalone map from a Quarto document (.qmd) rather than an R Markdown one (.Rmd or .Rmarkdown). You can find the map’s Quarto document alongside this blog post.\nAccording to the Leaflet library documentation and this Stack Overflow answer, fixing the map to be mobile-responsive required adding the following metadata to the HTML code:\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" /&gt;\nI used the metathis R package to add this metadata to an R code chunk in my Quarto document using the meta_viewport() function:\n# make mobile-responsive\nmeta_viewport(\n  width = \"device-width\",\n  initial_scale = \"1.0\",\n  maximum_scale = \"1.0\",\n  user_scalable = \"no\"\n  )\n\nUpdate: In the process of updating this post I’m noticing that specifying the viewport metadata tag doesn’t seem to be necessary anymore, and I don’t understand why 🤔 …so I’ll leave the step as is, just in case it’s helpful to anyone 🤷🏽‍♀️\n\n\n\nAdding social media tags\nThen I added more metadata. I was particularly interested in adding social media tags so that if I (or anyone else) shared this map webpage, an informative preview would display as a social card.\nI used the meta_social() function to add these tags:\n# tags for social media\nmeta_social(\n  title = \"Philly CCD Sips 2022 Interactive Map\",\n  url = \"https://www.silviacanelon.com/blog/2022-ccd-sips/map.html\",\n  image = \"https://github.com/spcanelon/silvia/blob/main/content/blog/2022-05-31-ccd-sips/featured.png?raw=true\",\n  image_alt = \"Map of Philly's Center City with a pop-up saying Center City District Sips 2022\",\n  og_type = \"website\",\n  og_author = \"Silvia Canelón\",\n  twitter_card_type = \"summary_large_image\",\n  twitter_creator = \"@spcanelon\"\n)\nGreat, I had added all of the metadata I was interested in! Except that because I was using Quarto, and not one of the more common outputs I had a couple of extra steps to take:\n\nWrite my metadata tags to an HTML file, using the write_meta() function:\n# write meta tags to file\nwrite_meta(path = \"meta-map.html\")\nManually include this HTML in my webpage via the Quarto file. The include-in-header Quarto YAML option helped me here:\ninclude-in-header: meta-map.html\n\n\n\nMaking the map fullscreen\nA side effect of creating the map from a Quarto (or R Markdown) document is that the output is styled by default to fit within the width of an article (in this case 900 pixels). I wanted the map to take up the whole width of the page, so I made use of the page-layout Quarto YAML option:\nformat: \n  html:\n    page-layout: custom\nAnother option that worked pretty well was to use the column: screen code chunk option built into Quarto. The Quarto documentation even shows an example to display a Leaflet map I but it left a thin margin at the top margin, and I wanted the map to be flush against the top edge of the webpage.\n\n\nRendering the standalone map\nLastly, I added one more option to the YAML that would render the Quarto document into a self-contained HTML with all of the content needed to create the map.\nformat:\n  html:\n    page-layout: custom\n    self-contained: true"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html",
    "href": "blog/2022-01-18-hello-umami/index.html",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "",
    "text": "A brief walkthrough of the steps I took to deploy Umami web analytics for my personal website, as documented in a short Twitter thread.\n\n07:50pmMonths ago I removed GA from my #RStats #blogdown site & this weekend I added http://umami.is 🍚 (@caozilla) as an open source, privacy-friendly, web analytics alternative\nI was intimidated by the self-hosting aspect, but the docs + @Railway made it possible! Steps in 🧵"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#what-to-expect",
    "href": "blog/2022-01-18-hello-umami/index.html#what-to-expect",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "",
    "text": "A brief walkthrough of the steps I took to deploy Umami web analytics for my personal website, as documented in a short Twitter thread.\n\n07:50pmMonths ago I removed GA from my #RStats #blogdown site & this weekend I added http://umami.is 🍚 (@caozilla) as an open source, privacy-friendly, web analytics alternative\nI was intimidated by the self-hosting aspect, but the docs + @Railway made it possible! Steps in 🧵"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#installation",
    "href": "blog/2022-01-18-hello-umami/index.html#installation",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "Installation",
    "text": "Installation\n07:50pmSteps I followed:\n\nInstall Railway CLI with Homebrew https://docs.railway.app/develop/cli\nInstall PostgreSQL with Homebrew https://wiki.postgresql.org/wiki/Homebrew\nFork Umami repo & follow steps in “Running on Railway from a forked repository” at https://umami.is/docs/running-on-railway\nClone repo locally w git"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#railway",
    "href": "blog/2022-01-18-hello-umami/index.html#railway",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "Railway",
    "text": "Railway\n07:50pm5. Link local setup to Railway project in the terminal w/ railway link &lt;projectid&gt;. Project ID is in the Railway dashboard under Setup\n\nCreate PostgreSQL tables using railway run in local umami directory + steps in “Create database tables” at https://umami.is/docs/running-on-railway\n\n07:50pm7. Deploy with railway up! 🚄\n\nFollow steps in Umami Getting Started docs https://umami.is/docs/login to login & add website\nAdd tracking code to website. In my #HugoApero #blogdown site I added it to layouts/partials/head.html. My example at https://github.com/spcanelon/silvia/blob/main/layouts/partials/head.html#L21-L29"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#tracker-configuration",
    "href": "blog/2022-01-18-hello-umami/index.html#tracker-configuration",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "Tracker Configuration",
    "text": "Tracker Configuration\n07:50pm10. In order to not track my own visits to my site, I followed the tip in @DeepankarBhade’s post https://dpnkr.in/blog/self-host-umami and disabled Umami from my browser’s local storage. He kindly explained the steps to me in this thread 😅 https://twitter.com/DeepankarBhade/status/1480214508987551750?s=20"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#pricing",
    "href": "blog/2022-01-18-hello-umami/index.html#pricing",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "Pricing",
    "text": "Pricing\n07:50pmA note about Railway pricing https://docs.railway.app/reference/limits:\nI’m using the free tier, the Starter Plan, which has $5 of credits. In the past 2 days I’ve used $0.7258 of my credits & it’s estimated I’ll use $3.04 by the end of the month. My site receives relatively low traffic, so YMMV\n07:51pmThere is a free-ish $10 credit Railway plan available also, where you would only get billed for any usage above $10\nFor a fully free & more adventurous experience you could give up the convenience of Railway & self-host! See the Umami docs for options https://umami.is/docs/hosting"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#goatcounter",
    "href": "blog/2022-01-18-hello-umami/index.html#goatcounter",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "GoatCounter",
    "text": "GoatCounter\n07:51pmI’ll leave you with another great free, open source, privacy-friendly option, which is GoatCounter 🐐 https://www.goatcounter.com/. And @mattdray wrote a blogdown post about it! https://twitter.com/mattdray/status/1306353556706992128?s=20\nFor more convos about GA web analytics alternatives, see https://twitter.com/ma_salmon/status/1379363183526285312?s=20"
  },
  {
    "objectID": "blog/2022-01-18-hello-umami/index.html#updating-umami",
    "href": "blog/2022-01-18-hello-umami/index.html#updating-umami",
    "title": "Hello Umami: Deploying a Privacy-Friendly Open Source Tool for Web Analytics",
    "section": "Updating Umami",
    "text": "Updating Umami\n01:52pmNote to self – how to update http://umami.is with new releases:\nRecent update to v1.25.0 https://github.com/mikecao/umami/releases/tag/v1.25.0\n\nFetch upstream from my umami fork\nLocally in terminal, change to my umami directory\ngit pull, npm install, railway up 🚄\n\n🚀 #WebAnalytics"
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html",
    "href": "blog/2020-05-12-trello-to-airtable/index.html",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "",
    "text": "Airtable is a user-friendly and powerful tool that until recently I’d been using for personal projects (i.e. document organizing, apartment hunting, etc.). A couple of weeks ago I leaned on Airtable to create a base designed for the Philadelphia Reproductive Freedom Collective to support our COVID-19 mutual aid efforts.\nHaving fallen in some Airtable deep-work I figured maybe it was time to retire my Trello boards in favor of some task bases. Airtable accepts CSV files from Trello but, alas, my free Trello account only gave me the option to print as a PDF or export as JSON. I decided this would be a good opportunity to learn how to parse JSON data and export it as a CSV ready for import to Airtable."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#cards",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#cards",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Cards",
    "text": "Cards\nThe first step is to extract information about the Trello cards themselves. This information is contained within a list of data frames and requires flattening which makes the nested hierarchical data structure into a flatter structure by assigning each of the nested variables its own column as much as possible. Then, the most important variables are selected as cards_trim before moving on to extracting label information.\n# selecting cards information\ncards &lt;- trello$cards # list of 1\n\n# flattening\ncards_flat &lt;- flatten(cards) #list of 37\n\n# tibble time\ncards_flat_tbl &lt;- as_tibble(cards_flat) # 32 obs of 37 variables\nglimpse(cards_flat_tbl)\n\n# selecting wanted variables\ncards_trim &lt;- cards_flat_tbl %&gt;%\n  select(id, idShort, idList, dateLastActivity, name, desc, dueComplete, due,\n         labels, attachments, shortUrl, closed) %&gt;%\n  arrange(desc(dateLastActivity))\n\nLabels\nRelevant information about the labels is selected and the unnest function is used to flatten because labels is a list of data frames. Again, I found Kan’s post helpful here! Particularly for saving the label details as a character list, which is helpful later on. Once we get to Airtable it’ll be important that label information for each card be structured as a simple list of words (i.e. label1, label2, label3). We get close once the labels are contained within a character list labelList, but there are still “c”s and parentheses that need to be removed. String manipulation is something I’m still learning about so the code below is far from elegant!\n# extracting labels details\nlabels_info &lt;- cards_trim %&gt;%\n  select(id, idShort, labels) %&gt;%\n  unnest() %&gt;% # no arguments because the nested items don't have names\n  rename(labelName = name) %&gt;%\n  select(id, idShort, labelName) %&gt;%\n  group_by(id, idShort) %&gt;%\n  summarize(labelList = list(labelName)) %&gt;%\n  mutate(labelList = as.character(labelList)) %&gt;%\n  mutate(labelList_tidy = str_remove_all(labelList, pattern = \"\\\"\")) %&gt;%\n  mutate(labelList_tidy = str_remove_all(labelList_tidy, pattern =\"c\\\\(\")) %&gt;%\n  mutate(labelList_tidy = str_remove_all(labelList_tidy, pattern =\"\\\\)\")) %&gt;%\n  unique()\n\nknitr::kable(labels_info %&gt;% head(n = 3L))\n\n# joining back with main cards data frame\nct_labels &lt;- left_join(cards_trim %&gt;% select(-labels), labels_info %&gt;% select(-labelList))\n\n\nAttachments\nThe next step is to download all of the items attached to the cards onto a local folder. I found this StackOverflow post really helpful. When I tried this out on my own Trello board I also found that I couldn’t download the few attachments I had made from my local drive. This StackOverflow post helped me figure out how to flag and catch these download errors so that I could create a list of the urls with “attachment errors” that I could follow up with manually.\n# expanding the attachment lists into separate url records\natt_urls &lt;- ct_labels %&gt;%\n  select(idShort, attachments) %&gt;%\n  unnest() %&gt;%\n  select(idShort, url) %&gt;%\n  mutate(url = as.character(url),\n         attachmentError = 'FALSE')\nknitr::kable(att_urls %&gt;% head(n = 3L))\n\n# creating directory for attachments\ndirAttachments &lt;- \"attachments/\"\ndir.create(dirAttachments)\n\n# downloading urls and checking for errors using try()\nfor (i in 1:length(att_urls$url)){\n  locAttachments &lt;-\n        paste(dirAttachments, \"/\", att_urls$idShort[i], \"_\", basename(att_urls$url[i]), sep = \"\")\n  step_to_try &lt;- try(attachment_check &lt;- download.file(att_urls$url[i], destfile = locAttachments))\n  if(\"try-error\" %in% class(step_to_try)) {\n    cat(\"Error row: \", i, \"\\n\", \"Error message: \", step_to_try[1], sep = \"\")\n    att_urls$attachmentError[i] = 'TRUE'\n  }\n}\nThe following selects the attachment records with errors, renames somes variables, and exports the data frame as a CSV.\n# preparing data frame for export to CSV\nattachment_errors &lt;- att_urls %&gt;%\n  filter(attachmentError == TRUE) %&gt;%\n  rename(Task_Id = idShort, Attachment_URL = url, Attachment_Error = attachmentError)\n\n# exporting to CSV\nwrite.csv(attachment_errors, file = \"attachment_errors.csv\")\n\nAside: If you have a lot of attachments per card, you may want to create a directory folder for each card. This for loop will get you there – use it instead of the one above:\n\n# creates individual directory folders for each card id\nfor (i in 1:length(att_urls$url)){\n  dirAttachments &lt;- paste(dirFiles, \"attachments\", att_urls$idShort[i], sep = \"/\")\n  dir.create(dirAttachments) # creates directory for each unique card id\n  locAttachments &lt;- paste(dirAttachments, basename(url[i]), sep = \"/\")\n  download.file(url[i], destfile = locAttachments)\n}\nRecords in the main cards data frame are labeled “TRUE” within the attachments column if they have attachments and “FALSE” if they don’t.\n# converts the attachment column to a categorical variable in the main cards+labels data frame\nct_labels &lt;- ct_labels %&gt;%\n  mutate(attachments = ifelse(idShort %in% att_urls$idShort, TRUE, FALSE))"
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#lists",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#lists",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Lists",
    "text": "Lists\nThe lists information is extracted similarly to the cards information, but flattening is a little more straightforward because it involves only one data frame. With more data frames, the unnest function is a better choice.\n# selecting lists information\nlists &lt;- trello$lists # list of 1 data frame\nglimpse(lists)\n\n# flattening\nlists_flat &lt;- lists[[1]] # 17 obs of 9 variables\n\n# selecting wanted variables\nlists_trim &lt;- lists_flat %&gt;%\n  select(id, name, closed) %&gt;%\n  rename(idList = id, nameList = name, closedList = closed)\nknitr::kable(lists_trim %&gt;% head(n = 3L))\n\n# joining back with main cards+labels data frame\nct_labels_list &lt;- left_join(ct_labels, lists_trim) %&gt;%\n  select(id:shortUrl, labelList_tidy:nameList, closed, closedList)"
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#data-prepping",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#data-prepping",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Data prepping",
    "text": "Data prepping\nColumns in the new ct_labels_list data frame are given new names, and the lubridate package is used next to convert the date fields. This resource was helpful in understanding date conversions and formatting.\n# changing variable names\ntidy_cards &lt;- ct_labels_list %&gt;%\n  select(-id, -idList, -closedList) %&gt;%\n  rename(Task = name, Task_ID = idShort, Notes = desc, Done = dueComplete, Date_Due = due,\n         Labels = labelList_tidy, Trello_List = nameList, Trello_Last_Modified = dateLastActivity,\n         Trello_Url = shortUrl, Trello_Attachments = attachments, Archived = closed) %&gt;%\n  select(Task, Task_ID, Notes, Done, Date_Due, Labels, Trello_List, Trello_Last_Modified,\n         Trello_Url, Trello_Attachments, Archived) %&gt;%\n  mutate(Trello_Last_Modified = as_datetime(Trello_Last_Modified, tz = \"\"),\n         Date_Due = as_datetime(Date_Due, tz = \"\"),\n         Done = ifelse(is.na(Date_Due) == TRUE, 'NA', Done)) # ensures only undone tasks assigned a due date get marked as \"FALSE\"\nThe last step before exporting the final data frame tidy_cards is to check the unique number of tasks to make sure it matches the number of records in the data frame (i.e. one task per observation).\n# determining the number of unique tasks\nlength(unique(tidy_cards$Task_ID))\n\n# final look at tidy_cards\nglimpse(tidy_cards)"
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#data-exporting",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#data-exporting",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Data exporting",
    "text": "Data exporting\nwrite.csv(tidy_cards, file = \"tidy_cards.csv\")"
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#task",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#task",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Task",
    "text": "Task\n\nBecause we know each observation in our table is unique, we can copy and paste Task into the first column and hide/delete the original column. Task is now the primary field."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#task_id",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#task_id",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Task_ID",
    "text": "Task_ID\n\nConvert Task_ID field type to “number”."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#notes",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#notes",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Notes",
    "text": "Notes\n\nConvert Notes field type to “long text” and enable rich text formatting. This gives us the option of using Markdown in the future, but sadly doesn’t automatically recognize fully formatted Markdown in the imported text."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#labels-1",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#labels-1",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Labels",
    "text": "Labels\n\nChange Labels field type to “multiple select” so that it turns each item in each list into a label.\nOptional: Create a new Projects column next to Labels and use the labels to guide you in creating Project labels/categories: Group the records by Labels field and add to Projects field as appropriate.\n\nI recommend creating an NA project from the NA labels so that these tasks aren’t marked as “uncategorized” in the Projects column. Having records with an “empty” assignment gets in the way whenever you want to group by that category. To that end, it’s helpful to group by Project and make sure any “empty” records get assigned to the “NA” Project.\n\nDelete from Labels any labels that were converted to Projects and ungroup the records."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#trello_lists",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#trello_lists",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Trello_Lists",
    "text": "Trello_Lists\n\nConvert Trello_Lists column field type into “single select”. This gives us the option of using the Kanban style we were used to in Trello.\n\nEvery record should already be associated with a Trello_List.\n\nIf you want to replicate the Trello kanban layout, change the order of the single select options in Trello_Lists to match the order from left-to-right of your Trello board"
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#trello_url",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#trello_url",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Trello_URL",
    "text": "Trello_URL\n\nConvert the Trello_URL field type to “URL” and then hide it if you don’t think you’ll reference it often."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#trello_attachments",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#trello_attachments",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Trello_Attachments",
    "text": "Trello_Attachments\n\nConvert Trello_Attachments field type to “single select”\nCreate a new Attachments column with field type “attachment”. This is where you’ll upload your downloaded attachments.\nFilter your records by Trello_Attachments so only show “TRUE” results\nSort records by Task_ID and simplify your view by temporarily hiding all columns except for Task_Name (primary field), Task_ID, Trello_Attachments, and the new Attachments column.\nOpen your local attachments folder and drag and drop the files to their corresponding Attachments field according to their Task_ID in the filename.\n\nIncrease the height of the records for this step. It’ll make it easier to make sure you’re dragging and dropping to the correct record\n\nIf you encountered errors downloading some of your attachment URLs, now is the time to check your local attachment_errors.csv file for the URLs with errors during the download process. These are attachments you’ll have to find elsewhere and upload to the Attachments field as needed.\nRemove the filter to your view and unhide any columns you wish to remain visible."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#done-archived",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#done-archived",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Done & Archived",
    "text": "Done & Archived\n\nConvert the Done and Archived field types to “Checkbox” and it will automatically assign a “check” to all records marked “TRUE” and leave the ones marked “FALSE” or “NA” unchecked. So easy!\nThere is no direct option to “archive” tasks that have been completed like you can do in Trello, but you can apply a filter to your table view to hide the tasks that are complete. To do this, set the filter so that the Done and Archived fields are unchecked.\n\nThis must be repeated for each saved View of your records."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#dates",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#dates",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Dates",
    "text": "Dates\n\nModify Date_Due and Trello_Last_Modified field types to “Date” with time.\nYou can sort the records by Trello_Last_Modified if that’s helpful, but otherwise you can hide the column and keep it for historical reference.\nCreate a new column Last_Modified with field type “Last modified time” and select all columns you want to track changes to on a date/time basis moving forward."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#record-views",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#record-views",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Record Views",
    "text": "Record Views\n\nSelect Kanban from the Views options and group by Trello_List to see your tasks similar to how you saw them in Trello, complete with attachment covers! A bonus is that if you have multiple images attached to a card, you can view them without expanding the card by just hovering over the attachment cover!\nMove, collapse, and delete stacks as you see fit. Customize cards with as little or as much information as you want."
  },
  {
    "objectID": "blog/2020-05-12-trello-to-airtable/index.html#tasks-vs-subtasks",
    "href": "blog/2020-05-12-trello-to-airtable/index.html#tasks-vs-subtasks",
    "title": "Migrating from Trello to Airtable: Working with JSON Data in R",
    "section": "Tasks vs Subtasks",
    "text": "Tasks vs Subtasks\nThere are probably many ways to parallel the checklist option Trello gives you within a card.\n\nThe most straightforward is to use the basic checklist formatting within the Description field to create lists\nAnother is to think of your primary field Tasks instead as ‘subtasks’ and create a new column to serve as the umbrella ‘task’. This new ‘task’ column would be field type “single selection”, then you could group your records by ‘task’."
  },
  {
    "objectID": "blog/2020-05-26-dark-mode-custom-with-atom/index.html",
    "href": "blog/2020-05-26-dark-mode-custom-with-atom/index.html",
    "title": "Customizing Hugo Academic’s Dark Mode with Help from Atom",
    "section": "",
    "text": "With Alison Hill’s Up and Running with Blogdown post! Super helpful, though because I came to it 2.5 years late, it was more like ‘up and running with lots of water breaks’ because I had to stop and account for changes made to the Hugo Academic theme in the meantime.\n\nFor example, prior to Academic version 4.6, custom CSS was added using the plugins_css option in params.toml, but in current version 4.8, the theme supports SCSS (a superset of CSS) and a custom.scss file is added in the assets/scss/ folder.\n\nThe going futher section in Alison’s post specifically talks about customizing the out-of-the-box theme and Alison directly links to her custom CSS file, which I closely referred to when changing colors in my custom SCSS file.\nAlison’s CSS helped me customize everything from text colors and fonts to alert colors and borders for Academic’s light mode. At this point I had the light mode looking the way I wanted but the dark mode still used out-of-the-box colors for the most part and they just didn’t go.\n\nSo I decided not to enable the dark-mode option in params.toml until I could figure out how to customize my stylesheet accordingly. That time has come because it turns out it’s pretty straightforward!\nThe Blogdown book does an excellent job summarizing what you need to know about CSS. This post builds on that a little by incorporating features made possible by SCSS including variables."
  },
  {
    "objectID": "blog/2020-05-26-dark-mode-custom-with-atom/index.html#basics-of-dark-theme-design",
    "href": "blog/2020-05-26-dark-mode-custom-with-atom/index.html#basics-of-dark-theme-design",
    "title": "Customizing Hugo Academic’s Dark Mode with Help from Atom",
    "section": "Basics of dark theme design",
    "text": "Basics of dark theme design\n\nThe primary surface color for dark themes should be dark gray, rather than black. The recommended color is #121212\nAs you layer components, surfaces with a higher elevation (closer to the hypothetical ‘light source’) should be lighter than those below it to create a visual hierarchy. This can be achieved by applying a semi-transparent white overlay to the primary dark gray surface.\n\n\n\nThe primary text color for dark themes should not be 100% opaque white (i.e. #FFFFFF) because it can appear to bleed or blur against dark backgrounds and be difficult to read.\nText hierarchy is established by controlling the opacity, for example:\n\nHigh emphasis text is white with 87% opacity:rgba(255, 255, 255, 0.87)\nMedium emphasis is white with 60% opacity:rgba(255, 255, 255, 0.60)\nDisabled text is white with 38% opacity:rgba(255, 255, 255, 0.38)\n\nTo meet WCAG AA standard, there must be a 4.5:1 contrast level between the body text and the dark theme surface at the highest/lightest elevation. The contrast level is 7:1 for the WCAG AAA standard."
  },
  {
    "objectID": "blog/2020-05-26-dark-mode-custom-with-atom/index.html#tools-to-explore-palettes",
    "href": "blog/2020-05-26-dark-mode-custom-with-atom/index.html#tools-to-explore-palettes",
    "title": "Customizing Hugo Academic’s Dark Mode with Help from Atom",
    "section": "Tools to explore palettes",
    "text": "Tools to explore palettes\nMaterial Design has a color palette generator and a color tool that can be used to dark and light variants of a color. I used the color tool to find a dark and light variant of my primary and secondary colors. The accessibility feature of the color tool is helpful to determine the minimum opacity for white text to ensure enough contrast. The Coolors color contrast checker is another great tool."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "R codding for fun",
    "section": "",
    "text": "Curvas Rango-Abundancia\n\n\nNotas de Ecología de Comunidades Vegetales.\n\n\n\n\nR\n\n\nEcologia de comunidades\n\n\nCurvas Rango-Abundancia\n\n\n \n\n\n\n\nOct 22, 2023\n\n\nPaul Santos - Andrade\n\n\n\n\n\n\nNo matching items\n\n Back to topReusehttps://creativecommons.org/licenses/by-sa/4.0/"
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html",
    "href": "blog/2021-06-02-wave-audit-1/index.html",
    "title": "WAVE Audit No. 1",
    "section": "",
    "text": "Did you know that 97.4% of home pages have web accessibility failures??? 😱\nThis finding is one of many from an accessibility analysis that non-profit WebAIM (Web Accessibility in Mind) conducts annually on home pages of the top one million websites. You can find a summary of these findings in a WebAIM blog post and detailed information in the full report, both published on April 30, 2021.\nLearning about all of the ways that digital content is made inaccessible to people with disabilities has made me take inventory of the different ways that I have contributed to this problem (there was some shame to process here 🙈).\nThe magic of R Markdown has given me the gift of turning R code into a variety of HTML outputs including R notebooks, xaringan presentation slides, and websites like this one – all of which I’ve been able to share freely online with others. This magic though (like all magic?) comes with limitations. R tools (and technology more broadly) can’t automatically ensure that its various outputs are accessible to everyone. That’s where we come in as software developers and content creators and take personal responsibility. At the risk of extending this metaphor too far, I’ll finish by offering the framework that we all need to practice (accessibility) spells/skills in order to use these magical tools responsibly.\nAll of this to say that gaining awareness about accessibility as a way to create the more inclusive world that I want to live in has motivated me to do better. I even found myself excited to conduct accessibility audits on my digital content, including my personal website (data viz too)!"
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html#home-page",
    "href": "blog/2021-06-02-wave-audit-1/index.html#home-page",
    "title": "WAVE Audit No. 1",
    "section": "Home page",
    "text": "Home page\nLink: https://silvia.rbind.io/\nAudit results:\n\n1 error\n\n1 x Missing alternative text\n\n6 alerts\n\n1 x Suspicious link text\n5 x Redundant title text\n\n\n\n\n\nFigure 1: Audit for my home page"
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html#about-page",
    "href": "blog/2021-06-02-wave-audit-1/index.html#about-page",
    "title": "WAVE Audit No. 1",
    "section": "About page",
    "text": "About page\nLink: https://silvia.rbind.io/about/\nAudit results:\n\n4 errors\n\n3 x Linked image missing alternative text\n1 x Empty link\n\n19 alerts\n\n2 x Skipped heading level\n1 x Possible heading\n4 x Suspicious link text\n7 x Redundant link\n5 x Redundant title text\n\n\n\n\n\nFigure 2: Audit for my About page header\n\n\n\n\n\nFigure 3: Audit for the main section of my About page\n\n\n\n\nFull page screenshot\n\n\n\n\nFigure 4: Audit for my About page in full page view"
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html#blog-page",
    "href": "blog/2021-06-02-wave-audit-1/index.html#blog-page",
    "title": "WAVE Audit No. 1",
    "section": "Blog page",
    "text": "Blog page\nLink: https://silvia.rbind.io/blog/\nAudit results:\n\n6 errors\n\n1 x Missing alternative text\n5 x Linked image missing alternative text\n\n10 alerts\n\n5 x Redundant link\n5 x Redundant title text\n\n\n\n\n\nFigure 5: Audit for my Blog listing page in full page view\n\n\n\nBlog example\nLink: https://silvia.rbind.io/blog/hello-hugo-apero/\nAudit results:\n\n25 errors\n\n1 x Missing alternative text\n24 x Empty link\n\n177 contrast errors\n\n177 x Very low contrast\n\n17 alerts\n\n9 x Long alternative text\n1 x Skipped heading level\n1 x Broken same-page link\n6 x Redundant title text\n\n\n\n\n\nAlerts shown for links in the navigation bar and for a heading item. Errors shown for the blog decorative image and for a link symbol next to a header."
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html#talk-page",
    "href": "blog/2021-06-02-wave-audit-1/index.html#talk-page",
    "title": "WAVE Audit No. 1",
    "section": "Talk page",
    "text": "Talk page\nLink: https://silvia.rbind.io/talk/\nAudit results:\n\n5 errors\n\n5 x Linked image missing alternative text\n\n13 alerts\n\n5 x Linked image missing alternative text\n2 x Skipped heading level\n5 x Redundant link\n5 x Redundant title text\n1 x YouTube video\n\n\n\n\n\nFigure 7: Audit for my Talk listing page\n\n\n\n\nFull page screenshot\n\n\n\n\nFigure 8: Audit for my About page header in full page view\n\n\n\n\nTalk example\nLink: https://silvia.rbind.io/talk/2021-data-viz-accessibility/\nAudit results:\n\n1 error\n\n1 x Empty link\n\n6 alerts\n\n1 x Skipped heading level\n5 x Redundant title text\n\n\n\n\n\nFigure 9: Audit for my CSV Conf talk post in full page view"
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html#publication-page",
    "href": "blog/2021-06-02-wave-audit-1/index.html#publication-page",
    "title": "WAVE Audit No. 1",
    "section": "Publication page",
    "text": "Publication page\nLink: https://silvia.rbind.io/publication/\nAudit results:\n\n0 errors\n5 alerts\n\n5 x Redundant title text\n\n\n\n\n\nFigure 10: Audit for my Publication listing page\n\n\n\n\nFull page screenshot\n\n\n\n\nAlerts displayed for the navigation bar and footer\n\n\n\n\nPublication example\nLink: https://silvia.rbind.io/publication/2021-geospatial-analysis-pregnancy-outcomes/\nAudit results:\n\n1 errors\n\n1 x Empty link\n\n6 alerts\n\n1 x Skipped heading level\n5 x Redundant title text\n\n\n\n\n\nFigure 12: Audit for my geospatial analysis publication in full page view"
  },
  {
    "objectID": "blog/2021-06-02-wave-audit-1/index.html#project-page",
    "href": "blog/2021-06-02-wave-audit-1/index.html#project-page",
    "title": "WAVE Audit No. 1",
    "section": "Project page",
    "text": "Project page\nLink: https://silvia.rbind.io/project/\nAudit results:\n\n3 errors\n\n3 x Empty link\n\n5 alerts\n\n5 x Redundant title text\n\n\n\n\n\nFigure 13: Audit for my Project listing page\n\n\n\nProject example\nLink: https://silvia.rbind.io/project/2021-tidy-tuesday-alt-text/\nAudit results:\n\n2 errors\n\n2 x Empty link\n\n10 alerts\n\n3 x Long alternative text\n1 x Skipped heading level\n6 x Redundant title text\n\n\n\n\n\nFigure 14: Audit for my TidyTuesdayAltText project post\n\n\n\n\nFull page screenshot\n\n\n\n\nFigure 15: Audit for my TidyTuesdayAltText project post in full page view"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#what-to-expect",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#what-to-expect",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "What to expect",
    "text": "What to expect\nBy the end of the tutorial you will have switched your website from using the Hugo Academic theme to using the new Hugo Apéro theme designed by Alison Hill \nSpecifically you will be able to migrate your blog, publications, and talks. If you need to migrate courses, I recommend taking a look at how Alison and Kelly Bodwin organized their courses and workshops into projects using this theme. I didn’t have projects prior to converting my site, but after creating a few projects post-Apéro I’m confident any projects you’ve created pre-Apéro will carry over easily.\n\nProjects on my site: https://silvia.rbind.io/project\n\n\n\n\nMy Project listing: https://silvia.rbind.io/project\n\n\n\nIf you like videos, Alison recorded a walkthrough of this conversion process using Julia Silge’s site as an example.\nWhat not to expect\nA tutorial on how to create a Hugo Apéro site from scratch – but don’t worry! Alison covers this in a workshop she gave for R-Ladies Tunis and in the Get started series of blog posts included in the documentation site."
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#the-plan",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#the-plan",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "The Plan",
    "text": "The Plan\nI was lucky that Alison had already started converting her own personal site because she gave me a template and example to follow! \nWe’ll follow the steps below throughout the tutorial and each of the six steps comes with its own commit in my git history, so you can see exactly what I changed and when. \n\nThen we’ll reuse and migrate your existing content, set up a contact form, tidy up your directory, explore some resources for customizing your new site, and end with the grand finale: deploying your new site!"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#prework",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#prework",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "Prework",
    "text": "Prework\nBranch deploy\nCreate a new apero branch from the primary branch of your website repository\n\ngit checkout -b apero to create new local branch\ngit push --set-upstream origin apero to push new branch to GitHub\n\nCreate a new Netlify deploy from your apero branch by enabling branch deploys on Netlify.com. Garrick Aden-Buie kindly provided some great resources on how to do this on Twitter. Netlify will automatically deploy a live preview of your site from your new branch to a link like &lt;branch-name&gt;--silvia.netlify.app. In my case it was https://apero–silvia.netlify.app\n Site settings: Build & deploy &gt; Continuous Deployment &gt; Deploy contexts\n\n\n\n\nNetlify site settings for deploy contexts\n\n\n\nYour new apero branch deploy at this point is an independent copy of your current website so from here on out you can make changes freely without affecting anything in your main branch :tada:\nHugo version\nThe last piece of prework before we dive in is to update your local version of Hugo and update the Hugo version accordingly in a few different places.\n\n\nUpdate Hugo locally using blogdown::install_hugo() (for me the latest version was v0.82.1)\nblogdown::install_hugo()\n\n\nUpdate .Rprofile and then restart R per the instructions that appear in the console.\n# fix Hugo version\noptions(blogdown.hugo.version = \"0.82.1\")\n\n\nUpdate netlify.toml\n[context.production.environment]\n  HUGO_VERSION = \"0.82.1\"\n  HUGO_ENV = \"production\"\n  HUGO_ENABLEGITINFO = \"true\"\n\n[context.branch-deploy.environment]\n  HUGO_VERSION = \"0.82.1\"\n\n[context.deploy-preview.environment]\n  HUGO_VERSION = \"0.82.1\"\n\n\nRun blogdown::check_site() to find any issues. In my case these checking functions found a Hugo version mismatch and I ended up having to specifically run blogdown::install_hugo(\"0.82.1\") to resolve it.\nConsole output\n― Checking netlify.toml...\n○ Found HUGO_VERSION = 0.82.1 in [build] context of netlify.toml.\n| Checking that Netlify & local Hugo versions match...\n| Mismatch found:  blogdown is using Hugo version (0.69.2) to build site locally.  Netlify is using Hugo version (0.82.1) to build site.\n● [TODO] Option 1: Change HUGO_VERSION = \"0.69.2\" in netlify.toml to match local version.\n● [TODO] Option 2: Use blogdown::install_hugo(\"0.82.1\") to match Netlify version, and set options(blogdown.hugo.version = \"0.82.1\") in .Rprofile to pin this Hugo version (also remember to restart R).\n| Checking that Netlify & local Hugo publish directories match...\n○ Good to go - blogdown and Netlify are using the same publish directory: public\n― Check complete: netlify.toml\n\n\n\nIf you end up needing to make your own changes, I recommend running blogdown::check_site() again when you’re done to make sure you’ve resolved all of the issues.\nAnd then run blogdown::serve_site() to render a live preview of your site :rocket:"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#install-theme-alongside-academic-change-in-config.toml",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#install-theme-alongside-academic-change-in-config.toml",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "1. Install theme alongside Academic, change in config.toml\n",
    "text": "1. Install theme alongside Academic, change in config.toml\n\n\n Follow along with me at commit cc5d24\n\nThe first step is to install all of the Hugo Apéro theme files to the theme/ folder in your site directory:\n\nblogdown::install_theme(theme = \"hugo-apero/hugo-apero\",\n                        update_config = FALSE, \n                        force = TRUE)\n\n\nConsole output\n\ntrying URL 'https://github.com/hugo-apero/hugo-apero/archive/main.tar.gz'\ndownloaded 21.4 MB\n\nDo not forget to change the 'theme' option in 'config.toml' to \"hugo-apero\"\nWarning message:\nThe theme has provided an example site. You should read the theme's documentation and at least take a look at the config file config.toml (or .yaml) of the example site, because not all Hugo themes work with any config files. \n\nAs indicated in console output, modify the config.toml file so it points to your new theme folder instead of hugo-academic:\n#theme = \"hugo-academic\"\ntheme = \"hugo-apero\"\n At this point you will probably start to get some error messages like the one below. Don’t panic! Let’s get through the rest of the steps first. I’m including my errors in this post in case they are helpful/validating for you!\nCould not build site because certain shortcodes weren't found\n\nError: Error building site: \"/Users/silvia/Documents/Website/silvia/content/home/demo.md:58:1\": failed to extract shortcode: template for shortcode \"alert\" not found"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#copy-all-academic-shortcodes-to-layouts-root-remove-later",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#copy-all-academic-shortcodes-to-layouts-root-remove-later",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "2. Copy all Academic shortcodes to layouts/ root (remove later)",
    "text": "2. Copy all Academic shortcodes to layouts/ root (remove later)\n\n Follow along with me at commit f3c7d53\n\nCopy the shortcodes\n\nFrom themes/hugo-academic/layouts/shortcodes/\nTo layouts/shortcodes/\n\n My error message:\nError: Error building site: TOCSS: failed to transform \"style.main.scss\" (text/x-scss): SCSS processing failed: file \"stdin\", line 7, col 24: Invalid CSS after \"...textFontFamily:\": expected expression (e.g. 1px, bold), was \"&lt;no value&gt;;\""
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#remove-all-assets",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#remove-all-assets",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "3. Remove all assets",
    "text": "3. Remove all assets\n\n Follow along with me at commit 3843c76\n\nBefore deleting anything, I recommend making a backup of your entire website folder, just in case.\nIn the assets/ root folder, delete:\n\nthe images/ folder which might contain your site icon\nthe scss/ folder which might contain your custom.scss file\n\n My error message:\nError: Error building site: TOCSS: failed to transform \"style.main.scss\" (text/x-scss): SCSS processing failed: file \"stdin\", line 7, col 24: Invalid CSS after \"...textFontFamily:\": expected expression (e.g. 1px, bold), was \"&lt;no value&gt;;\""
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#remove-all-custom-layouts",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#remove-all-custom-layouts",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "4. Remove all custom layouts",
    "text": "4. Remove all custom layouts\n\n Follow along with me at commit 1ad7e3d\n\nI had a couple of partials that I deleted from the layouts/ folder:\n\n\npartials/site_footer.html which provided a custom footer for my website\n\npartials/widgets/about.html which included the custom formatting for certificates in the Education section of the About page of my Academic site\n\n My error message:\nError: Error building site: TOCSS: failed to transform \"style.main.scss\" (text/x-scss): SCSS processing failed: file \"stdin\", line 7, col 24: Invalid CSS after \"...textFontFamily:\": expected expression (e.g. 1px, bold), was \"&lt;no value&gt;;\""
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#copy-over-apéro-example-site-config.toml-file",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#copy-over-apéro-example-site-config.toml-file",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "5. Copy over Apéro example site config.toml file",
    "text": "5. Copy over Apéro example site config.toml file\n\n Follow along with me at commit db37289\n\nRename config.toml in the root folder to config_old.toml\nCopy config.toml\n\nFrom themes/hugo-apero/exampleSite/\nTo your root directory (in my case it was silvia/)\n\n My error message:\nError: Error building site: failed to render pages: render of \"page\" failed: execute of template failed: template: _default/single.html:3:8: executing \"_default/single.html\" at &lt;partial \"head.html\" .&gt;: error calling partial: \"/Users/silvia/Documents/Website/silvia/themes/hugo-apero/layouts/partials/head.html:14:53\": execute of template failed: template: partials/head.html:14:53: executing \"partials/head.html\" at &lt;js&gt;: can't evaluate field Build in type string"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#remove-academic-config-directory",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#remove-academic-config-directory",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "6. Remove Academic config/ directory",
    "text": "6. Remove Academic config/ directory\n\n Follow along with me at commit 5541f38\n\nDelete the config/ folder from your root directory (in my case silvia/)\nI learned the hard way that the error below was due to not using an updated version of Hugo, which is why I included that step in the Prework. All this to say, I’m hoping you don’t see the error below!\n My error message:\nError: Error building site: failed to render pages: render of \"page\" failed: execute of template failed: template: _default/single.html:3:8: executing \"_default/single.html\" at &lt;partial \"head.html\" .&gt;: error calling partial: \"/Users/silvia/Documents/Website/silvia/themes/hugo-apero/layouts/partials/head.html:14:53\": execute of template failed: template: partials/head.html:14:53: executing \"partials/head.html\" at &lt;js&gt;: can't evaluate field Build in type string"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#migrating-the-content",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#migrating-the-content",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "Migrating the content",
    "text": "Migrating the content\nAssuming you have made it this far and are able to at least serve a live site that uses the new Hugo Apéro theme, you are ready to start migrating your content! :tada:\nFile are organized differently in Hugo Apéro and the next steps detail the high-level changes I made to get my content to fit the new structure. The goal was to have my site parallel the Hugo Apéro example site and Alison’s personal site.\nFile organization\nTo get an overview of how the file structure is different between the Academic and Apéro themes we’ll look at the content/ folder of the Apéro example site, my old Academic site, and my current Apéro site. These are organized into the panelsets below.\n\n\nExample site\nMy Academic site\nMy Apéro site\n\n\n\n\n Location: silvia/themes/hugo-apero/exampleSite\n├── config.toml\n├── content\n    ├── _index.md\n    ├── about\n    ├── blog\n    ├── collection\n    ├── contributors.md\n    ├── elements\n    ├── form\n    ├── license.md\n    ├── project\n    └── talk\n\n\n\n Location: silvia/\n.\n├── config.toml\n├── content\n    ├── authors\n    ├── courses\n    ├── home\n    ├── license.md\n    ├── post\n    ├── project\n    ├── publication\n    ├── slides\n    └── talk\n\n\n\n Location: silvia/\n.\n├── config.toml\n├── content\n    ├── _index.md       # &lt;-- new!\n    ├── about           # &lt;-- new!\n    ├── blog            # &lt;-- renamed (formerly post)\n    ├── collection      # &lt;-- new!\n    ├── form            # &lt;-- new!\n    ├── license.md\n    ├── project\n    ├── publication\n    └── talk\n\n\n\n\nAbout page\nResource: Customize your about page | Hugo Apéro\nMy About page:\n\ncontent/about/header/index.md\ncontent/about/main/index.md\ncontent/about/sidebar/index.md\n\n\n\n\n\nThe header of my About page: https://silvia.rbind.io/about/\n\n\n\n\n\n\n\nThe main section of my About page: https://silvia.rbind.io/about/\n\n\n\nI wanted to reuse my content from the About section of my Academic site, so I did a lot of copy-and-pasting into the right spots before editing. These steps are outlined in the table below.\n\n\n\n\n\n\n\n\nStep\nContent to copy\nFrom\nTo\n\n\n\n1\nFolder\nthemes/hugo-apero/exampleSite/content/about/\ncontent/\n\n\n2\nBody part 1\ncontent/authors/silvia/_index.md\ncontent/about/header/index.md\n\n\n3\nBody part 2\ncontent/authors/silvia/_index.md\ncontent/about/main/index.md\n\n\n4\nBiography → outro\ncontent/authors/silvia/_index.md\ncontent/about/main/index.md\n\n\n5\nInterests → link_list\ncontent/authors/silvia/_index.md\ncontent/about/sidebar/index.md\n\n\n6\nPhoto\ncontent/authors/silvia/avatar.png\ncontent/about/sidebar/avatar.png\n\n\nHomepage\nResource: Customize your homepage | Hugo Apéro\nMy homepage: content/_index.md\n\n\n\n\nMy Homepage: https://silvia.rbind.io\n\n\n\n\nCopy _ index.md from themes/hugo-apero/content/ to content/\nSave an image for your homepage in the static/img/ folder\nSpecify your homepage image in _ index.md\nBlog\nMy blog listing: content/blog/_index.md\n\n\n\n\nMy Blog listing: https://silvia.rbind.io/blog\n\n\n\nUpdate [menu] options in config.toml to activate Blog by changing url = \"/blog/\" and renaming content/post/ to content/blog/ to activate the new Apéro layout with the sidebar on the blog post listing and to enable thumbnails\n[[menu.header]]\n  name = \"Blog\"\n  title = \"Blog\"\n  url = \"/blog/\"\n  weight = 2            # &lt;-- item 2 in the navigation bar\nEdit content/blog/_ index.md with heading for the Blog listing page\n\nMake sure text_link_url: /blog/\nThe author: field will populate the by-line in each blog post unless another author is indicated in the YAML of the blog post.\nPublications\nMy publication listing: content/publication/_index.md\n\n\n\n\nMy Publication listing: https://silvia.rbind.io/publication\n\n\n\nUpdate [menu] options in config.toml to activate Publications\n[[menu.header]]\n  name = \"Publications\"\n  title = \"Publications\"\n  url = \"/publication/\"\n  weight = 4            # &lt;-- item 4 in the navigation bar\nRename content/publication/_ index.md to _ index-old.md and copy over _ index.md from themes/hugo-apero/exampleSite/content/blog/\nEdit content/publication/_ index.md to suit your preferences\nModify individual publications:\n\nThe Apéro theme doesn’t have a built-in “abstract” field so I copied and pasted the content in this field from the YAML of each publication page into the area below the YAML.\nIf your publications have multiple authors, they can be included as a string list in the author: field of the YAML\nTalks\nMy talk listing: content/talk/_index.md\n\n\n\n\nMy Talk listing: https://silvia.rbind.io/talk\n\n\n\nRename content/talk/_ index.md to _ index-old.md and copy over _ index.md from themes/hugo-apero/exampleSite/content/talk/\nEdit content/talk/_ index.md to suit your preferences\n.Rmd → .Rmarkdown\nYou can create content for your blogdown site from .md, .Rmd, and .Rmarkdown files, anytime and anywhere. However, there are some limitations:\n\n\n.md is great if your file doesn’t contain any R code\n\n.Rmd files generate .html files while .Rmarkdown files generate .markdown files. Both can run R code, but only .markdown files generated from .Rmarkdown benefit from some of the features available from Hugo, like the syntax highlighting built into Apéro.\n\nIf you were writing R tutorials/posts/etc. in .Rmd (like me), you will notice any code chunks you were displaying will not be formatted with proper syntax highlighting :cry: To remedy this, you will have to:\n\nChange these index.Rmd files to index.Rmarkdown (I recommend using your computer’s file explorer for this)\nRebuild your index.Rmarkdown files to index.markdown (using blogdown::build_site(build_rmd = TRUE), see the helper functions for more granular control)\nDelete the index.html output files that had previously been generated\n\n Rebuilding your R Markdown pages may not be a good idea if they contain code that might break, so please proceed with caution!\nIf you made it this far, congratulations! You have a brand new site! :partying_face:"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#final-touches",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#final-touches",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "Final touches",
    "text": "Final touches\nContact form\nResource: Built-in Contact Form | Hugo Apéro\nIf you’d like to use Apéro’s built-in contact form powered by Formspree, copy the themes/hugo-apero/exampleSite/content/form/ folder into content/ and edit contact.md.\nTidying up your directory\nNow you can delete all of the files and folders you don’t need anymore!\nI’m including the files and folders I deleted as a list and as a directory tree. These are organized in the panelset below.\n\n\nList of items\nDirectory tree\n\n\n\n\n\nThe content folders carried over from Hugo Academic: authors, home, post, courses, and slides\nThe config folder\nThe resources folder\nThe data folder containing fonts and themes folders\nThe assets/images folder\nThe static/img/headers, static/publications, and static/rmarkdown-libs folders\nAll of the index.html files in the blog, publication, and talks folders\nThe old config file, that I had renamed config_old.toml\n\nThe old index files that I had renamed _ index-old.md\n\nThe partials in layouts/shortcodes\nAnd finally the themes/hugo-academic folder! 🔥\n\n\n\n\nI deleted the following files:\n\nAll of the index.html files in the blog, publication, and talks folders\nThe old config file, that I had renamed config_old.toml\n\nThe old index files that I had renamed _ index-old.md\n\n\nAnd I deleted the folders indicated in this directory tree:\n Location: silvia/\n.\n├── config                # &lt;-- this folder\n├── resources             # &lt;-- this folder\n├── data                  # &lt;-- this folder\n├── assets\n│   └── images            # &lt;-- this folder\n├── static\n│   ├── img\n│   │   └── headers       # &lt;-- this folder\n│   ├── publications      # &lt;-- this folder\n│   └── rmarkdown-libs    # &lt;-- this folder\n├── layouts\n│   └── shortcodes        # &lt;-- custom partials in this folder\n└── themes\n    └── hugo-academic     # &lt;-- this folder"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#customizing-your-site",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#customizing-your-site",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "Customizing your site",
    "text": "Customizing your site\nHopefully all of that wasn’t terrible, and if it was, please know I’m rooting for you. You’re doing great! :raised_hands:\nNow you get to enjoy the fun part which is customizing your site! The theme documentation goes through this in detail:\n\nSet up your social | Hugo Apéro\nStyle your site typography | Hugo Apéro\nStyle your site colors | Hugo Apéro"
  },
  {
    "objectID": "blog/2021-06-01-hello-hugo-apero/index.html#deploying-your-new-site",
    "href": "blog/2021-06-01-hello-hugo-apero/index.html#deploying-your-new-site",
    "title": "Hello Hugo Apéro: Converting a Blogdown Site from Hugo Academic",
    "section": "Deploying your new site",
    "text": "Deploying your new site\nOnce you’re happy with your new Apéro site, the last step is to merge your apero branch with the primary branch of your website repository. But first, a few steps:\n\nOptional: Create a branch of your primary branch and call it hugo-academic so that you have a snapshot of your Academic files right before the merge. Since we set up Netlify to deploy all of our branches, there will now be a live link for this new branch that you can visit whenever you feel like time traveling back to your old site. For me this link is https://hugo-academic–silvia.netlify.app/\n\nSwitch back to your apero branch and update the baseURL field in config.toml to your regular website path. In my case:\nbaseURL = \"https://silvia.rbind.io/\"\nThen commit and push this change to your apero branch.\n\nMerge your apero branch with your primary branch. I usually use git commands in a combination of the RStudio terminal and the Git pane, but for this big merge I felt more comfortable doing it on github.com! :sweat_smile: Do what feels most comfortable for you.\n\nResolve any merge conflicts (I had a few!) in the git tool of your choosing. These are the git commands GitHub recommended:\ngit fetch origin       # makes sure local files were recent\ngit checkout apero     # moves you to your `apero` branch\ngit merge main         # attempts a merge with your `main` branch\nWhen you’re finished, commit your changes and push. Then follow these next steps, also recommended by GitHub:\ngit checkout main       # moves you to your `main` branch\ngit merge --no-ff apero # creates a new commit for the merge\nThis step will sort of replace all of the files that both themes had in common with the apero version (e.g. config.toml, netlify.toml, content/publication), and leave the old Academic files alone. So you will have to delete these extra Academic files (again!). I’m not sure how to avoid this – maybe it’s not an issue when you don’t have merge conflicts? I don’t know :thinking:\n\n\nTidy up your directory (again?)\nGo through the steps above to clean out any residual Academic files from your directory. Make sure to check your content/ folders for any example files from Academic that might still be hanging around and delete them.\nThen run blogdown::serve_site() to build your new Apéro site locally. Go through the site and make sure everything looks the way it should and that links are generally pointing to the right places.\nWhen you’re satisfied, commit the changes to your primary branch!There may be a lot of files that were deleted and added during the switch to Apéro and, while not generally recommended, I used the git add . command to stage all of the changes at once, commited the changes, and then pushed. I did this after thoroughly looking through the list of changed files so I knew what was happening.\n\nWait a couple of minutes for the changes to get pushed to your primary branch (e.g. main) and then wait patiently for Netlify to build your site after the merge.\n Celebrate and share your brand new site! 🎉 🥳 🍾If you share on Twitter, use the #HugoApero hashtag so the Hugo Apéro squad can clap for you!"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Paul Efren",
    "section": "",
    "text": "I’m an enthusiastic data analyst and researcher with a strong background in the Andes Biodiversity and Ecosystem Research Group ABERG. My research is centered around plant ecology and code development.\nI’ve actively participated in projects across the Andes-Amazon gradient, providing valuable insights into the complexities of global change. To delve deeper into my research interests, explore my publications.\nIf you have any questions or wish to get in touch, don’t hesitate to send me a message."
  },
  {
    "objectID": "about/index.html#about-me",
    "href": "about/index.html#about-me",
    "title": "Paul Efren",
    "section": "About me",
    "text": "About me\n\n\nI’m a biologist on a journey to become proficient in informatics, with a keen interest in data management and biodiversity.\nI find joy in leveraging R to streamline my data science workflow and conduct biodiversity data analysis. I also take pride in developing and maintaining various R packages, such as:\n\nppendemic\nredbookperu\navesperu\n\nSince 2013, I’ve been actively involved in tropical forest ecology. During this time, I’ve gained extensive experience through collaborations with the Andes Biodiversity and Ecosystem Research Group ABERG in the Andes and Amazon regions of Peru.\n\n\nMy contributions have encompassed a wide range of projects, including the evaluation of:\n\nPlant trait ecology.\nCarbon and forest dynamics.\nCO2 and methane fluxes.\n\nMy educational background includes a:\n\n\n Master’s in Ecology & Environmental Management (graduated)∙ Universidad Nacional San Antonio Abad ∙ 2021\n\n\nBiologist ∙ Universidad Nacional San Antonio Abad ∙ 2016\n\n\nB.S. in Biology ∙ Universidad Nacional San Antonio Abad ∙ 2012\n\n\nYou could find my CV: Spanish - English"
  },
  {
    "objectID": "about/index.html#lately",
    "href": "about/index.html#lately",
    "title": "Paul Efren",
    "section": "Lately …",
    "text": "Lately …\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\nHello Quarto: Porting my Website from Hugo Apéro\n\n\nNotes from porting my personal Blogdown website to Quarto\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nPlace-based Interventions to Promote Structural Change & Equity\n\n\nTalk for the Philadelphia Department of Public Health Epis for Equity work group, highlighting the placed-based intervention projects at the Urban Health Lab\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\n\n\n\n\n\nA medication-wide association study (MWAS) on repurposed drugs for COVID-19 with Pre-pandemic prescription medication exposure and pregnancy outcomes\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nProfessional, Polished, Presentable\n\n\nMaking Great Slides with xaringan\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →"
  },
  {
    "objectID": "publication/2020-05-06-preprint-hydroxychloroquine-pregnancy/index.html",
    "href": "publication/2020-05-06-preprint-hydroxychloroquine-pregnancy/index.html",
    "title": "Is Hydroxychloroquine Safe During Pregnancy? Observations from Penn Medicine",
    "section": "",
    "text": "Type I collagen morphology can be characterized using fibril D-spacing, a metric which describes the periodicity of repeating bands of gap and overlap regions of collagen molecules arranged into collagen fibrils. This fibrillar structure is stabilized by enzymatic crosslinks initiated by lysyl oxidase (LOX), a step which can be disrupted using β-aminopropionitrile (BAPN). Murine in vivo studies have confirmed effects of BAPN on collagen nanostructure and the objective of this study was to evaluate the mechanism of these effects in vitro by measuring D-spacing, evaluating the ratio of mature to immature crosslinks, and quantifying gene expression of type I collagen and LOX. Osteoblasts were cultured in complete media, and differentiated using ascorbic acid, in the presence or absence of 0.25mM BAPN-fumarate. The matrix produced was imaged using atomic force microscopy (AFM) and 2D Fast Fourier transforms were performed to extract D-spacing from individual fibrils. The experiment was repeated for quantitative reverse transcription polymerase chain reaction (qRT-PCR) and Fourier Transform infrared spectroscopy (FTIR) analyses. The D-spacing distribution of collagen produced in the presence of BAPN was shifted toward higher D-spacing values, indicating BAPN affects the morphology of collagen produced in vitro, supporting aforementioned in vivo experiments. In contrast, no difference in gene expression was found for any target gene, suggesting LOX inhibition does not upregulate the LOX gene to compensate for the reduction in aldehyde formation, or regulate expression of genes encoding type I collagen. Finally, the mature to immature crosslink ratio decreased with BAPN treatment and was linked to a reduction in peak percent area of mature crosslink hydroxylysylpyridinoline (HP). In conclusion, in vitro treatment of osteoblasts with low levels of BAPN did not induce changes in genes encoding LOX or type I collagen, but led to an increase in collagen D-spacing as well as a decrease in mature crosslinks."
  },
  {
    "objectID": "publication/2020-05-06-preprint-hydroxychloroquine-pregnancy/index.html#abstract",
    "href": "publication/2020-05-06-preprint-hydroxychloroquine-pregnancy/index.html#abstract",
    "title": "Is Hydroxychloroquine Safe During Pregnancy? Observations from Penn Medicine",
    "section": "",
    "text": "Type I collagen morphology can be characterized using fibril D-spacing, a metric which describes the periodicity of repeating bands of gap and overlap regions of collagen molecules arranged into collagen fibrils. This fibrillar structure is stabilized by enzymatic crosslinks initiated by lysyl oxidase (LOX), a step which can be disrupted using β-aminopropionitrile (BAPN). Murine in vivo studies have confirmed effects of BAPN on collagen nanostructure and the objective of this study was to evaluate the mechanism of these effects in vitro by measuring D-spacing, evaluating the ratio of mature to immature crosslinks, and quantifying gene expression of type I collagen and LOX. Osteoblasts were cultured in complete media, and differentiated using ascorbic acid, in the presence or absence of 0.25mM BAPN-fumarate. The matrix produced was imaged using atomic force microscopy (AFM) and 2D Fast Fourier transforms were performed to extract D-spacing from individual fibrils. The experiment was repeated for quantitative reverse transcription polymerase chain reaction (qRT-PCR) and Fourier Transform infrared spectroscopy (FTIR) analyses. The D-spacing distribution of collagen produced in the presence of BAPN was shifted toward higher D-spacing values, indicating BAPN affects the morphology of collagen produced in vitro, supporting aforementioned in vivo experiments. In contrast, no difference in gene expression was found for any target gene, suggesting LOX inhibition does not upregulate the LOX gene to compensate for the reduction in aldehyde formation, or regulate expression of genes encoding type I collagen. Finally, the mature to immature crosslink ratio decreased with BAPN treatment and was linked to a reduction in peak percent area of mature crosslink hydroxylysylpyridinoline (HP). In conclusion, in vitro treatment of osteoblasts with low levels of BAPN did not induce changes in genes encoding LOX or type I collagen, but led to an increase in collagen D-spacing as well as a decrease in mature crosslinks."
  },
  {
    "objectID": "publication/2021-11-24-sct-stillbirth/index.html",
    "href": "publication/2021-11-24-sct-stillbirth/index.html",
    "title": "Evaluation of Stillbirth Among Pregnant People With Sickle Cell Trait",
    "section": "",
    "text": "Importance. Relative to what is known about pregnancy complications and sickle cell disease (SCD), little is known about the risk of pregnancy complications among those with sickle cell trait (SCT). There is a lack of clinical research among sickle cell carriers largely due to low sample sizes and disparities in research funding.\nObjective. To evaluate whether there is an association between SCT and a stillbirth outcome.\nDesign, Setting, and Participants. This retrospective cohort study included data on deliveries occurring between January 1, 2010, and August 15, 2017, at 4 quaternary academic medical centers within the Penn Medicine health system in Pennsylvania. The population included a total of 2482 deliveries from 1904 patients with SCT but not SCD, and 215 deliveries from 164 patients with SCD. Data were analyzed from May 3, 2019, to September 16, 2021.\nExposures. The primary exposure of interest was SCT, identified using clinical diagnosis codes recorded in the electronic health record.\nMain Outcomes and Measures. A multivariate logistic regression model was constructed to assess the risk of stillbirth using the following risk factors: SCD, numbers of pain crises and blood transfusions before delivery, delivery episode (as a proxy for parity), prior cesarean delivery, multiple gestation, patient age, marital status, race and ethnicity, ABO blood type, Rhesus (Rh) factor, and year of delivery.\nResults. This cohort study included 50 560 patients (63 334 deliveries), most of whom were aged 25 to 34 years (29 387 of 50 560 [58.1%]; mean [SD] age, 29.5 [6.1] years), were single at the time of delivery (28 186 [55.8%]), were Black or African American (23 777 [47.0%]), had ABO blood type O (22 879 [45.2%]), and were Rhesus factor positive (44 000 [87.0%]). From this general population, 2068 patients (4.1%) with a sickle cell gene variation were identified: 1904 patients (92.1%) with SCT (2482 deliveries) and 164 patients (7.9%) with SCD (215 deliveries). In the fully adjusted model, SCT was associated with an increased risk of stillbirth (adjusted odds ratio [aOR], 8.94; 95% CI, 1.05-75.79; P = .045) while adjusting for the risk factors of SCD (aOR, 26.40; 95% CI, 2.48-280.90; P = .007) and multiple gestation (aOR, 4.68; 95% CI, 3.48-6.29; P &lt; .001).\nConclusions and Relevance. The results of this large, retrospective cohort study indicate an increased risk of stillbirth among pregnant people with SCT. These findings underscore the need for additional risk assessment during pregnancy for sickle cell carriers.\n\n\n\nDirected Acyclic Graph With Stillbirth as the Outcome and Sickle Cell Trait (SCT) as the Primary Exposure of Interest.\n\n\n\n\nAlternative Figure 2\n\n\n\n\nAlternative Directed Acyclic Graph With Stillbirth as the Outcome and Sickle Cell Trait (SCT) as the Primary Exposure of Interest\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publication/2022-11-24-mwas-repurposed-covid19-meds/index.html",
    "href": "publication/2022-11-24-mwas-repurposed-covid19-meds/index.html",
    "title": "A medication-wide association study (MWAS) on repurposed drugs for COVID-19 with Pre-pandemic prescription medication exposure and pregnancy outcomes",
    "section": "",
    "text": "Information on effects of medication therapies during pregnancy is lacking as pregnant patients are often excluded from clinical trials. This retrospective study explores the potential of using electronic health record (EHR) data to inform safety profiles of repurposed COVID medication therapies on pregnancy outcomes using pre-COVID data. We conducted a medication-wide association study (MWAS) on prescription medication exposures during pregnancy and the risk of cesarean section, preterm birth, and stillbirth, using EHR data between 2010–2017 on deliveries at PennMedicine. Repurposed drugs studied for treatment of COVID-19 were extracted from ClinicalTrials.gov (n = 138). We adjusted for known comorbidities diagnosed within 2 years prior to birth. Using previously developed medication mapping and delivery-identification algorithms, we identified medication exposure in 2,830 of a total 63,334 deliveries; from 138 trials, we found 31 medications prescribed and included in our cohort. We found 21 (68%) of the 31 medications were not positively associated with increased risk of the outcomes examined. With caution, these medications warrant potential for inclusion of pregnant individuals in future studies, while drugs found to be associated with pregnancy outcomes require further investigation. MWAS facilitates hypothesis-driven evaluation of drug safety across all prescription medications, revealing potential drug candidates for further research."
  },
  {
    "objectID": "publication/2022-11-24-mwas-repurposed-covid19-meds/index.html#abstract",
    "href": "publication/2022-11-24-mwas-repurposed-covid19-meds/index.html#abstract",
    "title": "A medication-wide association study (MWAS) on repurposed drugs for COVID-19 with Pre-pandemic prescription medication exposure and pregnancy outcomes",
    "section": "",
    "text": "Information on effects of medication therapies during pregnancy is lacking as pregnant patients are often excluded from clinical trials. This retrospective study explores the potential of using electronic health record (EHR) data to inform safety profiles of repurposed COVID medication therapies on pregnancy outcomes using pre-COVID data. We conducted a medication-wide association study (MWAS) on prescription medication exposures during pregnancy and the risk of cesarean section, preterm birth, and stillbirth, using EHR data between 2010–2017 on deliveries at PennMedicine. Repurposed drugs studied for treatment of COVID-19 were extracted from ClinicalTrials.gov (n = 138). We adjusted for known comorbidities diagnosed within 2 years prior to birth. Using previously developed medication mapping and delivery-identification algorithms, we identified medication exposure in 2,830 of a total 63,334 deliveries; from 138 trials, we found 31 medications prescribed and included in our cohort. We found 21 (68%) of the 31 medications were not positively associated with increased risk of the outcomes examined. With caution, these medications warrant potential for inclusion of pregnant individuals in future studies, while drugs found to be associated with pregnancy outcomes require further investigation. MWAS facilitates hypothesis-driven evaluation of drug safety across all prescription medications, revealing potential drug candidates for further research."
  },
  {
    "objectID": "publication/2020-08-02-maddie-ehr-delivery-episode-algorithm/index.html",
    "href": "publication/2020-08-02-maddie-ehr-delivery-episode-algorithm/index.html",
    "title": "Development and Evaluation of MADDIE: Method to Acquire Delivery Date Information from Electronic Health Records",
    "section": "",
    "text": "Objective. To develop an algorithm that infers patient delivery dates (PDDs) and delivery-specific details from Electronic Health Records (EHRs) with high accuracy; enabling pregnancy-level outcome studies in women’s health.\nMaterials and Methods. We obtained EHR data from 1,060,100 female patients treated at Penn Medicine hospitals or outpatient clinics between 2010-2017. We developed an algorithm called MADDIE: Method to Acquire Delivery Date Information from Electronic Health Records that infers a PDD for distinct deliveries based on EHR encounter dates assigned a delivery code, the frequency of code usage, and the time differential between code assignments. We validated MADDIE’s PDDs against a birth log independently maintained by the Department of Obstetrics and Gynecology.\nResults. MADDIE identified 50,560 patients having 63,334 distinct deliveries. MADDIE was 98.6% accurate (F1-score 92.1%) when compared to the birth log. The PDD was on average 0.68 days earlier than the true delivery date for patients with only one delivery (± 1.43 days) and 0.52 days earlier for patients with more than one delivery episode (± 1.11 days).\nDiscussion. MADDIE is the first algorithm to successfully infer PDD information using only structured delivery codes and identify multiple deliveries per patient. MADDIE is also the first to validate the accuracy of the PDD using an external gold standard of known delivery dates as opposed to manual chart review of a sample.Conclusion. MADDIE augments the EHR with delivery-specific details extracted with high accuracy and relies only on structured EHR elements while harnessing temporal information and the frequency of code usage to identify accurate PDDs.\n\n\n\nPoster presented at the 2020 AMIA Annual Symposium"
  },
  {
    "objectID": "publication/2020-08-02-maddie-ehr-delivery-episode-algorithm/index.html#abstract",
    "href": "publication/2020-08-02-maddie-ehr-delivery-episode-algorithm/index.html#abstract",
    "title": "Development and Evaluation of MADDIE: Method to Acquire Delivery Date Information from Electronic Health Records",
    "section": "",
    "text": "Objective. To develop an algorithm that infers patient delivery dates (PDDs) and delivery-specific details from Electronic Health Records (EHRs) with high accuracy; enabling pregnancy-level outcome studies in women’s health.\nMaterials and Methods. We obtained EHR data from 1,060,100 female patients treated at Penn Medicine hospitals or outpatient clinics between 2010-2017. We developed an algorithm called MADDIE: Method to Acquire Delivery Date Information from Electronic Health Records that infers a PDD for distinct deliveries based on EHR encounter dates assigned a delivery code, the frequency of code usage, and the time differential between code assignments. We validated MADDIE’s PDDs against a birth log independently maintained by the Department of Obstetrics and Gynecology.\nResults. MADDIE identified 50,560 patients having 63,334 distinct deliveries. MADDIE was 98.6% accurate (F1-score 92.1%) when compared to the birth log. The PDD was on average 0.68 days earlier than the true delivery date for patients with only one delivery (± 1.43 days) and 0.52 days earlier for patients with more than one delivery episode (± 1.11 days).\nDiscussion. MADDIE is the first algorithm to successfully infer PDD information using only structured delivery codes and identify multiple deliveries per patient. MADDIE is also the first to validate the accuracy of the PDD using an external gold standard of known delivery dates as opposed to manual chart review of a sample.Conclusion. MADDIE augments the EHR with delivery-specific details extracted with high accuracy and relies only on structured EHR elements while harnessing temporal information and the frequency of code usage to identify accurate PDDs.\n\n\n\nPoster presented at the 2020 AMIA Annual Symposium"
  },
  {
    "objectID": "publication/2021-05-11-geospatial-analysis-pregnancy-outcomes/index.html",
    "href": "publication/2021-05-11-geospatial-analysis-pregnancy-outcomes/index.html",
    "title": "A Bayesian Hierarchical Modeling Framework for Geospatial Analysis of Adverse Pregnancy Outcomes",
    "section": "",
    "text": "Studying the determinants of adverse pregnancy outcomes like stillbirth and preterm birth is of considerable interest in epidemiology. Understanding the role of both individual and community risk factors for these outcomes is crucial for planning appropriate clinical and public health interventions. With this goal, we develop geospatial mixed effects logistic regression models for adverse pregnancy outcomes. Our models account for both spatial autocorrelation and heterogeneity between neighborhoods. To mitigate the low incidence of stillbirth and preterm births in our data, we explore using class rebalancing techniques to improve predictive power. To assess the informative value of the covariates in our models, we use posterior distributions of their coefficients to gauge how well they can be distinguished from zero. As a case study, we model stillbirth and preterm birth in the city of Philadelphia, incorporating both patient-level data from electronic health records (EHR) data and publicly available neighborhood data at the census tract level. We find that patient-level features like self-identified race and ethnicity were highly informative for both outcomes. Neighborhood-level factors were also informative, with poverty important for stillbirth and crime important for preterm birth. Finally, we identify the neighborhoods in Philadelphia at highest risk of stillbirth and preterm birth."
  },
  {
    "objectID": "publication/2021-05-11-geospatial-analysis-pregnancy-outcomes/index.html#abstract",
    "href": "publication/2021-05-11-geospatial-analysis-pregnancy-outcomes/index.html#abstract",
    "title": "A Bayesian Hierarchical Modeling Framework for Geospatial Analysis of Adverse Pregnancy Outcomes",
    "section": "",
    "text": "Studying the determinants of adverse pregnancy outcomes like stillbirth and preterm birth is of considerable interest in epidemiology. Understanding the role of both individual and community risk factors for these outcomes is crucial for planning appropriate clinical and public health interventions. With this goal, we develop geospatial mixed effects logistic regression models for adverse pregnancy outcomes. Our models account for both spatial autocorrelation and heterogeneity between neighborhoods. To mitigate the low incidence of stillbirth and preterm births in our data, we explore using class rebalancing techniques to improve predictive power. To assess the informative value of the covariates in our models, we use posterior distributions of their coefficients to gauge how well they can be distinguished from zero. As a case study, we model stillbirth and preterm birth in the city of Philadelphia, incorporating both patient-level data from electronic health records (EHR) data and publicly available neighborhood data at the census tract level. We find that patient-level features like self-identified race and ethnicity were highly informative for both outcomes. Neighborhood-level factors were also informative, with poverty important for stillbirth and crime important for preterm birth. Finally, we identify the neighborhoods in Philadelphia at highest risk of stillbirth and preterm birth."
  },
  {
    "objectID": "publication/2022-06-07-mwas-multiple-birth/index.html",
    "href": "publication/2022-06-07-mwas-multiple-birth/index.html",
    "title": "Medication-Wide Association Study Using Electronic Health Record Data of Prescription Medication Exposure and Multifetal Pregnancies: Retrospective Study",
    "section": "",
    "text": "Background. Medication-wide association studies (MWAS) have been applied to assess the risk of individual prescription use and a wide range of health outcomes, including cancer, acute myocardial infarction, acute liver failure, acute renal failure, and upper gastrointestinal ulcers. Current literature on the use of preconception and periconception medication and its association with the risk of multiple gestation pregnancies (eg, monozygotic and dizygotic) is largely based on assisted reproductive technology (ART) cohorts. However, among non-ART pregnancies, it is unknown whether other medications increase the risk of multifetal pregnancies.\nObjective. This study aimed to investigate the risk of multiple gestational births (eg., twins and triplets) following preconception and periconception exposure to prescription medications in patients who delivered at Penn Medicine.\nMethods. We used electronic health record data between 2010 and 2017 on patients who delivered babies at Penn Medicine, a health care system in the Greater Philadelphia area. We explored 3 logistic regression models: model 1 (no adjustment); model 2 (adjustment for maternal age); and model 3—our final logistic regression model (adjustment for maternal age, ART use, and infertility diagnosis). In all models, multiple births (MBs) were our outcome of interest (binary outcome), and each medication was assessed separately as a binary variable. To assess our MWAS model performance, we defined ART medications as our gold standard, given that these medications are known to increase the risk of MB.\nResults. Of the 63,334 distinct deliveries in our cohort, only 1877 pregnancies (2.96%) were prescribed any medication during the preconception and first trimester period. Of the 123 medications prescribed, we found 26 (21.1%) medications associated with MB (using nominal P values) and 10 (8.1%) medications associated with MB (using Bonferroni adjustment) in fully adjusted model 3. We found that our model 3 algorithm had an accuracy of 85% (using nominal P values) and 89% (using Bonferroni-adjusted P values).\nConclusions. Our work demonstrates the opportunities in applying the MWAS approach with electronic health record data to explore associations between preconception and periconception medication exposure and the risk of MB while identifying novel candidate medications for further study. Overall, we found 3 novel medications linked with MB that could be explored in further work; this demonstrates the potential of our method to be used for hypothesis generation.\n\n\n\nA graphical overview of the medication-wide association study analyses on multiple birth."
  },
  {
    "objectID": "publication/2022-06-07-mwas-multiple-birth/index.html#abstract",
    "href": "publication/2022-06-07-mwas-multiple-birth/index.html#abstract",
    "title": "Medication-Wide Association Study Using Electronic Health Record Data of Prescription Medication Exposure and Multifetal Pregnancies: Retrospective Study",
    "section": "",
    "text": "Background. Medication-wide association studies (MWAS) have been applied to assess the risk of individual prescription use and a wide range of health outcomes, including cancer, acute myocardial infarction, acute liver failure, acute renal failure, and upper gastrointestinal ulcers. Current literature on the use of preconception and periconception medication and its association with the risk of multiple gestation pregnancies (eg, monozygotic and dizygotic) is largely based on assisted reproductive technology (ART) cohorts. However, among non-ART pregnancies, it is unknown whether other medications increase the risk of multifetal pregnancies.\nObjective. This study aimed to investigate the risk of multiple gestational births (eg., twins and triplets) following preconception and periconception exposure to prescription medications in patients who delivered at Penn Medicine.\nMethods. We used electronic health record data between 2010 and 2017 on patients who delivered babies at Penn Medicine, a health care system in the Greater Philadelphia area. We explored 3 logistic regression models: model 1 (no adjustment); model 2 (adjustment for maternal age); and model 3—our final logistic regression model (adjustment for maternal age, ART use, and infertility diagnosis). In all models, multiple births (MBs) were our outcome of interest (binary outcome), and each medication was assessed separately as a binary variable. To assess our MWAS model performance, we defined ART medications as our gold standard, given that these medications are known to increase the risk of MB.\nResults. Of the 63,334 distinct deliveries in our cohort, only 1877 pregnancies (2.96%) were prescribed any medication during the preconception and first trimester period. Of the 123 medications prescribed, we found 26 (21.1%) medications associated with MB (using nominal P values) and 10 (8.1%) medications associated with MB (using Bonferroni adjustment) in fully adjusted model 3. We found that our model 3 algorithm had an accuracy of 85% (using nominal P values) and 89% (using Bonferroni-adjusted P values).\nConclusions. Our work demonstrates the opportunities in applying the MWAS approach with electronic health record data to explore associations between preconception and periconception medication exposure and the risk of MB while identifying novel candidate medications for further study. Overall, we found 3 novel medications linked with MB that could be explored in further work; this demonstrates the potential of our method to be used for hypothesis generation.\n\n\n\nA graphical overview of the medication-wide association study analyses on multiple birth."
  },
  {
    "objectID": "publication/2021-08-11-ehr-environmental-exposure-pfas/index.html",
    "href": "publication/2021-08-11-ehr-environmental-exposure-pfas/index.html",
    "title": "Harnessing electronic health records to study emerging environmental disasters: a proof of concept with perfluoroalkyl substances (PFAS)",
    "section": "",
    "text": "Environmental disasters are anthropogenic catastrophic events that affect health. Famous disasters include the Seveso disaster and the Fukushima-Daiichi nuclear meltdown, which had disastrous health consequences. Traditional methods for studying environmental disasters are costly and time-intensive. We propose the use of electronic health records (EHR) and informatics methods to study the health effects of emergent environmental disasters in a cost-effective manner. An emergent environmental disaster is exposure to perfluoroalkyl substances (PFAS) in the Philadelphia area. Penn Medicine (PennMed) comprises multiple hospitals and facilities within the Philadelphia Metropolitan area, including over three thousand PFAS-exposed women living in one of the highest PFAS exposure areas nationwide. We developed a high-throughput method that utilizes only EHR data to evaluate the disease risk in this heavily exposed population. We replicated all five disease/conditions implicated by PFAS exposure, including hypercholesterolemia, thyroid disease, proteinuria, kidney disease and colitis, either directly or via closely related diagnoses. Using EHRs coupled with informatics enables the health impacts of environmental disasters to be more easily studied in large cohorts versus traditional methods that rely on interviews and expensive serum-based testing. By reducing cost and increasing the diversity of individuals included in studies, we can overcome many of the hurdles faced by previous studies, including a lack of racial and ethnic diversity. This proof-of-concept study confirms that EHRs can be used to study human health and disease impacts of environmental disasters and produces equivalent disease-exposure knowledge to prospective epidemiology studies while remaining cost-effective.\n\n\n\nHorsham-Warminster-Warrington area PFAS exposure timeline"
  },
  {
    "objectID": "publication/2021-08-11-ehr-environmental-exposure-pfas/index.html#abstract",
    "href": "publication/2021-08-11-ehr-environmental-exposure-pfas/index.html#abstract",
    "title": "Harnessing electronic health records to study emerging environmental disasters: a proof of concept with perfluoroalkyl substances (PFAS)",
    "section": "",
    "text": "Environmental disasters are anthropogenic catastrophic events that affect health. Famous disasters include the Seveso disaster and the Fukushima-Daiichi nuclear meltdown, which had disastrous health consequences. Traditional methods for studying environmental disasters are costly and time-intensive. We propose the use of electronic health records (EHR) and informatics methods to study the health effects of emergent environmental disasters in a cost-effective manner. An emergent environmental disaster is exposure to perfluoroalkyl substances (PFAS) in the Philadelphia area. Penn Medicine (PennMed) comprises multiple hospitals and facilities within the Philadelphia Metropolitan area, including over three thousand PFAS-exposed women living in one of the highest PFAS exposure areas nationwide. We developed a high-throughput method that utilizes only EHR data to evaluate the disease risk in this heavily exposed population. We replicated all five disease/conditions implicated by PFAS exposure, including hypercholesterolemia, thyroid disease, proteinuria, kidney disease and colitis, either directly or via closely related diagnoses. Using EHRs coupled with informatics enables the health impacts of environmental disasters to be more easily studied in large cohorts versus traditional methods that rely on interviews and expensive serum-based testing. By reducing cost and increasing the diversity of individuals included in studies, we can overcome many of the hurdles faced by previous studies, including a lack of racial and ethnic diversity. This proof-of-concept study confirms that EHRs can be used to study human health and disease impacts of environmental disasters and produces equivalent disease-exposure knowledge to prospective epidemiology studies while remaining cost-effective.\n\n\n\nHorsham-Warminster-Warrington area PFAS exposure timeline"
  },
  {
    "objectID": "publication/2020-03-05-climate-change-menarche/index.html",
    "href": "publication/2020-03-05-climate-change-menarche/index.html",
    "title": "A Systematic Literature Review of Factors Affecting the Timing of Menarche: The Potential for Climate Change to Impact Women’s Health",
    "section": "",
    "text": "Menarche is the first occurrence of a woman’s menstruation, an event that symbolizes reproductive capacity and the transition from childhood into womanhood. The global average age for menarche is 12 years and this has been declining in recent years. Many factors that affect the timing menarche in girls could be affected by climate change. A systematic literature review was performed regarding the timing of menarche and four publication databases were interrogated: EMBASE, SCOPUS, PubMed, and Cochrane Reviews. Themes were identified from 112 articles and related to environmental causes of perturbations in menarche (either early or late), disease causes and consequences of perturbations, and social causes and consequences. Research from climatology was incorporated to describe how climate change events, including increased hurricanes, avalanches/mudslides/landslides, and extreme weather events could alter the age of menarche by disrupting food availability or via increased toxin/pollutant release. Overall, our review revealed that these perturbations in the timing of menarche are likely to increase the disease burden for women in four key areas: mental health, fertility-related conditions, cardiovascular disease, and bone health. In summary, the climate does have the potential to impact women’s health through perturbation in the timing of menarche and this, in turn, will affect women’s risk of disease in future."
  },
  {
    "objectID": "publication/2020-03-05-climate-change-menarche/index.html#abstract",
    "href": "publication/2020-03-05-climate-change-menarche/index.html#abstract",
    "title": "A Systematic Literature Review of Factors Affecting the Timing of Menarche: The Potential for Climate Change to Impact Women’s Health",
    "section": "",
    "text": "Menarche is the first occurrence of a woman’s menstruation, an event that symbolizes reproductive capacity and the transition from childhood into womanhood. The global average age for menarche is 12 years and this has been declining in recent years. Many factors that affect the timing menarche in girls could be affected by climate change. A systematic literature review was performed regarding the timing of menarche and four publication databases were interrogated: EMBASE, SCOPUS, PubMed, and Cochrane Reviews. Themes were identified from 112 articles and related to environmental causes of perturbations in menarche (either early or late), disease causes and consequences of perturbations, and social causes and consequences. Research from climatology was incorporated to describe how climate change events, including increased hurricanes, avalanches/mudslides/landslides, and extreme weather events could alter the age of menarche by disrupting food availability or via increased toxin/pollutant release. Overall, our review revealed that these perturbations in the timing of menarche are likely to increase the disease burden for women in four key areas: mental health, fertility-related conditions, cardiovascular disease, and bone health. In summary, the climate does have the potential to impact women’s health through perturbation in the timing of menarche and this, in turn, will affect women’s risk of disease in future."
  },
  {
    "objectID": "publication/2022-03-09-ppd-ontology/index.html",
    "href": "publication/2022-03-09-ppd-ontology/index.html",
    "title": "Design and Evaluation of a Postpartum Depression Ontology",
    "section": "",
    "text": "Objective. Postpartum depression (PPD) remains an understudied research area despite its high prevalence. The goal of this study is to develop an ontology to aid in the identification of patients with PPD and to enable future analyses with electronic health record (EHR) data.\nMethods. We used Protégé-OWL to construct a postpartum depression ontology (PDO) of relevant comorbidities, symptoms, treatments, and other items pertinent to the study and treatment of PPD.\nResults. The PDO identifies and visualizes the risk factor status of variables for PPD, including comorbidities, confounders, symptoms, and treatments. The PDO includes 734 classes, 13 object properties, and 4,844 individuals. We also linked known and potential risk factors to their respective codes in the International Classification of Diseases versions 9 and 10 that would be useful in structured EHR data analyses. The representation and usefulness of the PDO was assessed using a task-based patient case study approach, involving 10 PPD case studies. Final evaluation of the ontology yielded 86.4% coverage of PPD symptoms, treatments, and risk factors. This demonstrates strong coverage of the PDO for the PPD domain.\nConclusion. The PDO will enable future researchers to study PPD using EHR data as it contains important information with regard to structured (e.g., billing codes) and unstructured data (e.g., synonyms of symptoms not coded in EHRs). The PDO is publicly available through the National Center for Biomedical Ontology (NCBO) BioPortal (https://bioportal.bioontology.org/ontologies/PARTUMDO) which will enable other informaticists to utilize the PDO to study PPD in other populations.\n\n\n\nA graphical overview of the Postpartum Depression Ontology Superclasses and Direct Subclasses of the Ontology."
  },
  {
    "objectID": "publication/2022-03-09-ppd-ontology/index.html#abstract",
    "href": "publication/2022-03-09-ppd-ontology/index.html#abstract",
    "title": "Design and Evaluation of a Postpartum Depression Ontology",
    "section": "",
    "text": "Objective. Postpartum depression (PPD) remains an understudied research area despite its high prevalence. The goal of this study is to develop an ontology to aid in the identification of patients with PPD and to enable future analyses with electronic health record (EHR) data.\nMethods. We used Protégé-OWL to construct a postpartum depression ontology (PDO) of relevant comorbidities, symptoms, treatments, and other items pertinent to the study and treatment of PPD.\nResults. The PDO identifies and visualizes the risk factor status of variables for PPD, including comorbidities, confounders, symptoms, and treatments. The PDO includes 734 classes, 13 object properties, and 4,844 individuals. We also linked known and potential risk factors to their respective codes in the International Classification of Diseases versions 9 and 10 that would be useful in structured EHR data analyses. The representation and usefulness of the PDO was assessed using a task-based patient case study approach, involving 10 PPD case studies. Final evaluation of the ontology yielded 86.4% coverage of PPD symptoms, treatments, and risk factors. This demonstrates strong coverage of the PDO for the PPD domain.\nConclusion. The PDO will enable future researchers to study PPD using EHR data as it contains important information with regard to structured (e.g., billing codes) and unstructured data (e.g., synonyms of symptoms not coded in EHRs). The PDO is publicly available through the National Center for Biomedical Ontology (NCBO) BioPortal (https://bioportal.bioontology.org/ontologies/PARTUMDO) which will enable other informaticists to utilize the PDO to study PPD in other populations.\n\n\n\nA graphical overview of the Postpartum Depression Ontology Superclasses and Direct Subclasses of the Ontology."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Send me a message ",
    "section": "",
    "text": "Send me a message \nFeel free to use this form for collaboration inquiries, questions about blog content, and feedback on my packages. I will try to respond as soon as possible.\n  \n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publication/2021-04-08-smm-individual-neighborhood/index.html",
    "href": "publication/2021-04-08-smm-individual-neighborhood/index.html",
    "title": "Individual-Level and Neighborhood-Level Risk Factors for Severe Maternal Morbidity",
    "section": "",
    "text": "Objective: To investigate the association between individual-level and neighborhood-level risk factors and severe maternal morbidity.\nMethods: This was a retrospective cohort study of all pregnancies delivered between 2010 and 2017 in the University of Pennsylvania Health System. International Classification of Diseases codes classified severe maternal morbidity according to the Centers for Disease Control and Prevention guidelines. Logistic regression modeling evaluated individual-level risk factors for severe maternal morbidity, such as maternal age and preeclampsia diagnosis. Additionally, we used spatial autoregressive modeling to assess Census-tract, neighborhood-level risk factors for severe maternal morbidity such as violent crime and poverty.\nResults: Overall, 63,334 pregnancies were included, with a severe maternal morbidity rate of 2.73%, or 272 deliveries with severe maternal morbidity per 10,000 delivery hospitalizations. In our multivariable model assessing individual-level risk factors for severe maternal morbidity, the magnitude of risk was highest for patients with a cesarean delivery (adjusted odds ratio [aOR] 3.50, 95% CI 3.15-3.89), stillbirth (aOR 4.60, 95% CI 3.31-6.24), and preeclampsia diagnosis (aOR 2.71, 95% CI 2.41-3.03). Identifying as White was associated with lower odds of severe maternal morbidity at delivery (aOR 0.73, 95% CI 0.61-0.87). In our final multivariable model assessing neighborhood-level risk factors for severe maternal morbidity, the rate of severe maternal morbidity increased by 2.4% (95% CI 0.37-4.4%) with every 10% increase in the percentage of individuals in a Census tract who identified as Black or African American when accounting for the number of violent crimes and percentage of people identifying as White.\nConclusion: Both individual-level and neighborhood-level risk factors were associated with severe maternal morbidity. These factors may contribute to rising severe maternal morbidity rates in the United States. Better characterization of risk factors for severe maternal morbidity is imperative for the design of clinical and public health interventions seeking to lower rates of severe maternal morbidity and maternal mortality."
  },
  {
    "objectID": "publication/2021-04-08-smm-individual-neighborhood/index.html#abstract",
    "href": "publication/2021-04-08-smm-individual-neighborhood/index.html#abstract",
    "title": "Individual-Level and Neighborhood-Level Risk Factors for Severe Maternal Morbidity",
    "section": "",
    "text": "Objective: To investigate the association between individual-level and neighborhood-level risk factors and severe maternal morbidity.\nMethods: This was a retrospective cohort study of all pregnancies delivered between 2010 and 2017 in the University of Pennsylvania Health System. International Classification of Diseases codes classified severe maternal morbidity according to the Centers for Disease Control and Prevention guidelines. Logistic regression modeling evaluated individual-level risk factors for severe maternal morbidity, such as maternal age and preeclampsia diagnosis. Additionally, we used spatial autoregressive modeling to assess Census-tract, neighborhood-level risk factors for severe maternal morbidity such as violent crime and poverty.\nResults: Overall, 63,334 pregnancies were included, with a severe maternal morbidity rate of 2.73%, or 272 deliveries with severe maternal morbidity per 10,000 delivery hospitalizations. In our multivariable model assessing individual-level risk factors for severe maternal morbidity, the magnitude of risk was highest for patients with a cesarean delivery (adjusted odds ratio [aOR] 3.50, 95% CI 3.15-3.89), stillbirth (aOR 4.60, 95% CI 3.31-6.24), and preeclampsia diagnosis (aOR 2.71, 95% CI 2.41-3.03). Identifying as White was associated with lower odds of severe maternal morbidity at delivery (aOR 0.73, 95% CI 0.61-0.87). In our final multivariable model assessing neighborhood-level risk factors for severe maternal morbidity, the rate of severe maternal morbidity increased by 2.4% (95% CI 0.37-4.4%) with every 10% increase in the percentage of individuals in a Census tract who identified as Black or African American when accounting for the number of violent crimes and percentage of people identifying as White.\nConclusion: Both individual-level and neighborhood-level risk factors were associated with severe maternal morbidity. These factors may contribute to rising severe maternal morbidity rates in the United States. Better characterization of risk factors for severe maternal morbidity is imperative for the design of clinical and public health interventions seeking to lower rates of severe maternal morbidity and maternal mortality."
  },
  {
    "objectID": "publication/2018-05-01-dissertation/index.html",
    "href": "publication/2018-05-01-dissertation/index.html",
    "title": "Characterization of Type I Collagen and Osteoblast Response to Mechanical Loading",
    "section": "",
    "text": "Bone is a composite material made up of an inorganic (hydroxyapatite mineral) phase, a proteinaceous organic phase, and water. Comprising 90% of bone’s organic phase, type I collagen is the most abundant protein in the human body. Both hydroxyapatite and collagen contribute to bone mechanical properties, and because bone is a hierarchical material, changes in properties of either phase can influence bulk mechanical properties of the tissue and bone structure. Type I collagen in bone is synthesized by osteoblasts as a helical structure formed from three polypeptide chains of amino acids. These molecules are staggered into an array and the resulting collagen fibrils are stabilized by crosslinks. Enzymatic crosslinking can be limited by compounds such as β-aminopropionitrile (BAPN) and result in a crosslink deficiency characterizing a disease known as lathyrism. BAPN acts by irreversibly binding to the active site of the lysyl oxidase enzyme, blocking the formation of new crosslinks and the maturation of pre-existing immature crosslinks. Understanding how changes in bone properties on a cellular level transcend levels of bone hierarchy provides an opportunity to detect or diagnose bone disease before disease-related changes are expressed at the organ or tissue level. This dissertation studies the in vitro effect of BAPN-induced enzymatic crosslink reduction on osteoblast-produced collagen nanostructure, mechanical properties, crosslink ratio, and expression of genes related to type I collagen synthesis and crosslinking. The work also explores the effect of mechanical loading via applied substrate strain on these properties to investigate its potential compensatory impact."
  },
  {
    "objectID": "publication/2018-05-01-dissertation/index.html#abstract",
    "href": "publication/2018-05-01-dissertation/index.html#abstract",
    "title": "Characterization of Type I Collagen and Osteoblast Response to Mechanical Loading",
    "section": "",
    "text": "Bone is a composite material made up of an inorganic (hydroxyapatite mineral) phase, a proteinaceous organic phase, and water. Comprising 90% of bone’s organic phase, type I collagen is the most abundant protein in the human body. Both hydroxyapatite and collagen contribute to bone mechanical properties, and because bone is a hierarchical material, changes in properties of either phase can influence bulk mechanical properties of the tissue and bone structure. Type I collagen in bone is synthesized by osteoblasts as a helical structure formed from three polypeptide chains of amino acids. These molecules are staggered into an array and the resulting collagen fibrils are stabilized by crosslinks. Enzymatic crosslinking can be limited by compounds such as β-aminopropionitrile (BAPN) and result in a crosslink deficiency characterizing a disease known as lathyrism. BAPN acts by irreversibly binding to the active site of the lysyl oxidase enzyme, blocking the formation of new crosslinks and the maturation of pre-existing immature crosslinks. Understanding how changes in bone properties on a cellular level transcend levels of bone hierarchy provides an opportunity to detect or diagnose bone disease before disease-related changes are expressed at the organ or tissue level. This dissertation studies the in vitro effect of BAPN-induced enzymatic crosslink reduction on osteoblast-produced collagen nanostructure, mechanical properties, crosslink ratio, and expression of genes related to type I collagen synthesis and crosslinking. The work also explores the effect of mechanical loading via applied substrate strain on these properties to investigate its potential compensatory impact."
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Code sharing increases citations, but remains uncommon\n\n\n\n\n\n\n\nCode sharing\n\n\nData sharing\n\n\nOpen science\n\n\n \n\n\n\n\nChristopher E. Doughty, Paul Efren Santos Andrade, Luna Lei, George Barbosa, Brad Boyle, Matiss Castorena, Brian Enquist, Xiao Feng, Jamie Kass, Hannah Owens, Daniel Park, Andrea Paz, Gonzalo Pinilla-Buitrago, Cory Merow, Adam Wilson\n\n\n\n\n\n\n\n\nTropical tree mortality has increased with rising atmospheric water stress\n\n\n\n\n\n\n\nClimate-change ecology\n\n\nCommunity ecology\n\n\nForest ecology\n\n\nPopulation dynamics\n\n\nTropical ecology\n\n\n \n\n\n\n\nMay 18, 2022\n\n\nDavid Bauman, Claire Fortunel, Guillaume Delhaye, Yadvinder Malhi, Lucas A. Cernusak, Lisa Patrick Bentley, Sami W. Rifai, Jesús Aguirre-Gutiérrez, Imma Oliveras Menor, Oliver L. Phillips, Brandon E. McNellis, Matt Bradford, Susan G. W. Laurance, Michael F. Hutchinson, Raymond Dempsey, Paul Efren Santos Andrade, Hugo R. Ninantay-Rivera, Jimmy R. Chambi Paucar, Sean M. McMahon\n\n\n\n\n\n\n\n\nTropical tree growth sensitivity to climate is driven by species intrinsic growth rate and leaf traits\n\n\n\n\n\n\n\nClimate anomalies\n\n\nClimate change\n\n\nFunctional traits\n\n\nTree vital rates\n\n\nForest ecology\n\n\nVapour pressure deficit (VPD)\n\n\n \n\n\n\n\nNov 6, 2021\n\n\nDavid Bauman, Claire Fortunel, Lucas A. Cernusak, Lisa Patrick Bentley, Sean M. McMahon, Sami W. Rifai, Jesús Aguirre-Gutiérrez, Imma Oliveras Menor, Matt Bradford, Susan G. W. Laurance, Guillaume Delhaye, Michael F. Hutchinson, Raymond Dempsey, Brandon E. McNellis, Paul Efren Santos Andrade, Hugo R. Ninantay-Rivera, Jimmy R. Chambi Paucar, Oliver L. Phillips, Yadvinder Malhi\n\n\n\n\n\n\n\n\nReduced tree density and basal area in Andean forests are associated with bamboo dominance\n\n\n\n\n\n\n\nCarbon dynamics\n\n\nForest structure\n\n\nMonitoring plots\n\n\nPlant interactions\n\n\nTropical forests\n\n\n \n\n\n\n\nJan 15, 2021\n\n\nBelen Fadrique, Paul Efren Santos Andrade, William Farfan-Rios, Norma Salinas, Miles Silman, Kenneth J. Feeley\n\n\n\n\n\n\n\n\nTropical forest leaves may darken in response to climate change\n\n\n\n\n\n\n\nSpectroscopy\n\n\nNIR\n\n\nForest structure\n\n\nTropical forests\n\n\n \n\n\n\n\nNov 18, 2018\n\n\nChristopher E. Doughty, Paul Efren Santos Andrade, Alexander Shenkin, Gregory R. Goldsmith, Lisa P. Bentley, Benjamin Blonder, Sandra Díaz, Norma Salinas, Brian J. Enquist, Roberta E. Martin, Gregory P. Asner, Yadvinder Malhi\n\n\n\n\n\n\n\n\nCan Leaf Spectroscopy Predict Leaf and Forest Traits Along a Peruvian Tropical Forest Elevation Gradient?\n\n\n\n\n\n\n\nLeaf Spectroscopy\n\n\nLeaf traits\n\n\nForest structure\n\n\nTropical forests\n\n\n \n\n\n\n\nOct 26, 2017\n\n\nChristopher E. Doughty, Paul Efren Santos Andrade, G. R. Goldsmith, B. Blonder, A. Shenkin, L. P. Bentley, C. Chavana-Bryant, W. Huaraca-Huasco, S. Díaz, N. Salinas, B. J. Enquist, R. Martin, G. P. Asner, Y. Malhi\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "publication/2020-12-10-csections-emergency-admissions/index.html",
    "href": "publication/2020-12-10-csections-emergency-admissions/index.html",
    "title": "Not All C-sections Are the Same: Investigating Emergency vs. Elective C-section Deliveries as an Adverse Pregnancy Outcome",
    "section": "",
    "text": "Session: Advanced Methods for Big Data Analytics in Women’s Health"
  },
  {
    "objectID": "publication/2020-12-10-csections-emergency-admissions/index.html#abstract",
    "href": "publication/2020-12-10-csections-emergency-admissions/index.html#abstract",
    "title": "Not All C-sections Are the Same: Investigating Emergency vs. Elective C-section Deliveries as an Adverse Pregnancy Outcome",
    "section": "Abstract",
    "text": "Abstract\nElectronic Health Records (EHR) contain detailed information about a patient’s medical history and can be helpful in understanding clinical outcomes among populations generally underrepresented in research, including pregnant individuals. A cesarean delivery is a clinical outcome often considered in studies as an adverse pregnancy outcome, when in reality there are circumstances in which a cesarean delivery is considered the safest or best choice given the patient’s medical history, situation, and comfort. Rather than consider all cesarean deliveries to be negative outcomes, it is important to examine other risk factors that may contribute to a cesarean delivery being an adverse event. Looking at emergency admissions can be a useful way to ascertain whether or not a cesarean delivery is part of an adverse event. This study utilizes EHR data from Penn Medicine to assess patient characteristics and pregnancy-related conditions as risk factors for an emergency admission at the time of delivery. After adjusting for pregnancy number and cesarean number for each patient, preterm birth increased risk of an emergency admission, and patients younger than 25, or identifying as Black/African American, Asian, or Other/Mixed, had an increased risk. Later pregnancies and repeat cesareans decreased the risk of an emergency delivery, and White, Hispanic, and Native Hawaiian/Pacific Islander patients were at decreased risk. The same risk factors and trends were found among cesarean deliveries, except that Asian patients did not have an increased risk, and Native Hawaiian/Pacific Islander patients did not have a reduced risk in this group.\n\n\n\n2021 PSB Poster (letter)"
  },
  {
    "objectID": "publication/2016-11-06-bapn-morphology/index.html",
    "href": "publication/2016-11-06-bapn-morphology/index.html",
    "title": "β-Aminopropionitrile-Induced Reduction in Enzymatic Crosslinking Causes In Vitro Changes in Collagen Morphology and Molecular Composition",
    "section": "",
    "text": "Type I collag een morphology can be characterized using fibril D-spacing, a metric which describes the periodicity of repeating bands of gap and overlap regions of collagen molecules arranged into collagen fibrils. This fibrillar structure is stabilized by enzymatic crosslinks initiated by lysyl oxidase (LOX), a step which can be disrupted using β-aminopropionitrile (BAPN). Murine in vivo studies have confirmed effects of BAPN on collagen nanostructure and the objective of this study was to evaluate the mechanism of these effects in vitro by measuring D-spacing, evaluating the ratio of mature to immature crosslinks, and quantifying gene expression of type I collagen and LOX. Osteoblasts were cultured in complete media, and differentiated using ascorbic acid, in the presence or absence of 0.25mM BAPN-fumarate. The matrix produced was imaged using atomic force microscopy (AFM) and 2D Fast Fourier transforms were performed to extract D-spacing from individual fibrils. The experiment was repeated for quantitative reverse transcription polymerase chain reaction (qRT-PCR) and Fourier Transform infrared spectroscopy (FTIR) analyses. The D-spacing distribution of collagen produced in the presence of BAPN was shifted toward higher D-spacing values, indicating BAPN affects the morphology of collagen produced in vitro, supporting aforementioned in vivo experiments. In contrast, no difference in gene expression was found for any target gene, suggesting LOX inhibition does not upregulate the LOX gene to compensate for the reduction in aldehyde formation, or regulate expression of genes encoding type I collagen. Finally, the mature to immature crosslink ratio decreased with BAPN treatment and was linked to a reduction in peak percent area of mature crosslink hydroxylysylpyridinoline (HP). In conclusion, in vitro treatment of osteoblasts with low levels of BAPN did not induce changes in genes encoding LOX or type I collagen, but led to an increase in collagen D-spacing as well as a decrease in mature crosslinks."
  },
  {
    "objectID": "publication/2016-11-06-bapn-morphology/index.html#abstract",
    "href": "publication/2016-11-06-bapn-morphology/index.html#abstract",
    "title": "β-Aminopropionitrile-Induced Reduction in Enzymatic Crosslinking Causes In Vitro Changes in Collagen Morphology and Molecular Composition",
    "section": "",
    "text": "Type I collag een morphology can be characterized using fibril D-spacing, a metric which describes the periodicity of repeating bands of gap and overlap regions of collagen molecules arranged into collagen fibrils. This fibrillar structure is stabilized by enzymatic crosslinks initiated by lysyl oxidase (LOX), a step which can be disrupted using β-aminopropionitrile (BAPN). Murine in vivo studies have confirmed effects of BAPN on collagen nanostructure and the objective of this study was to evaluate the mechanism of these effects in vitro by measuring D-spacing, evaluating the ratio of mature to immature crosslinks, and quantifying gene expression of type I collagen and LOX. Osteoblasts were cultured in complete media, and differentiated using ascorbic acid, in the presence or absence of 0.25mM BAPN-fumarate. The matrix produced was imaged using atomic force microscopy (AFM) and 2D Fast Fourier transforms were performed to extract D-spacing from individual fibrils. The experiment was repeated for quantitative reverse transcription polymerase chain reaction (qRT-PCR) and Fourier Transform infrared spectroscopy (FTIR) analyses. The D-spacing distribution of collagen produced in the presence of BAPN was shifted toward higher D-spacing values, indicating BAPN affects the morphology of collagen produced in vitro, supporting aforementioned in vivo experiments. In contrast, no difference in gene expression was found for any target gene, suggesting LOX inhibition does not upregulate the LOX gene to compensate for the reduction in aldehyde formation, or regulate expression of genes encoding type I collagen. Finally, the mature to immature crosslink ratio decreased with BAPN treatment and was linked to a reduction in peak percent area of mature crosslink hydroxylysylpyridinoline (HP). In conclusion, in vitro treatment of osteoblasts with low levels of BAPN did not induce changes in genes encoding LOX or type I collagen, but led to an increase in collagen D-spacing as well as a decrease in mature crosslinks."
  },
  {
    "objectID": "publication/2019-12-01-bapn-substrate-strain/index.html",
    "href": "publication/2019-12-01-bapn-substrate-strain/index.html",
    "title": "Substrate Strain Mitigates Effects of β-Aminopropionitrile-Induced Reduction in Enzymatic Crosslinking",
    "section": "",
    "text": "Enzymatic crosslinks stabilize type I collagen and are catalyzed by lysyl oxidase (LOX), a step interrupted through β-aminopropionitrile (BAPN) exposure. This study evaluated dose-dependent effects of BAPN on osteoblast gene expression of type I collagen, LOX, and genes associated with crosslink formation. The second objective was to characterize collagen produced in vitro after exposure to BAPN, and to explore changes to collagen properties under continuous cyclical substrate strain. To evaluate dose-dependent effects, osteoblasts were exposed to a range of BAPN dosages (0–10 mM) for gene expression analysis and cell proliferation. Results showed significant upregulation of BMP-1, POST, and COL1A1 and change in cell proliferation. Results also showed that while the gene encoding LOX was unaffected by BAPN treatment, other genes related to LOX activation and matrix production were upregulated. For the loading study, the combined effects of BAPN and mechanical loading were assessed. Gene expression was quantified, atomic force microscopy was used to extract elastic properties of the collagen matrix, and Fourier Transform infrared spectroscopy was used to assess collagen secondary structure for enzymatic crosslinking analysis. BAPN upregulated BMP-1 in static samples and BAPN combined with mechanical loading downregulated LOX when compared to control-static samples. Results showed a higher indentation modulus in BAPN-loaded samples compared to control-loaded samples. Loading increased the mature-to-immature crosslink ratios in control samples, and BAPN increased the height ratio in static samples. In summary, effects of BAPN (upregulation of genes involved in crosslinking, mature/immature crosslinking ratios, upward trend in collagen elasticity) were mitigated by mechanical loading."
  },
  {
    "objectID": "publication/2019-12-01-bapn-substrate-strain/index.html#abstract",
    "href": "publication/2019-12-01-bapn-substrate-strain/index.html#abstract",
    "title": "Substrate Strain Mitigates Effects of β-Aminopropionitrile-Induced Reduction in Enzymatic Crosslinking",
    "section": "",
    "text": "Enzymatic crosslinks stabilize type I collagen and are catalyzed by lysyl oxidase (LOX), a step interrupted through β-aminopropionitrile (BAPN) exposure. This study evaluated dose-dependent effects of BAPN on osteoblast gene expression of type I collagen, LOX, and genes associated with crosslink formation. The second objective was to characterize collagen produced in vitro after exposure to BAPN, and to explore changes to collagen properties under continuous cyclical substrate strain. To evaluate dose-dependent effects, osteoblasts were exposed to a range of BAPN dosages (0–10 mM) for gene expression analysis and cell proliferation. Results showed significant upregulation of BMP-1, POST, and COL1A1 and change in cell proliferation. Results also showed that while the gene encoding LOX was unaffected by BAPN treatment, other genes related to LOX activation and matrix production were upregulated. For the loading study, the combined effects of BAPN and mechanical loading were assessed. Gene expression was quantified, atomic force microscopy was used to extract elastic properties of the collagen matrix, and Fourier Transform infrared spectroscopy was used to assess collagen secondary structure for enzymatic crosslinking analysis. BAPN upregulated BMP-1 in static samples and BAPN combined with mechanical loading downregulated LOX when compared to control-static samples. Results showed a higher indentation modulus in BAPN-loaded samples compared to control-loaded samples. Loading increased the mature-to-immature crosslink ratios in control samples, and BAPN increased the height ratio in static samples. In summary, effects of BAPN (upregulation of genes involved in crosslinking, mature/immature crosslinking ratios, upward trend in collagen elasticity) were mitigated by mechanical loading."
  },
  {
    "objectID": "publication/2022-07-21-ten-simple-rules-inclusive-conference/index.html",
    "href": "publication/2022-07-21-ten-simple-rules-inclusive-conference/index.html",
    "title": "Ten simple rules to host an inclusive conference",
    "section": "",
    "text": "Conferences are spaces to meet and network within and across academic and technical fields, learn about new advances, and share our work. They can help define career paths and create long-lasting collaborations and opportunities. However, these opportunities are not equal for all. This article introduces 10 simple rules to host an inclusive conference based on the authors’ recent experience organizing the 2021 edition of the useR! statistical computing conference, which attracted a broad range of participants from academia, industry, government, and the nonprofit sector. Coming from different backgrounds, career stages, and even continents, we embraced the challenge of organizing a high-quality virtual conference in the context of the Coronavirus Disease 2019 (COVID-19) pandemic and making it a kind, inclusive, and accessible experience for as many people as possible. The rules result from our lessons learned before, during, and after the organization of the conference. They have been written mainly for potential organizers and selection committees of conferences and contain multiple practical tips to help a variety of events become more accessible and inclusive. We see this as a starting point for conversations and efforts towards building more inclusive conferences across the world. * Translated versions of the English abstract and the list of rules are available in 10 languages in S1 Text: Arabic, French, German, Italian, Japanese, Korean, Portuguese, Spanish, Tamil, and Thai.\n\n\n\nSchematic diagram of the rules organized in 3 groups: foundation (Rules 1 to 3), design (Rules 4 to 9), and continuity (Rule 10)."
  },
  {
    "objectID": "publication/2022-07-21-ten-simple-rules-inclusive-conference/index.html#abstract",
    "href": "publication/2022-07-21-ten-simple-rules-inclusive-conference/index.html#abstract",
    "title": "Ten simple rules to host an inclusive conference",
    "section": "",
    "text": "Conferences are spaces to meet and network within and across academic and technical fields, learn about new advances, and share our work. They can help define career paths and create long-lasting collaborations and opportunities. However, these opportunities are not equal for all. This article introduces 10 simple rules to host an inclusive conference based on the authors’ recent experience organizing the 2021 edition of the useR! statistical computing conference, which attracted a broad range of participants from academia, industry, government, and the nonprofit sector. Coming from different backgrounds, career stages, and even continents, we embraced the challenge of organizing a high-quality virtual conference in the context of the Coronavirus Disease 2019 (COVID-19) pandemic and making it a kind, inclusive, and accessible experience for as many people as possible. The rules result from our lessons learned before, during, and after the organization of the conference. They have been written mainly for potential organizers and selection committees of conferences and contain multiple practical tips to help a variety of events become more accessible and inclusive. We see this as a starting point for conversations and efforts towards building more inclusive conferences across the world. * Translated versions of the English abstract and the list of rules are available in 10 languages in S1 Text: Arabic, French, German, Italian, Japanese, Korean, Portuguese, Spanish, Tamil, and Thai.\n\n\n\nSchematic diagram of the rules organized in 3 groups: foundation (Rules 1 to 3), design (Rules 4 to 9), and continuity (Rule 10)."
  },
  {
    "objectID": "publication/2019-11-20-amia-annual-symposium/index.html",
    "href": "publication/2019-11-20-amia-annual-symposium/index.html",
    "title": "Investigating Pregnancy-Related Health Outcomes Among Patients with Sickle Cell Disease and Linking with Health Disparities",
    "section": "",
    "text": "Poster presented at the AMIA 2019 Annual Symposium\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paul Efren, Blgo.",
    "section": "",
    "text": "I’m a passionate plant ecologist and a dedicated R programming enthusiast.My research revolves around the fascinating world of plant traits and innovative code development. In my coding projects, I strive to establish a meaningful connection between open-source tools and effective data management, aiming to support fellow researchers in their endeavors.\nMore about me →\n\n    \n    \n  \n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "",
    "text": "February 12th, 2021 was Greg Wilson’s last day at RStudio. This means he is no longer running the instructor training program, so the future of the program is unclear. You may want to contact traininginstructor@rstudio.com with any specific questions.\nYou can also jump down in this blog post to Teaching resources to find a consolidated list of materials previously taught as part of the RStudio certification program, in addition to some related resources (thanks to Yanina for adding to this list!).\nLastly, you may want to consider becoming certified through The Carpentries. Yanina Bellini Saibene (@yabellini) and Dorris Scott (@Dorris_Scott) are a couple of RStudio instructors that have also been certified through The Carpentries."
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#asking-around",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#asking-around",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Asking around",
    "text": "Asking around\nIn the interest of making an informed decision, I asked a few instructors to share a bit about their experience with the certification process before I made the commitment.\n\n🇺🇸 Stephan Kadauke is a colleague I met through the Children’s Hospital of Philadelphia R User group. Specifically at an Intro to Machine Learning with the Tidyverse workshop led by Alison Hill.\n\nI met these wonderful people through the R-Ladies Global Slack workspace:\n\n🇦🇷 Yanina Bellini Saibene shared her experience with this process in her blog post Obtaining RStudio certification. A shared path.\n🇧🇴 🇳🇱 Paloma Rojas-Saunero shared her view as a Ph.D. candidate with experience teaching R in R-Ladies workshops and as a teaching assistant in biostatistics courses.\n🇰🇪 Shelmith Kariuki speaks to her experience in her blog post The RStudio Certification Process.\n\n\nHere’s what folks had to say!\nHighlights are shared with the authors’ permission and my gratitude to them.\n\nWhat made you want to pursue the certification?\n\nOne, it gave me incentive to read R for Data Science cover to cover which I had been meaning to do for a while. Two, I’m teaching R and the certification gives me some gravitas to do that. Three, it forced me to critically think about cognitive load theory, concept mapping, and lesson development, all of which are super useful when you actually have to develop lessons. –Stephan\n\n\nI pursued the certification for a few reasons. First because I am really enthusiastic about teaching and I love the tidyverse, so I wanted to find opportunities of teaching it outside my university. Second because I feel that RStudio is growing so fast on teaching materials that sharing with other instructors would be a great way to always keep updated. And last but not least, I was also encouraged by the idea of going through the process with Yanina Bellini Saibene and other amazing women as she discusses in her blog post. –Paloma\n\n\n\nWhat did you think of the training itself, particularly the pedagogical aspects?\n\nThe training is top notch. In my mind, Greg Wilson is the #1 authority when it comes to teaching programming and R in particular. –Stephan\n\n\nI think the best part is to take and learn the pedagogical aspect. Greg Wilson is an awesome trainer and you will love as a student all the tools and techniques that he teaches you. For learning this, the course is already worth it. All the content is more developed in the book Teaching Tech Together by Greg Wilson if you want to look it over. –Yanina\n\n\nAbout the training, it was mind blowing, it changed my view of teaching not only programming but everything. Greg’s book is amazing and the way he teaches is outstanding. I learned so much. –Paloma\n\n\n\nHave you been able to implement what you learned in some way either at work or elsewhere?\n\nYes! As you probably saw, I’m teaching the intro to R course for R/Medicine. And I’ve been teaching a similar course to doctors, in addition to some other teaching sessions that I’ve done for the CHOP R User group and/or medical resident teaching. –Stephan\n\n\nYes! Immediately. I delivered 3 in-person courses after training and more than 15 courses on-line using all the pedagogical tools. I also co-founded Metadocencia where we share practical tools to help teachers teach online (volunteer-run and free) using these principles learned in the training. I also use some of the tools for my work as the chief of a research group for some meetings and identifying the target of some of our development (especially concept maps and learner personas). –Yanina\n\n\nI think I use what I learned so far even when I am preparing my work presentations, when developing any type of class, event, book club, etc. Overall it was an experience that made me reflect a lot on how teaching is usually done, how is my teaching so far and an inspiration about the teacher I want to be. –Paloma"
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#attending-the-mir-panel",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#attending-the-mir-panel",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Attending the MiR panel",
    "text": "Attending the MiR panel\nIf you read the questions and responses above you might have noticed one character in this story that I haven’t introduced yet. His name is Greg Wilson. I tend to take such strong endorsements with a spoonful of skepticism and (spoiler alert) I’m happy to say I fully agree with everything that was said by those above. In that spirit, I’ll add my own glowing recommendation:\n\nGreg Wilson is truly a programming pedagogy expert, and an incredibly kind human being. I’m grateful to have learned from him within the context of the certification, and appreciate being able to continue learning from his example in a variety of other contexts.\n\nNevertheless, I’m glad I got to do a gut-check beforehand when I attended the RStudio Instructor Certification Panel hosted by the MiR Community and facilitated by Dorris Scott and Danielle Smalls-Perkins. You’ll notice Yanina and Shelmith were two of the panelists! Hearing from this panel was the last piece of the puzzle I needed to feel like pursuing the certification was an enthusiastic yes."
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#teaching-exam",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#teaching-exam",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Teaching exam",
    "text": "Teaching exam\nI relied on the instructor training materials and my in-class notes to prepare my demonstration lesson and study for the written component of the teaching exam. For anything that didn’t fully sink in, I consulted Teaching Tech Together.\nYou can find all materials for my teaching demonstration on GitHub. They include the slides below and this R Markdown file that I used to incorporate live coding into the lesson.\nI found an excellent reference for a demonstration lesson in Florencia D’Andrea’s post Two examples of iteration with purrr - Class for the RStudio certification.\nIt comes highly recommended that someone take a look at each one of our lessons before it ships out because it seems it’s common not to realize we’ve packed too much in! Yanina was kind enough to sit through my lesson as a learner and provided fantastic feedback. Some of the things I was able to work on before my teaching exam included providing context for the lesson at the beginning (asking the learner to download the file, introducing the lesson in the context of a workshop, etc.) and talking through all my key strokes during the live coding portions. Thanks again Yani!"
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#tidyverse-exam",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#tidyverse-exam",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Tidyverse exam",
    "text": "Tidyverse exam\nLucky for me the MiR Community organized some study sessions specifically for preparing for the Tidyverse exam! Dorris and I met regularly to discuss our approach to the sample exam (v2.0) and present chapters of R for Data Science that we weren’t as comfortable with. Yanina joined us for some of the sessions to lend us her expertise and provide tips and tricks!\nOne of the recommendations Yanina made was to explore the RStudio Primers for any topic we wanted to practice. For me that meant iteration using purrr’s map functions. After the iteration primer and the companion R for Data Science chapter, I felt like I could iterate all day every day.\nIn real life, both the written portion of the teaching exam and the Tidyverse exam are very much like the sample exams provided on the RStudio Education blog. You can find those here:\n\nSample exam v1.0 (February 2020): Instructor Certification Exams\nSample exam v2.0 (August 2020): More sample exams\n\nIf you’re a member of the MiR Community and like the idea of studying with some structure and friendly accountability, join us for the study group! And if you’re not, you can learn more about joining MiR as a member or ally here."
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#teaching-resources",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#teaching-resources",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Teaching resources",
    "text": "Teaching resources\n\n Slides for the instructor training course • Greg Wilson\n Teaching Tech Together • Greg Wilson\n Teaching R and Data Science with RStudio • Mine Çetinkaya-Rundel\n Teaching in Production • Alison Hill\n Teaching Online at Short Notice - RStudio 2020 • Greg Wilson\n Sharing on Short Notice - RStudio 2020 • Alison Hill & Desirée De Leon\n Evidence Based Teaching: What We Know and How to Use It - EuroSciPy 2015 • Greg Wilson\n rstudio-education/r4ds-instructors: Instructors’ Guide to accompany “R for Data Science” • RStudio Education\n Cursos cortos para enseñar online • MetaDocencia\n Flattening the leaRning curve: Teaching R online during COVID-19 • Brendan Cullen"
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#experiences-with-the-certification-process",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#experiences-with-the-certification-process",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Experiences with the certification process",
    "text": "Experiences with the certification process\n\nYanina Bellini Saibene – Obtaining RStudio certification. A shared path\nShelmith Kariuki – The RStudio Certification Process\nTed Laderas – My Experience with RStudio Instructor Training\nRayna Harris – A Review: RStudio Teaching Certification Course\nBrendan Cullen – Reflections on RStudio Instructor Training\nYuqi Liao – Getting Certified as an RStudio Instructor\nBeatriz Milz – Certificação da RStudio\nRohan Alexander – In Appreciation of Greg Wilson"
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#teaching-exam-examples",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#teaching-exam-examples",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Teaching exam examples",
    "text": "Teaching exam examples\n\nSilvia Canelón\n\nLesson slides: Using lubridate to work with time intervals\nGitHub repo\n\nYanina Bellini Saibene\n\nLesson materials: Concept map and formative assessments\nLesson slides: Código en R Markdown\nGitHub repo\n\nPaloma Rojas-Saunero\n\nLesson overview\nLesson slides: Tidy data\nLesson script: Tidyr: Reshape\n\nFlorencia D’Andrea\n\nLesson slides: Iteration with purrr package for automatized file management\nBlog post: Two examples of iteration with purrr - Class for the RStudio certification\nGitHub repo\n\nBeatriz Milz\n\nLesson slides: Adding figures in R Markdown\nGitHub repo\n\nLaurie Baker\n\nLesson slides\nGitHub repo\n\nCorrado Lanera\n\nLesson slides: (Meta)data texting in {ggplot2}\nGitHub repo\n\nBrendan Cullen\n\nLesson slides: Column-wise operations with dplyr: Old and New\nGitHub repo\n\nAdi Sarid\n\nGitHub repo: Exercise on purrr\n\nDavid John Baker\n\nLesson slides: Learn to Pivot!\nGitHub repo\n\nYuqi Liao\n\nLesson slides: Creating animated visualizations in R\nGitHub repo\n\nLuis Verde\n\nGitHub repo"
  },
  {
    "objectID": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#tidyverse-sample-exam-solutions",
    "href": "blog/2020-10-07-rstudio-instructor-certification-tidyverse/index.html#tidyverse-sample-exam-solutions",
    "title": "Becoming certified as an RStudio Tidyverse Instructor",
    "section": "Tidyverse sample exam solutions",
    "text": "Tidyverse sample exam solutions\n\nSilvia Canelón – August 2020 sample exam (v2.0)\nBrendan Cullen – August 2020 sample exam (v2.0)\nMarly Gotti – February 2020 sample exam (v1.0)\nEzekiel Adebayo Ogundepo – August 2020 sample exam (v2.0)"
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html",
    "href": "blog/2023-09-29-hello-quarto/index.html",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "",
    "text": "In 2019 I created my first website using blogdown and the Hugo Academic theme, and in 2021 I migrated that site to the Hugo Apéro theme created by Alison Hill. I even documented that trek in great detail 😅 to provide something resembling a paved path for others to use on their journey adopting this beautiful and functional theme. Over the past two years, my Apéro site became a wonderful place to cultivate a digital garden1. Now the time has come to say thank you, say goodbye, and travel down a new path. Enter Quarto."
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#goodbye-hugo-apéro",
    "href": "blog/2023-09-29-hello-quarto/index.html#goodbye-hugo-apéro",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "",
    "text": "In 2019 I created my first website using blogdown and the Hugo Academic theme, and in 2021 I migrated that site to the Hugo Apéro theme created by Alison Hill. I even documented that trek in great detail 😅 to provide something resembling a paved path for others to use on their journey adopting this beautiful and functional theme. Over the past two years, my Apéro site became a wonderful place to cultivate a digital garden1. Now the time has come to say thank you, say goodbye, and travel down a new path. Enter Quarto."
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#why-port-to-quarto",
    "href": "blog/2023-09-29-hello-quarto/index.html#why-port-to-quarto",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Why port to Quarto?",
    "text": "Why port to Quarto?\nQuarto is a new and powerful technical publishing system in active development, natively friendly to a variety of programming languages and IDEs. There are countless features available in Quarto that I was interested in having access to in my website, so I’ll list just my top five:\n\nFlexibility in design. Quarto offers options for website navigation (top navigation vs. side navigation), “about” pages like the postcard landing pages that a lot of folks use in their distill sites, listing pages that are generated from a list of documents, among others. These components would act like modules that I could combine to design a site that truly fit my needs.\nFlexibility in content layout. With Quarto I would gain the ability to organize content in ways I hadn’t even dreamt of. Tables and figures could be laid out across multiple columns and rows as sub-tables and sub-figures of a panel. How much of the page the content took up would be up to me. Content could even go in the margin! 🤩 \nFreezing computations. Quarto would give me the option to freeze posts containing executable code. This was huge for me, because updating my blogdown site always produced a little bit of anxiety. I used to worry that blogdown might try to re-render old R Markdown blog posts and break things! 😱\nSite search. With content spread out over blog posts, talks, publications, and projects, I wanted site visitors (including myself!) to be able to keyword search for something in particular. Tags and categories in blogdown would get me part of the way there, but Quarto offers built-in search capability.\nEmbedding computations. Ok this is a feature that I learned about a week ago at posit::conf(2023) from Mine Çetinkaya-Rundel’s talk “Reproducible Manuscripts with Quarto.” Mine demo’d pulling a table from a .qmd file and a figure from an .ipynb file into the same manuscript .qmd file, without having to execute the source code again! 👀 This is a game changer for anyone compiling results from computations performed in different files, and in my personal site I could see it coming in handy if I have a series of blog posts with outputs that I want to cross-reference. Currently we can only embed computations from Jupyter notebook files, but hopefully we’ll be able to embed .qmd computations in Quarto v1.4! 🤞🏽\nI mean, it’s pretty cool that content can go in the margin, right??  It could even be a picture: The book page reads verdere expedities which is Dutch for “further expeditions”"
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#the-cost-of-porting",
    "href": "blog/2023-09-29-hello-quarto/index.html#the-cost-of-porting",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "The cost of porting",
    "text": "The cost of porting\nGaining the aforementioned Quarto functionality did not come without a cost. To me, the biggest cost was the aesthetics – the Hugo Apéro theme is quite lovely! – but I customized my Quarto site enough to reflect the design aspects I loved the most. More on styling later. Some other costs included:\n\n\nLoss of the sidebar layout. The Apéro theme has options for adding a nicely styled sidebar that I had been using for my blog. It included a featured image and details about the individual post like tags, categories, length, etc. All of this information is available in Quarto posts, it’s just organized a little differently.\nExample of the sidebar\n\n\n\n\nLoss of Utterances comments. Any interactions with folks through Utteranc.es comments did not transfer over, and I’ll miss them. The few comments on my site have been very kind (see example below) and I’ve really enjoy reading them!\nExample of Utterances comments\n\n\n\n\n\n\n\nUpdate: Oct. 12, 2023\n\n\n\n\n\nThanks to Emily Riederer, I’m happy to report this is a non-issue! See her tips in the comments at the end of the post, or on GitHub.\n\n\n\n\nHugo Apéro collections. The Apéro theme had a way of bundling together posts that were related into a collection of posts. Garrick and I used this feature for our user!2021 xaringan workshop and it worked well to organize learning materials for different sections of our workshop. If I wanted something like this in the future, I would probably use Quarto’s sidebar navigation feature like Andrew Bray did in his Rmd to Quarto workshop for rstudio::conf(2022).\nName pronunciation on the About page. Apéro had a built-in option for adding an audio clip under your name that you could use to help people learn how your name is pronounced. I never used this feature myself, but if I wanted to, I could look to Emil Hvitfeldt’s site as an example.\nRedirects. The Apéro theme gave me the option to define the combination of date information and URL slugs for each of my posts, and I had them set up as year-slug (e.g., blog/2023-hello-quarto). Quarto does not offer that functionality and strictly creates URLs for posts based on the filepath (e.g., blog/2023-09-29-hello-quarto). In order to continue using my year-month-day-slug naming convention for my post folders, I had to set up redirects from the old URLs to the new ones so that there wouldn’t be broken links sprinkled throughout the internet. More on redirects later."
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#designing-the-site",
    "href": "blog/2023-09-29-hello-quarto/index.html#designing-the-site",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Designing the site",
    "text": "Designing the site\nApéro had posts in the content folder but with Quarto I could pull blog, talk, publication, and project subfolders into the root folder.\nStructure\n\n\nQuarto\nHugo Apéro\n\n\n\n.\n├── _quarto.yml\n├── about\n├── blog\n├── talk\n├── publication\n├── project\n├── index.qmd\n└── silvia.Rproj\n\n\n.\n├── content\n│   ├── _index.md\n│   ├── about    \n│   ├── blog      \n│   ├── project\n│   ├── publication\n│   └── talk\n├── config.toml\n├── netlify.toml\n├── index.Rmd\n└── silvia.Rproj\n\n\n\nAbout & listing pages\nI decided to use a combination of about pages and listing pages to replicate the site structure I enjoyed in my Apéro site. I used an about page layout for both my home and about pages, and listing pages to generate a list of blog posts, talks, publications, and projects. There was also some custom CSS involved, but we haven’t gotten to that part of the blog post yet wink 😉 Dear reader, it’s at this point in my blog post drafting that I start realizing that what was meant to be a brief post is turning into a novel (sigh)\nOne of the aspects I liked about my Apéro site was that the About page highlighted the most recent posts from the blog, talk, publication, and project groups. In my Quarto site, I achieved this by including a one-entry listing grid for each group on my About page.\n\n\n\n\n\n\nCSS tip\n\n\n\n\n\nThese listing cards looked great on a wide screen but as the screen got narrower (think mobile device) the listing cards just became narrower and narrower 😂. I took care of this by tweaking the CSS so that the cards would wrap.\n\n\nassets/about.css\n\n  /* wrap lately section */\n  #lately .grid {\n    display: flex;\n    flex-wrap: wrap;\n  }\n\n  /* listings */\n  #blog, #talks, #publications, #projects {\n    flex-basis: 100% !important;\n  }\n\n source code\n\n\n\nContact form\nThe last structural piece I wanted to recreate from Apéro was a contact form. I saw a great example on Michael McCarthy’s Tidy Tales blog, site and did a little repo-diving to get ideas on how to organize the content. I ended up using the Bootstrap CSS Grid to layout the content exactly how I wanted to. See what I mean about Quarto offering a lot of flexibility in content layout?\n\n\n\n\n\n\nBones of the contact page\n\n\n\n\n\n\n\ncontact.qmd\n\n\n&lt;!-- start grid --&gt;\n::: {.grid} \n\n&lt;!-- column for the body text --&gt;\n::: {.g-col-5}\n\n# Send me a note\n\n&lt; Body text&gt;\n\n&lt; HTML code for social media icons &gt;\n\n:::\n\n&lt;!-- column for spacing --&gt;\n::: {.g-col-1}\n:::\n\n&lt;!-- column for the form --&gt;\n::: {.g-col-6}\n\n&lt; HTML embed code provided by Formspree &gt;\n\n:::\n\n:::\n&lt;!-- end grid --&gt;\n\n source code"
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#adapting-old-posts",
    "href": "blog/2023-09-29-hello-quarto/index.html#adapting-old-posts",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Adapting old posts",
    "text": "Adapting old posts\nYAML fields\nOk so once I figured out how to incorporate all of the structural elements I wanted, I had to deal with the (relatively small) challenge of porting my old posts to Quarto. Quarto was designed to be compatible with existing R Markdown documents, but I didn’t actually want it trying to re-render old R Markdown files so I had Quarto ignore those completely2 and render the Markdown version of each post instead3. There were also a handful of YAML fields that I had to remove or modify to play nicely with Quarto:\n\nRemove layout: (e.g., layout: single-sidebar)\nRemove publishDate\n\nRemove lastUpdated\n\nRemove featured (e.g., featured: yes)\nMerge categories with tags and keep tags\n\nAdd image (e.g. image: featured.png)\nPartials\nIf you’ve used Apéro you might have come to love the cute button links created from the links field in the YAML, like the one at the top of this post, pointing to the Quarto docs.\nlinks:\n- icon: journal-text\n  name: Quarto Docs\n  url: https://quarto.org/docs/websites/\nWell, these button links are not ready-made by Quarto, but lucky for me (us!), Garrick figured out a way to bring them in right where we I need them. I repo-dived and found that he modified an HTML partial for title blocks, which controls the layout and styling of the post title, description, and tags, shown at the top of the page.\nThe original title block partial has sections controlling the placement and styling of text provided in the title, subtitle, authors, date, and abstract fields of the document YAML.\nAs an example, line 2 might read:If there’s something in the title field of the YAML, give it a level 1 heading and style it with CSS class .title\n\n\ngithub.com/quarto-dev/quarto-cli/src/resources/formats/html/pandoc/title-block.html\n\n&lt;header id=\"title-block-header\"&gt;\n$if(title)$&lt;h1 class=\"title\"&gt;$title$&lt;/h1&gt;$endif$\n$if(subtitle)$\n&lt;p class=\"subtitle\"&gt;$subtitle$&lt;/p&gt;\n$endif$\n$for(author)$\n&lt;p class=\"author\"&gt;$author$&lt;/p&gt;\n$endfor$\n\n$if(date)$\n&lt;p class=\"date\"&gt;$date$&lt;/p&gt;\n$endif$\n$if(abstract)$\n&lt;div class=\"abstract\"&gt;\n&lt;div class=\"abstract-title\"&gt;$abstract-title$&lt;/div&gt;\n$abstract$\n&lt;/div&gt;\n$endif$\n&lt;/header&gt;\n\nThe modified partial  includes some additional sections but I’ll break down the one for links.\nThere’s more styling going on in the modified partial, but starting on line 6, the structure for the link buttons is effectively:If the links field is populated in the YAML, create an HTML container styled by some CSS, and give it some specific HTML properies.Then, for each sub-item of the links field: create a hyperlink from the url sub-item, apply some CSS classes to create the button, add a Bootstrap icon defined by the icon sub-item, and finally add hyperlink text defined by the name sub-item.\n\n\n_partials/title-block-link-buttons/title-block.html\n\n&lt;header id=\"title-block-header\" class=\"quarto-title-block default page-columns\"&gt;\n...\n$if(links)$\n&lt;div class=\"mt-4 d-flex flex-row flex-wrap gap-1 justify-content-left justify-content-sm-start\" role=\"group\" aria-label=\"Links\"&gt;\n$for(links)$\n&lt;a href=\"$links.url$\" class=\"btn btn-secondary text-capitalize\"&gt;&lt;i class=\"bi bi-$links.icon$ me-1\"&gt;&lt;/i&gt;$links.name$&lt;/a&gt;\n$endfor$\n&lt;/div&gt;\n$endif$\n...\n&lt;/header&gt;"
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#styling-the-site",
    "href": "blog/2023-09-29-hello-quarto/index.html#styling-the-site",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Styling the site",
    "text": "Styling the site\nSCSS variables\nI may have published my brand new Quarto site in the fall of 2023, but I had actually taken my first go at porting it from Apéro in the fall of 2022. In an attempt to replicate the styling of my old site I tried to strong-arm my Quarto site into using tachyons4, and I wrote way too much custom CSS in the process 😩.\nSomething didn’t feel right about it, so I took a break and came back to my in-progress Quarto site several months later – outside of the Quarto release craze, and with the perspective that I wanted to work with the built-in theming capabilities, rather than against them 😌. This time around I leaned into the Bootstrap SCSS variables, and added custom styling as needed.\nTo be more specific, I created a custom theme 5, and populated it with definitions for some of the built-in Bootstrap Sass variables. If you poke around my custom theme, you will notice that some colors are defined by color variables that start with $spc-. These are variables pointing to colors that I’ve defined elsewhere, and they can be replaced with a hex code of your choosing! 🎨\n\n\nassets/silvia-theme.scss\n\n/*-- scss:defaults --*/\n\n// Colors\n$primary:                    $spc-primary-light;\n$secondary:                  $spc-secondary;\n\nThe Sass variables listed in the documentation got me pretty far, but there were still elements of my site using default colors and styling that I wanted to change. So I repo-dived to learn about additional Quarto Bootstrap variables and rules:\n\nBootstrap variables: https://github.com/quarto-dev/quarto-cli/blob/main/src/resources/formats/html/bootstrap/_bootstrap-variables.scss\n\nBootstrap rules: https://github.com/quarto-dev/quarto-cli/blob/main/src/resources/formats/html/bootstrap/_bootstrap-rules.scss\n\n\nTo give you an example, someone in the Quarto Discussions forum asked how to set the color of block quotes, figure captions, and outlines, something not defined in the documentation. One of the replies suggested using the browser’s inspector to find the CSS rule applied to the element in question, we’ll say the figure caption, and then looking at how Quarto implemented that rule. I’ll make that process a little more explicit here and use the figure caption as an example:\n\nUse the browser inspector to highlight a figure caption\nThe inspector will reveal that the element is styled by CSS class .figure-caption\nRun a keyword search for “figure-caption” in the Bootstrap variables file. This will yield no results 🤔\n\nRun the same keyword search in the Bootstrap rules file – success! We have a clue, which is that .figure-caption is styled by a variable named body-secondary 🔍\n.panel-caption,\n.figure-caption,\n.subfigure-caption,\n.table-caption,\nfigcaption.quarto-float-caption,\ncaption {\n  font-size: 0.9rem;\n  @include body-secondary;\n}\n\n\nDefine your own body-secondary color variable in your custom theme file\n\n\nassets/custom-theme.scss\n\n/*-- scss:defaults --*/\n\n$body-secondary: #6C757D;\n\n\nEnjoy the fruits of your labor as a CSS detective 🕵🏽‍♀️\nIndividual page styling\nIn some cases, I wanted to style individual pages differently than I was styling my entire site. Poking around Garrick’s repo (again!), I learned that you can reference CSS stylesheets in the YAML of any particular document. I used this approach to separately style my Home, About, and listing pages.\n\n\nindex.qmd\n\n\n\n::: {#hero-heading}\n\n\n\n\nindex.qmd  source code\n\n\nassets/index.css  source code\n\nBackground image for title blocks\nOk one of the aesthetic pieces I was excited to add to my Quarto site that I didn’t have in my old site is a featured image at the top of each post. I was inspired by Matt Worthington’s site while I was reading one of his excellent blog posts on a workflow for interactive maps in R – I loved that the title block included an image in the background! 🤩\nAfter some web inspecting and repo-diving in Matt’s website repo I came up with some styling that I could apply to the title block of each post. It took modifying the title block partial described earlier, and adding some CSS to my custom theme.\nFor styling that I wanted to apply uniformly to all post title blocks, I created a .figured-image CSS class in my custom theme:\nDon’t miss the code annotations for this code chunk! 👀 Such a cool feature!\n\n\nassets/custom-theme.scss\n\n  // style background image on posts\n  .featured-image {\n1    background-size: cover;\n2    background-position: center;\n3    color: white;\n4    box-shadow: inset 0 0 0 1000px rgba(0,38,66,0.75);\n  }\n\n\n1\n\nHave the image cover the background\n\n2\n\nCenter the image\n\n3\n\nMake the overlaying text color white\n\n4\n\nAdd shadowing that gives the appearance of a dark overlay\n\n\nAlright, but why didn’t I use a background-image property to define the image? If I define the background image in my .featured-image class then that is the image that’s going to show up in all of the posts across my site. I want the title block of each post to display the featured image belonging to each individual post. That’s where the partial comes back in.\n\n\n_partials/title-block-link-buttons/title-block.html\n\n&lt;div \nid=\"title-block-header-title\" \nclass=\"quarto-title page-columns page-full page-layout-full featured-image p-4\" \nstyle=\"background-image: url(featured.png), url(featured.jpg), url(../featured.jpg);\"&gt;\n...\n&lt;/div&gt;\n\nAlong with some other CSS classes, the div container for the title block is styled with the featured-image class defined earlier, using the class= property. In addition, the div itself is styled using the style= property, and that’s where I defined the background image. The partial will look for a featured image of some kind either in the same folder as the post, or in the parent folder, and display it in the background of the title block.\nThis is a hacky way of getting what I really want which is for the partial to check for a featured image in the order I specify and then just stop once it finds one. As it stands right now, the styling attempts to layer on all three images, with the first image on the top and the last image on the bottom. This means that I get a warning every time I preview or render my site because the partial can’t find one or two of the other urls, but I’m ok with that!\n\n\n\n\n\n\nExample warning\n\n\n\n/blog/2023-09-29-hello-quarto/featured.png (404: Not Found)"
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#setting-up-redirects",
    "href": "blog/2023-09-29-hello-quarto/index.html#setting-up-redirects",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Setting up redirects",
    "text": "Setting up redirects\nResources\nLast but not least, I can’t finish this blog post without talking about the necessary task of redirecting old Apéro URLs to new Quarto ones 6. Many thanks to Danielle Navarro and Tom Mock for documenting their solutions for automatically generating a _redirects file with each render of the site.\n\nDanielle’s blog post: Notes from a data witch - Porting a distill blog to quarto\n\nTom’s source code: index.qmd - The MockUp\n\n\nI adapted their code to (1) fit my site, which has multiple folders of posts that all need redirects, and (2) combine these automatically generated redirects with ones I had already defined manually. I placed this code within my home page index.qmd file  and I specified freeze: false in the YAML so that the code would run each time I rendered the site. The following sections will take a look at the code piece by piece.\nImporting manual redirects\nThe first step imports the manually-defined redirects that I had already been using in my old site. These redirects primarily have the task of redirecting my rbind.io domain to my custom domain silviacanelon.com.\n\n\n\nindex.qmd\n\nmanual_redirects &lt;-\n  readr::read_table(here::here(\"static\", \"_manualredirects.txt\"),\n                    col_names = FALSE) |&gt; \n  dplyr::mutate(redirect = paste0(X1, \" \", X2, \" \", X3))\n\nmanual_redirects &lt;- manual_redirects$redirect\n\nhead(manual_redirects)\n\n\n[1] \"https://silvia.rbind.io/authors/silvia/avatar.png https://silviacanelon.com/about/sidebar/avatar.png 301!\"\n[2] \"https://silvia.rbind.io/post/* https://silviacanelon.com/blog/:splat 301!\"                                \n[3] \"https://silvia.rbind.io/blog/* https://silviacanelon.com/blog/:splat 301!\"                                \n[4] \"https://silvia.rbind.io/talk/* https://silviacanelon.com/talk/:splat 301!\"                                \n[5] \"https://silvia.rbind.io/project/* https://silviacanelon.com/project/:splat 301!\"                          \n[6] \"http://silvia.rbind.io/* https://silviacanelon.com/:splat 301!\"                                           \n\n\nListing subdirectories\nThe next step defines a function that obtains a list of subdirectories, iterates it over the four groups of posts in my site (blog posts, talks, publications, and projects), and compiles the files into a data frame.\n\n\n\nindex.qmd\n\n# function: obtain list of post paths\nlist_paths &lt;- function(folder) {\n  posts &lt;-\n    list.dirs(\n    path = c(here::here(folder)),\n    full.names = FALSE,\n    recursive = FALSE\n    ) |&gt; \n    tibble::as_tibble_col(column_name = \"path\")  |&gt;\n    dplyr::mutate(folder = folder)\n}\n\n# define post folders\nfolders &lt;- c(\"blog\", \"project\", \"publication\", \"talk\")\n\n# list post paths by folder\nposts &lt;- purrr::map(folders, list_paths) |&gt; purrr::list_rbind()\n\nhead(posts)\n\n\n\n\n  \n\n\n\nDefining redirects\nThis next chunk removes the month and day from year-month-day-slug so that I’m left with shorter paths with the format year-slug, and uses these paths and the folders they are housed in to create redirects.\nThe resulting redirects will point the short year-slug link to this blog post:\nhttps://silviacanelon.com/blog/2023-hello-quarto\nTo the longer year-month-day-slug Quarto link to this post:\nhttps://silviacanelon.com/blog/2023-09-29-hello-quarto\n\n# extract short paths and create redirects\nposts &lt;- \n  posts |&gt; \n  dplyr::mutate(\n    # extract the year-slugs\n    short_path = stringr::str_remove(path, \"(?!\\\\d{4}-)\\\\d{2}-\\\\d{2}-(?!\\\\d)\"),\n    # create short paths\n    short_path = paste0(folder, \"/\", short_path),\n    # create lines to insert to a netlify _redirect file\n    redirects = paste0(\"/\", short_path, \" \", \"/\", folder, \"/\", path)\n    )\n\nhead(posts)\n\n\n\n  \n\n\n\nWriting redirects file\nThe last step takes the redirects from the data frame produced in the previous step, combines them with the manual redirects, and writes them to a new text file _redirects. This file is written into the _site folder where my Quarto site is rendered, and where Netlify will know to find it.\n\n\n\nindex.qmd\n\n# extract redirects\nredirects &lt;- posts$redirects\n\n# combine with manual redirects\nredirects_combined &lt;- c(manual_redirects, redirects)\n\n# write the _redirect file\nwriteLines(redirects_combined, here::here(\"_site\", \"_redirects\"))"
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#fin",
    "href": "blog/2023-09-29-hello-quarto/index.html#fin",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Fin",
    "text": "Fin\nNow that it’s been over a year since Quarto made its debut, there are so many wonderful blog posts walking folks through how to create a Quarto site. My hope is that the notes I took in this blog post help provide some stepping stones for folks making the transition from blogdown sites, and offer some ideas for what is possible with the flexibility of this new publishing framework. I, for one, am looking forward to growing my digital garden with Quarto tools 🌱."
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#acknowledgments",
    "href": "blog/2023-09-29-hello-quarto/index.html#acknowledgments",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nFeatured photo by Annelies Geneyn on Unsplash."
  },
  {
    "objectID": "blog/2023-09-29-hello-quarto/index.html#footnotes",
    "href": "blog/2023-09-29-hello-quarto/index.html#footnotes",
    "title": "Hello Quarto: Porting my Website from Hugo Apéro",
    "section": "Footnotes",
    "text": "Footnotes\n\nSee also Digital gardens let you cultivate your own little bit of the internet | MIT Technology Review↩︎\nBy adding an underscore prefix (e.g., _index.Rmarkdown)↩︎\nI.e., index.markdown↩︎\nA CSS design system used in the Hugo Apéro theme: https://tachyons.io↩︎\nFor more on custom theming, see Quarto - HTML Theming and Quarto - More About Quarto Themes↩︎\nDon’t skip this step! People will almost certainly have shared one or more of your posts somewhere on the internet, and it helps everyone if you can point them to the right spot↩︎"
  },
  {
    "objectID": "blog/2023-06-05-ccd-sips/index.html",
    "href": "blog/2023-06-05-ccd-sips/index.html",
    "title": "Philly Center City District Sips 2023: An Interactive Map",
    "section": "",
    "text": "Expand to see web analytics\n\n\n\n\n\n1,760 unique visitors in the United States visited this post 2,330 times between June 7th and August 30th, 2023!\n\n\n\n\nPhilly’s Center City District posted a list of restaurants and bars participating in Philly’s 2023 CCD Sips. CCD Sips is a series of summer Wednesday evenings (5-7pm) filled with happy hour specials, between June 7th and August 30th.\nI prefer to take in this information as a map instead of a list, so I scraped some information from the website and made one! You can click or tap on the circle map markers to see information about each restaurant/bar along with a direct link to their posted happy hour specials.\nCheck out the link at the top of this post for a larger version of the interactive map below, or take a look at CCD Sips maps from 2022 and 2019.\n\n\n\n\n\n\n\n Back to topReusehttps://creativecommons.org/licenses/by-sa/4.0/CitationFor attribution, please cite this work as:\nCanelón, Silvia. 2023. “Philly Center City District Sips 2023: An\nInteractive Map.” June 5, 2023. https://paulefrensa.rbind.io//blog/2023-06-05-ccd-sips."
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "",
    "text": "This post was featured in the RStudio Blog R Views under a revised title: Deploying xaringan Slides with GitHub Pages. The original title was “Deploying xaringan Slides: A Ten-Step GitHub Pages Workflow.” Other changes include an introductory paragraph and greater clarity in the “Choose your own adventure” section. Many thanks to Chief Editor Joe Rickert for a very encouraging and helpful editorial process! I am humbled by his note on R Views:\nSilvia’s post is a mini masterpiece of clear, concise writing that elucidates complex technology within the narrow context of explaining a single well-defined task. Silvia does not attempt to say everything she knows about the subject, and she resists digressions that might obscure the path she is laying out. It is an example of achieving clarity through saying less.\nThis post will guide you step-by-step through the process of creating an HTML xaringan slide deck and deploying it to the web for easy sharing with others. We will be using the xaringan package to build the slide deck, GitHub to help us host our slides for free with GitHub Pages, and the usethis package to help us out along the way. You will get the most out of this workflow if you are already familiar with R Markdown and GitHub, and if you have already connected RStudio (or your preferred IDE) to Git and GitHub.1 The post will not cover the nuts and bolts of xaringan or talk about slide design & customization, but you can find lots of learning resources listed at the end."
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#packages",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#packages",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Packages",
    "text": "Packages\nThis workflow was developed using:\n\n\n\nSoftware / package\nVersion\n\n\n\n\nR\n4.0.3\n\n\nRStudio\n1.4.1103\n\n\nxaringan\n0.19\n\n\nusethis\n2.0.0\n\n\n\ninstall.packages(\"xaringan\")\ninstall.packages(\"usethis\")"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#creating-your-xaringan-slide-deck",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#creating-your-xaringan-slide-deck",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Creating your xaringan slide deck",
    "text": "Creating your xaringan slide deck\n1. Create a new RStudio project for your presentation:\nusethis::create_project(\"filepath/for/your/presentation/repo-name\")\n\n📍 If you’re not sure where you are on your computer, check your working directory with getwd() and use it as a filepath reference point\n\n\n2. Create a xaringan deck using a xaringan template: File &gt; New File &gt; R Markdown &gt; From Template &gt; Ninja Presentation &gt; OK\n3. Delete what you don’t need and save your R Markdown file with whatever name you like. If you pick index.Rmd the live link you share at the end will be relatively short.\n4. Render HTML slides from the open Rmd file using xaringan’s infinite moon reader:\nxaringan::infinite_moon_reader()"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#initialize-version-control-with-git",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#initialize-version-control-with-git",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Initialize version control with git",
    "text": "Initialize version control with git\n5. Initialize version control of your slides with git:\nusethis::use_git()\nYou’ll be asked if you want to commit the files in your project (with the message “Initial commit”) and then if you want to restart to activate the Git pane. Say yes to both ✅\n\nNote: At the moment usethis names the primary branch “master” by default. Issue #1341 suggests the option to instead name it “main” is in the works.\n\n\n6. Connect your local project with a GitHub repo:\nusethis::use_github()\n\nYou could use the function argument private = TRUE to create a private GitHub repository. But you may have to remember to change the visibility before deploying to GitHub Pages.\n\n\n7. Your new GitHub repo with all of your xaringan project files will automatically open up in your browser\n\nRepo for the R-Ladies xaringan template: https://github.com/spcanelon/RLadies-xaringan-template"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#making-and-committing-changes-to-your-slides",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#making-and-committing-changes-to-your-slides",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Making and committing changes to your slides",
    "text": "Making and committing changes to your slides\n8. Edit your slides as you wish. Commit often! And then push to GitHub. Use the tools provided by the Git pane in RStudio, or use the following commands in the Terminal:\n# Step 1: Stage all modified files\ngit add .\n# Step 2: Describe the changes you made to your files\ngit commit -m \"&lt;A brief but descriptive commit message&gt;\"\n\nConsider writing a commit message that finishes the following sentence:2 “If applied, this commit will…” (e.g. “Change the slide theme,” “Add hello slide”)\n\n# Step 3: Push the changes to your GitHub repository\ngit push"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#deploying-your-slides",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#deploying-your-slides",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Deploying your slides",
    "text": "Deploying your slides\n9. When you’re ready to deploy your slides, you can use the usethis::use_github_pages() function which makes the process of deploying via GitHub Pages super easy. I recommend pointing branch to the name of your primary branch.\nusethis::use_github_pages(branch = \"master\")\n\nNote: Your repository must be public for your deployed slides to be available publicly, unless you have a paid GitHub account.\n\n\nAlso, you only need to follow this step once to deploy your slides to the web. As long as you remember to push to your repo any changes that you make to your slides (Rmd and HTML), GitHub Pages will know how to render them.\n\n\n10. Visit the link provided to see your newly deployed slides! 🚀Don’t panic if you don’t see them right away, sometimes it takes a little time. This is the link you will share with the world when you present. Notice it looks very similar to your GitHub repo link.\n\nLink to the R-Ladies xaringan template rendered slides: https://spcanelon.github.io/RLadies-xaringan-template"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#foundation",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#foundation",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Foundation",
    "text": "Foundation\n\nSharing Your Work with xaringan • Silvia Canelón – workshop site\nIntroducción al Paquete xaringan • Silvia Canelón – R-Ladies Meetup\nMaking Slides with R Markdown • Alison Hill – workshop slides\nPresentation Ninja • xaringan Official Document – package documentation\nChapter 7 xaringan Presentations • R Markdown: The Definitive Guide – book chapter"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#sharing-your-slides",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#sharing-your-slides",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Sharing your slides",
    "text": "Sharing your slides\n\nSharing Your xaringan Slides • Garrick Aden‑Buie – blog post\nFunctions For Building Xaringan Slides To Different Outputs • xaringanBuilder – package site\nSharing on Short Notice • Alison Hill & Desirée De Leon – video resource for deploying via Netlify"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#making-your-slides-extra-special",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#making-your-slides-extra-special",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Making your slides extra special",
    "text": "Making your slides extra special\n\nProfessional, Polished, Presentable • Garrick Aden‑Buie & Silvia Canelón • useR!2021 – site for an intermediate xaringan workshop\nHome • yihui/xaringan Wiki • GitHub – wiki of customizations for xaringan\nMaking Extra Great Slides • Garrick Aden‑Buie – talk & slides with xaringan overview and featuring CSS styling and xaringanthemer\nApplying design guidelines to slides with {xaringanthemer} • katie jolly – blog post\nA playground of extensions for xaringan • xaringanExtra – package site\nCustom xaringan CSS Themes • xaringanthemer – package site"
  },
  {
    "objectID": "blog/2021-03-16-deploying-xaringan-slides/index.html#footnotes",
    "href": "blog/2021-03-16-deploying-xaringan-slides/index.html#footnotes",
    "title": "Deploying xaringan Slides with GitHub Pages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChapter 12 Connect RStudio to Git and GitHub | Happy Git and GitHub for the useR↩︎\nHow to Write a Git Commit Message | Chris Beams↩︎"
  },
  {
    "objectID": "blog/2019-08-04-ccd-sips/index.html",
    "href": "blog/2019-08-04-ccd-sips/index.html",
    "title": "Philly Center City Sips 2019: An Interactive Map",
    "section": "",
    "text": "For a more recent map, check out Philly Center City District Sips 2022: An Interactive Map\n\nI used R to play around with webscraping and create an interactive map showing restaurants participating in Philly’s 2019 Center City District Sips. Center City Sips is a series of summer Wednesday evenings filled with happy hour specials.\nThis mini-project was motivated by two of my close friends’ frustration with the website shortcomings. The information was presented beautifully, but there was no map view! This made it difficult to figure out what happy hours you could check out nearby.\nThe map pop-ups could use information on the specials for each restaurant, but my newly acquired skills with the RSelenium and Leaflet packages weren’t quite up to the task!\n\n\n\n\n\n\n\n\n\n\n Back to topReusehttps://creativecommons.org/licenses/by-sa/4.0/CitationFor attribution, please cite this work as:\nCanelón, Silvia. 2019. “Philly Center City Sips 2019: An\nInteractive Map.” August 4, 2019. https://paulefrensa.rbind.io//blog/2019-08-04-ccd-sips."
  },
  {
    "objectID": "blog/2021-09-23-data-viz-a11y/index.html",
    "href": "blog/2021-09-23-data-viz-a11y/index.html",
    "title": "Resources for Data Viz Accessibility",
    "section": "",
    "text": "Updated on 2021-11-19 to include a link to a series of educational blog posts written by Mara Averick that cover {highcharter} and the accessibility module in detail. Thank you Mara!"
  },
  {
    "objectID": "blog/2021-09-23-data-viz-a11y/index.html#general-resources",
    "href": "blog/2021-09-23-data-viz-a11y/index.html#general-resources",
    "title": "Resources for Data Viz Accessibility",
    "section": "General resources",
    "text": "General resources\n\nNote: There is an extensive list of data viz accessibility resources at https://github.com/dataviza11y/resources/blob/main/README.md\nArticle by Doug Schepers that provides great background on data viz accessibility, explains how data visualization itself is assistive technology, and offers considerations for a variety of readers: Why Accessibility Is at the Heart of Data Visualization\nThe Chartability methodology helps data viz practitioners audit their data viz, and it’s language/tool-agnostic. Chartability was created by Frank Elavsky with input from the broader community and is the most thorough set of standards I’ve come across.\n\nØystein Moseng provides some basic practices to consider in the post 10 Guidelines for DataViz Accessibility. Among those included are these two which I don’t see covered as often:\n\nDo not rely on color alone to convey information. I’ve also heard this referred to as “color independence” and in some cases as “dual-encoding” of information\n\nPrefer simple, familiar visualizations over complex novelties.\n\n\n\n\nTalk at the Data Visualization Society’s Outlier conference earlier this year, by Frank Elavsky, Larene Le Gassick, and Sarah Fossheim: Are your visualizations excluding people?\n\nGuidelines by Amy Cesal on how to write alt text for data visualizations: Writing Alt Text for Data Visualization\n\n\n\nPost by Lisa Charlotte Muth on how to pick colors for your data viz that everyone can appreciate: How to pick more beautiful colors for your data visualizations - Datawrapper Blog\n\n\n\nPost by Gareth Ford Williams from The Readability Group about how to make more informed font choices: A Guide to Understanding What Makes a Typeface Accessible\n\n\nRelated is a talk from The Readability Group sharing findings from a survey study about font preferences including 2000+ participants. Among these were participants with dyslexia characteristics and participants with poor near vision: Don’t Believe the Type!\n\n\n\n\n\nSarah Fossheim authored a post titled An intro to designing accessible data visualizations which uses real-word applications to demonstrate 10 different accessibility practices.\nSarah also provides a code-through showing how to make screenreader-friendly graphs using D3.js in their post How to create a screen reader accessible graph like Apple’s with D3.js\n\n\n\n\n\n\nAmber Thomas provides an example of how to make scrollytales more accessible in a piece created with Ofunne Amaka for The Pudding: The Naked Truth\n\n\n\nI'm excited to release this project for many reasons, but one among them is that I introduced lots of new (to me) #a11y features. My favorite: the ability to turn off scrollytelling 💖 Would love to hear what folks think! pic.twitter.com/eDSQr9RjFe\n\n— Amber Thomas (@ProQuesAsker) March 25, 2021\n\n\n\n\nChris DeMartini has a series of blog posts documenting his journey auditing one of his public Tableau visualizations for accessibility:\n\nA Tableau Accessibility Journey\nFocus Order\nColor Contrast and Font Size\n\nKeyboard Accessibility"
  },
  {
    "objectID": "blog/2021-09-23-data-viz-a11y/index.html#r-resources",
    "href": "blog/2021-09-23-data-viz-a11y/index.html#r-resources",
    "title": "Resources for Data Viz Accessibility",
    "section": "R resources",
    "text": "R resources\n\n\nLiz Hare and I gave a talk earlier this year on alt text in data viz shared as a part of TidyTuesday on Twitter. After web-scraping alt text from TidyTuesday tweets we found that only 3% of data viz tweets had alt text to accompany them (over the first 3 years of the TidyTuesday project). Links to the video recording, slides, and resources at https://github.com/spcanelon/csvConf2021. The talk includes guidelines on what makes effective alt text for data viz (complementary to those Amy Cesal includes in her post).\n\n\n\nPost from RStudio on how to add alt text to visualizations produced in R Markdown using code chunk option fig.alt: New in knitr: Improved accessibility with image alt text. New in knitr v1.35: You can now add code chunk options inside the code chunk!\nExample updated thanks to a heads-up from Garrick:\n#| fig.cap: Bigger flippers, bigger bills \n#| fig.alt: Scatterplot of flipper length by bill length of 3 penguin species, where we show penguins with bigger flippers have bigger bills.\n\nggplot(data = penguins, aes(x = flipper_length_mm,\n                            y = bill_length_mm,\n                            color = species)) +\n  geom_point(aes(shape = species), alpha = 0.8) +\n  scale_color_manual(\n    values = c(\"darkorange\",\"purple\",\"cyan4\")) \n\n\nCode chunk adapted from the RStudio blog post\n\nThe previous example included fig.cap and fig.alt in the code chunk heading:\n\n\n\n\nBigger flippers, bigger bills\n\n\n\n\n\n\nThe ggpattern R package developed by Mike FC supports filling ggplot2 geometries with patterns. If used judiciously, patterns can help make visualizations more accessible by providing an additional way to encode information without relying on color. Below is one example using ggpattern v0.2.2:\n\nlibrary(ggpattern)\n\npenguinColors &lt;- c(\"darkorange\", \"purple\", \"cyan4\")\n\nggplot(penguins, aes(species)) +\ngeom_bar_pattern(aes(fill = species,\n                    pattern = species,\n                    pattern_color = species),\n  fill = penguinColors,\n  alpha = 0.1,\n  linewidth = 1,\n  color = penguinColors,\n  pattern_color = penguinColors,\n  pattern_fill = penguinColors,\n  pattern_spacing = 0.025\n) +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\n\nDifferent patterns mapped onto penguin species along with different colors\n\n\n\n\n\nThe Highcharter R package developed by Joshua Kunst adds interactivity to data viz using Highcharts JavaScript components designed with web accessibility in mind. The package has a learning curve, but lucky for us Mara Averick wrote an excellent series of blog posts on using the Highcharts accessibility module with {highcharter}."
  },
  {
    "objectID": "project/2021-01-01-useR-2021/index.html",
    "href": "project/2021-01-01-useR-2021/index.html",
    "title": "useR!2021 Cost Conversion Tool",
    "section": "",
    "text": "I created a cost conversion tool used for the useR!2021 Conference to adapt conference and sponsorship fees according to the country of residence. The cost conversion was done according to Gross Domestic Product (GDP) adjusted by Purchasing Power Parity (PPP) provided by the World Bank. We wanted conference attendees and sponsors from different parts of the world to be able to participate in useR! and believed that this is a fair approach. It reflects the reality that attendees and sponsors from “High income” countries have a different purchasing power than those from “Low income” countries. The income categories and cost conversions are listed in a data table created in R that attendees can use to search for their country and attendee status if applicable (i.e. Industry, Academia, Student). They can use a built-in search bar, or filter according to a specific column.\nYou can read more about purchasing power parities, price level indexes, and PPP-based expenditures in the May 2020 World Bank post New results from the International Comparison Program shed light on the size of the global economy.\nThe Global Income Groups listed in the fee tables below were obtained using data from the 2017 International Comparison Program (ICP) which you can read more about in the report Purchasing Power Parities and the Size of World Economies: Results from the 2017 International Comparison Program. The conversion factors were calculated using PPP-based GDP per capita for each Global Income Group using data from the ICP 2017 World Bank Database.\n\n\n\n Back to top"
  },
  {
    "objectID": "project/2020-11-03-xaringan-nhs-r/index.html#workshop",
    "href": "project/2020-11-03-xaringan-nhs-r/index.html#workshop",
    "title": "Sharing Your Work with xaringan",
    "section": "Workshop",
    "text": "Workshop\nDay 1 covers the nuts and bolts of creating presentation slides using xaringan and deploying them in HTML format for easy sharing with others.\nDay 2 covers how to take your slides to the next level with the xaringanExtra package and how to customize slides with CSS.\nThis workshop is designed for R users already familiar with R Markdown and GitHub."
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "ppendemic\n\n\n\nR package\n\n\nEndemic peruvian flora\n\n\nDiversity\n\n\n\n\nOct 22, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "project/2020-12-28-teaching-tech-together-es/index.html",
    "href": "project/2020-12-28-teaching-tech-together-es/index.html",
    "title": "Enseñar Tecnología en Comunidad",
    "section": "",
    "text": "Este proyecto llevó acabo la traducción al castellano del libro Teaching Tech Together escrito por Greg Wilson. El proyecto fue coordinado por mi amiga y colega Yanina Bellini Saibene.\nYo participé como traductora del capítulo “Motivation and Demotivation” y revisora del capítulo “Expertise and Memory.”\n\nSeptiembre de 2020 – Diciembre de 2020. Traducción colaborativa al castellano del libro “Teaching Tech Together. How to create and deliver lessons that work and build a teaching community around them” de Greg Wilson (2019, Taylor & Francis, ISBN 978-0-367-35328-5, https://teachtogether.tech/). Coordinación general de la traducción: Yanina Bellini Saibene; Edición general: Yanina Bellini Saibene y Natalia Morandeira. Participación como traductora y revisora de capítulos. Más información y grupo de traductoras: https://github.com/gvwilson/teachtogether.tech/blob/master/es/README.md"
  },
  {
    "objectID": "project/2020-12-28-teaching-tech-together-es/index.html#proyecto",
    "href": "project/2020-12-28-teaching-tech-together-es/index.html#proyecto",
    "title": "Enseñar Tecnología en Comunidad",
    "section": "",
    "text": "Este proyecto llevó acabo la traducción al castellano del libro Teaching Tech Together escrito por Greg Wilson. El proyecto fue coordinado por mi amiga y colega Yanina Bellini Saibene.\nYo participé como traductora del capítulo “Motivation and Demotivation” y revisora del capítulo “Expertise and Memory.”\n\nSeptiembre de 2020 – Diciembre de 2020. Traducción colaborativa al castellano del libro “Teaching Tech Together. How to create and deliver lessons that work and build a teaching community around them” de Greg Wilson (2019, Taylor & Francis, ISBN 978-0-367-35328-5, https://teachtogether.tech/). Coordinación general de la traducción: Yanina Bellini Saibene; Edición general: Yanina Bellini Saibene y Natalia Morandeira. Participación como traductora y revisora de capítulos. Más información y grupo de traductoras: https://github.com/gvwilson/teachtogether.tech/blob/master/es/README.md"
  },
  {
    "objectID": "project/2020-10-29-nhsrtheme/index.html",
    "href": "project/2020-10-29-nhsrtheme/index.html",
    "title": "ppendemic",
    "section": "",
    "text": "Introducing a novel and updated database showcasing Peru’s endemic plants. This meticulously compiled and revised botanical collection encompasses a remarkable assemblage of over 7249 distinct species."
  },
  {
    "objectID": "project/2020-10-29-nhsrtheme/index.html#theme",
    "href": "project/2020-10-29-nhsrtheme/index.html#theme",
    "title": "ppendemic",
    "section": "",
    "text": "In preparation for a xaringan workshop I created for the NHS-R 2020 Conference, I designed a custom CSS theme with input from the NHS-R Community that followed the NHS identity guidelines. This theme was also contributed to the xaringan package developed by Yihui Xie."
  },
  {
    "objectID": "accessibility.html",
    "href": "accessibility.html",
    "title": "Feedback",
    "section": "",
    "text": "I value your input on my site’s content and the materials I’ve provided. If you encounter any issues, please don’t hesitate to reach out through my contact form, and I’ll respond promptly.\nThank you for visiting my site and taking the time to read this page. Your feedback is immensely appreciated!\nTravel back to the home page.\n\n\n\n Back to top"
  },
  {
    "objectID": "accessibility.html#feedback",
    "href": "accessibility.html#feedback",
    "title": "Feedback",
    "section": "Feedback",
    "text": "Feedback\nI welcome any feedback on the content of my site and the materials I’ve made available. If you come across any issues, please feel free to let me know via my contact form, and I’ll do my best to respond promptly.\nThank you for visiting my site and for taking the time to read this page . Your input is greatly appreciated!”"
  },
  {
    "objectID": "accessibility.html#accessibility-practices",
    "href": "accessibility.html#accessibility-practices",
    "title": "Accessibility commitment",
    "section": "Accessibility practices",
    "text": "Accessibility practices\nThis site has been designed with the following features in mind:\n\nA color palette that meets WCAG 2.1 AA standards for contrast\nAlternative text for all informative images\nReadable font faces, specifically to avoid impostor letter shapes and mirroring. This site primarily uses Red Hat Text which is freely available and has been found to be relatively accessible to users with dyslexic traits and poor near vision.1\nA table of contents in the blog post sidebar for easier navigation\n\nI’m aware there is much more to inclusive and accessible design and I’m learning how to implement better accessibility practices in the content I create. I’m not a web developer but my plan is to regularly audit my site for accessibility failures and learn how to remedy them as best I can. I’ll be using the following evaluation tools:\n\nWAVE Web Accessibility Evaluation Tool\n\nIn addition, I’ll document the failures and any corrective actions as issues in the GitHub repository housing the files used to build this site."
  },
  {
    "objectID": "accessibility.html#work-related-to-accessibility",
    "href": "accessibility.html#work-related-to-accessibility",
    "title": "Accessibility commitment",
    "section": "Work related to accessibility",
    "text": "Work related to accessibility\n\nTalk: Revealing Room for Improvement in Accessibility within a Social Media Data Visualization Learning Community\nBlog post: Resources for Data Viz Accessibility\nBlog post: Auditing for Web Accessibility"
  },
  {
    "objectID": "accessibility.html#footnotes",
    "href": "accessibility.html#footnotes",
    "title": "Accessibility commitment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more on font accessibility, I recommend the talk Don’t Believe the Type! by The Readability Group.↩︎"
  },
  {
    "objectID": "talk/2023-03-09-penn-community-scholars/index.html",
    "href": "talk/2023-03-09-penn-community-scholars/index.html",
    "title": "Data Analytics 101",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "talk/2022-11-10-thinking-big/index.html",
    "href": "talk/2022-11-10-thinking-big/index.html",
    "title": "Thinking Big with Maps in R",
    "section": "",
    "text": "The road to map-making can take some unexpected twists and turns when you scale up an interactive map from hundreds of features to hundreds of thousands. This talk is a story about creative spatial data wrangling, powerful R packages, and a heroic #RStats community.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talk/2022-04-27-dbei-research-day/index.html",
    "href": "talk/2022-04-27-dbei-research-day/index.html",
    "title": "Exploring Traumatic Brain Injury Mechanisms and Severity Using Electronic Health Records",
    "section": "",
    "text": "This abstract was one of 10 selected for a flashtalk presentation. And the flashtalk received an award! 🎉\nThe talk recording will be available to the public on April 28, 2022. The accompanying (PowerPoint) slides are available to download using the “Slides” link above.\n2022 CCEB & DBEI Research Day Poster (letter)"
  },
  {
    "objectID": "talk/2022-04-27-dbei-research-day/index.html#abstract",
    "href": "talk/2022-04-27-dbei-research-day/index.html#abstract",
    "title": "Exploring Traumatic Brain Injury Mechanisms and Severity Using Electronic Health Records",
    "section": "Abstract",
    "text": "Abstract\n\nObjective\nOften referred to as the “silent epidemic,” Traumatic Brain Injury (TBI) is a public health concern contributing to disability and death worldwide. Our study describes a cohort of TBI patients within the Penn Medicine health system and the distribution of TBI injury mechanisms and severity.\n\n\nMethods\nWe obtained Electronic Health Record data for 1,060,100 female patients treated at Penn Medicine inpatient or outpatient clinics from 2010-2017. We identified patients with TBI diagnoses using ICD-9/10 codes and the Disease Control and Prevention (CDC) and Department of Defense (DOD) definitions for TBI and TBI severity. The CDC/DOD codes for TBI were then manually annotated with mechanisms of TBI injury (e.g. Sport Mechanism of Injury, Collision or Crash, Foreign Body Object). The highest TBI severity category was noted for each patient and defined, in increasing severity, as “Mild,” “Moderate,” “Severe,” or “Penetrating.” We report the distribution of TBI mechanisms and severity among this patient population.\n\n\nResults\nThere were 4,392 patients with a total of 9,800 TBI diagnoses. The majority of diagnoses in the cohort were Mild (5,704; 58%), followed by Moderate (3,840; 39%), Severe (173; 1.8%), and Penetrating (83; 0.8%). The following are the six most common mechanisms observed to contribute to TBI diagnoses: “Injury,” “Mechanism of Injury,” “Accidents,” “Physical Accidents,” “Fall Mechanism of Injury,” and “Traffic Vehicle Accident.”"
  },
  {
    "objectID": "talk/2021-02-18-r-teaching-panel/index.html",
    "href": "talk/2021-02-18-r-teaching-panel/index.html",
    "title": "From Learn-R to Teach-R: An Expert Panel on Effective R Instruction",
    "section": "",
    "text": "This panel and discussion event will be focused on R instruction and education. As a learner, what are some recommended resources for developing my R skills? As an instructor, how can I make my materials and presentations more effective? What does data science education look like in 2021? What types of careers are out there in R education? How can I gain experience teaching R?\nPanelists:\nAma Nyame-Mensah is a Postdoctoral Fellow and Research Associate in the School of Social Policy & Practice and the Graduate School of Education at the University of Pennsylvania. Her current work rethinks whether and how quantitative and computational methods can contribute to a more equitable understanding of marginalized children’s and families’ learning and development. Ama is passionate about empowering people to use data for social analysis and critique data and data technologies. When not working, Ama can be found tinkering with data and designing visualizations. Website: https://www.anyamemensah.com\nCass Wilkinson Saldaña helps learners, researchers, and communities to engage meaningfully (and critically) with data. They currently work as a Data Instructional Specialist at the Children’s Hospital of Philadelphia’s Research Institute, and teach introduction to R courses at Yeshiva University. Previously, Cass worked as the Social Science and Geospatial Data Librarian at Cornell University. As a data educator and data librarian, Cass participates and collaborates in open source ecosystems, especially in the domains of open science, digital scholarship, and civic data. They also strive to advocate for justice in worker and learner communities. Website: https://cassws.net\nSilvia Canelón is a postdoctoral research scientist in biomedical informatics at the University of Pennsylvania in Philadelphia. She uses data science in the public and population health fields and is particularly interested in leveraging electronic health record data to study reproductive health outcomes. Silvia is formally trained as a biomedical engineer and an RStudio Certified Tidyverse Instructor. She loves community organizing and is passionate about R education as a way to help build power in her communities."
  },
  {
    "objectID": "talk/2021-02-18-r-teaching-panel/index.html#description",
    "href": "talk/2021-02-18-r-teaching-panel/index.html#description",
    "title": "From Learn-R to Teach-R: An Expert Panel on Effective R Instruction",
    "section": "",
    "text": "This panel and discussion event will be focused on R instruction and education. As a learner, what are some recommended resources for developing my R skills? As an instructor, how can I make my materials and presentations more effective? What does data science education look like in 2021? What types of careers are out there in R education? How can I gain experience teaching R?\nPanelists:\nAma Nyame-Mensah is a Postdoctoral Fellow and Research Associate in the School of Social Policy & Practice and the Graduate School of Education at the University of Pennsylvania. Her current work rethinks whether and how quantitative and computational methods can contribute to a more equitable understanding of marginalized children’s and families’ learning and development. Ama is passionate about empowering people to use data for social analysis and critique data and data technologies. When not working, Ama can be found tinkering with data and designing visualizations. Website: https://www.anyamemensah.com\nCass Wilkinson Saldaña helps learners, researchers, and communities to engage meaningfully (and critically) with data. They currently work as a Data Instructional Specialist at the Children’s Hospital of Philadelphia’s Research Institute, and teach introduction to R courses at Yeshiva University. Previously, Cass worked as the Social Science and Geospatial Data Librarian at Cornell University. As a data educator and data librarian, Cass participates and collaborates in open source ecosystems, especially in the domains of open science, digital scholarship, and civic data. They also strive to advocate for justice in worker and learner communities. Website: https://cassws.net\nSilvia Canelón is a postdoctoral research scientist in biomedical informatics at the University of Pennsylvania in Philadelphia. She uses data science in the public and population health fields and is particularly interested in leveraging electronic health record data to study reproductive health outcomes. Silvia is formally trained as a biomedical engineer and an RStudio Certified Tidyverse Instructor. She loves community organizing and is passionate about R education as a way to help build power in her communities."
  },
  {
    "objectID": "talk/index.html",
    "href": "talk/index.html",
    "title": "Talks & workshops",
    "section": "",
    "text": "Evaluating the effect of sampling scale to quantify different components of plant biodiversity with imaging spectroscopy\n\n\n\n\n\n\n\nEcology\n\n\nSpectroscopy\n\n\nBiodiversity\n\n\n \n\n\n\n\nDec 14, 2021\n\n\nSandra M Duran, Nicola Falco, Paul Efren Santos Andrade, Jesus N Pinto Ledezma, Haruko M Wainwright, Heidi Steltzer, Eoin Brodie, Brian Joseph Enquist, Jeannine Cavender-Bares\n\n\n\n\n\n\n  \n\n\n\n\nAssessing how long-term shifts in species and functional composition have impacted carbon and water cycling the southern Rocky Mountains\n\n\n\n\n\n\n\nEcology\n\n\nClimate Change\n\n\n \n\n\n\n\nDec 14, 2020\n\n\nJulia Chacon-Labella, Connor Wilson, Lindsay Backhaus, Paul Efren Santos Andrade, Sandra Milena Duran, Alex Brummer, Amanda Henderson, Lorah Seltzer, Nicola Falco, Haruko M. Wainwright, Eoin Brodie, Charles F. Williams, Susan S. Hubbard, Vigdis Vandvik, Brian Joseph Enquist, Kenneth Hurst Williams\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talk/2020-08-31-tour-of-the-tidyverse/index.html",
    "href": "talk/2020-08-31-tour-of-the-tidyverse/index.html",
    "title": "An Antarctic Tour of the Tidyverse",
    "section": "",
    "text": "Learn how to explore and manipulate data in R with packages from the Tidyverse. We’ll introduce the eight core packages that make up the Tidyverse and use at least one function from each package while exploring a dataset on the migration of penguins.\n\n\n\n Back to top"
  },
  {
    "objectID": "talk/2022-02-11-tdw-ehr-lessons/index.html",
    "href": "talk/2022-02-11-tdw-ehr-lessons/index.html",
    "title": "Lessons Learned from the EHR",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "about/index.html#section",
    "href": "about/index.html#section",
    "title": "Paul Efren",
    "section": "…",
    "text": "…\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\nCurvas Rango-Abundancia\n\n\nNotas de Ecología de Comunidades Vegetales.\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\nEvaluating the effect of sampling scale to quantify different components of plant biodiversity with imaging spectroscopy\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\n\n\n\n\n\nCode sharing increases citations, but remains uncommon\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\nppendemic\n\n\nA Glimpse at the Diversity of Peru’s Endemic Plants\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →"
  },
  {
    "objectID": "publication/2020-12-12-agu-chacon-rmbl/index.html",
    "href": "publication/2020-12-12-agu-chacon-rmbl/index.html",
    "title": "Assessing how long-term shifts in species and functional composition have impacted carbon and water cycling the southern Rocky Mountains",
    "section": "",
    "text": "Alpine communities of the Rocky Mountains show a directional shift in temperature and vapor pressure deficit with increasingly hotter and drier growing seasons. In much of the Western and Southwestern United States, the growing season is characterized by an early drought that occurs after snowmelt and lasts until the start of the summer monsoon season. Climate change models predict an increase in the length and severity of this dry period, due to a reduction in precipitations, rise of temperatures, earlier snowmelt dates and shifts in the North American monsoon. Here we integrated several efforts to characterize ~70 years of changes in plant community diversity, functional traits, and ecosystem functioning. First, to investigate the patterns and causes of temporal changes in montane plant communities, we used a resampled dataset of 121 transects surveyed from 1948 to 1952 in the East River Basin near Crested Butte, Colorado. We combined these data, long-term climate data and another biodiversity dataset of ~ 15 years of community composition monitoring across a 1,000m elevational gradient. To understand how variability in the timing and strength of the early season drought, temperature, and VPD affects ecosystem carbon exchange through potential shifts in biodiversity and traits, we used a long-term ( ~ 17 year) dataset of peak carbon and water flux measures of vegetation along the same elevational gradient. We test the hypothesis that (i) changing climate, in particular temperature, and (ii) shifts in snowmelt date drive shifts in diversity of plant traits linked to temperature. We next assessed the hypothesis that (iii) shifts in these traits would influence ecosystem carbon and water exchange. Our results show strong shifts in species and trait composition with time and elevation. In the 17 years of monitoring ecosystem functioning we find a strong signal of increased temperatures and minimum VPD. Our results suggest that increasing VPD as well as shifts in plant trait composition together have large effects on the carbon and water budgets in Western mountain watersheds. As plant water use influences recharge rates and productivity in the watershed these results suggest that over the past ~70 years but especially in the last decade climate driven shifts in vegetation have impacted ecosystem and watershed functioning."
  },
  {
    "objectID": "publication/2020-12-12-agu-chacon-rmbl/index.html#abstract",
    "href": "publication/2020-12-12-agu-chacon-rmbl/index.html#abstract",
    "title": "Assessing how long-term shifts in species and functional composition have impacted carbon and water cycling the southern Rocky Mountains",
    "section": "",
    "text": "Alpine communities of the Rocky Mountains show a directional shift in temperature and vapor pressure deficit with increasingly hotter and drier growing seasons. In much of the Western and Southwestern United States, the growing season is characterized by an early drought that occurs after snowmelt and lasts until the start of the summer monsoon season. Climate change models predict an increase in the length and severity of this dry period, due to a reduction in precipitations, rise of temperatures, earlier snowmelt dates and shifts in the North American monsoon. Here we integrated several efforts to characterize ~70 years of changes in plant community diversity, functional traits, and ecosystem functioning. First, to investigate the patterns and causes of temporal changes in montane plant communities, we used a resampled dataset of 121 transects surveyed from 1948 to 1952 in the East River Basin near Crested Butte, Colorado. We combined these data, long-term climate data and another biodiversity dataset of ~ 15 years of community composition monitoring across a 1,000m elevational gradient. To understand how variability in the timing and strength of the early season drought, temperature, and VPD affects ecosystem carbon exchange through potential shifts in biodiversity and traits, we used a long-term ( ~ 17 year) dataset of peak carbon and water flux measures of vegetation along the same elevational gradient. We test the hypothesis that (i) changing climate, in particular temperature, and (ii) shifts in snowmelt date drive shifts in diversity of plant traits linked to temperature. We next assessed the hypothesis that (iii) shifts in these traits would influence ecosystem carbon and water exchange. Our results show strong shifts in species and trait composition with time and elevation. In the 17 years of monitoring ecosystem functioning we find a strong signal of increased temperatures and minimum VPD. Our results suggest that increasing VPD as well as shifts in plant trait composition together have large effects on the carbon and water budgets in Western mountain watersheds. As plant water use influences recharge rates and productivity in the watershed these results suggest that over the past ~70 years but especially in the last decade climate driven shifts in vegetation have impacted ecosystem and watershed functioning."
  },
  {
    "objectID": "talk/2021-12-14-imaging-spectroscopy/index.html",
    "href": "talk/2021-12-14-imaging-spectroscopy/index.html",
    "title": "Evaluating the effect of sampling scale to quantify different components of plant biodiversity with imaging spectroscopy",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "talk/2021-12-14-imaging-spectroscopy/index.html#abstract",
    "href": "talk/2021-12-14-imaging-spectroscopy/index.html#abstract",
    "title": "Evaluating the effect of sampling scale to quantify different components of plant biodiversity with imaging spectroscopy",
    "section": "Abstract",
    "text": "Abstract\nDeclines in biodiversity have increased the need for better monitoring and quantification of biodiversity. Remote sensing has been used to assess the extent of natural ecosystems and has become an efficient approach to quantify plant biodiversity at larger spatial scales. Imaging spectroscopy have made it possible to quantify plant diversity at high spatial resolution, and has expanded the range of detectable biochemical and physiological plant properties and allowed the quantification of functional traits. Nonetheless, it remains unclear what the most appropriate spatial resolution for remote detection of diversity is, and how the spatial resolution affects distinct diversity components. We quantified three diversity components of plant communities at three different scales in alpine meadows along the East River watershed in Colorado. Our evaluation of scale focuses on variation in the spatial grain (or pixel) rather than the extent of the study area. Specifically, we estimated spectral, functional, and phylogenetic diversity from imaging spectroscopy at the level of leaf, canopy, and from airborne imaging. Diversity for the three components–i.e., functional, phylogenetic, and spectral–was estimated using distance-based metrics. We also evaluated the relationship between diversity metrics derived from spectra and from field-based methods with the long term aim of discerning the most generalizable metrics across scales. Overall, our results in Colorado showed a strong scale-dependency on the relationship between field-based diversity indices and diversity metrics from reflectance as has been found in experimental systems. Spectral diversity calculated at the leaf-level correlates strongly with spectral diversity at the canopy level, but only weakly predicts spectral diversity calculated from airborne imagery (1 m resolution). Three diversity metrics were able to explain variation in plant responses to changes in temperature, precipitation and evapotranspiration along the elevation gradient. Our findings highlight the importance of assessing the scale dependency of diversity metrics derived from spectra to understand plant responses to environmental gradients and improve the remote detection of diversity from imaging spectroscopy."
  },
  {
    "objectID": "talk/2020-12-12-agu-chacon-rmbl/index.html",
    "href": "talk/2020-12-12-agu-chacon-rmbl/index.html",
    "title": "Assessing how long-term shifts in species and functional composition have impacted carbon and water cycling the southern Rocky Mountains",
    "section": "",
    "text": "Alpine communities of the Rocky Mountains show a directional shift in temperature and vapor pressure deficit with increasingly hotter and drier growing seasons. In much of the Western and Southwestern United States, the growing season is characterized by an early drought that occurs after snowmelt and lasts until the start of the summer monsoon season. Climate change models predict an increase in the length and severity of this dry period, due to a reduction in precipitations, rise of temperatures, earlier snowmelt dates and shifts in the North American monsoon. Here we integrated several efforts to characterize ~70 years of changes in plant community diversity, functional traits, and ecosystem functioning. First, to investigate the patterns and causes of temporal changes in montane plant communities, we used a resampled dataset of 121 transects surveyed from 1948 to 1952 in the East River Basin near Crested Butte, Colorado. We combined these data, long-term climate data and another biodiversity dataset of ~ 15 years of community composition monitoring across a 1,000m elevational gradient. To understand how variability in the timing and strength of the early season drought, temperature, and VPD affects ecosystem carbon exchange through potential shifts in biodiversity and traits, we used a long-term ( ~ 17 year) dataset of peak carbon and water flux measures of vegetation along the same elevational gradient. We test the hypothesis that (i) changing climate, in particular temperature, and (ii) shifts in snowmelt date drive shifts in diversity of plant traits linked to temperature. We next assessed the hypothesis that (iii) shifts in these traits would influence ecosystem carbon and water exchange. Our results show strong shifts in species and trait composition with time and elevation. In the 17 years of monitoring ecosystem functioning we find a strong signal of increased temperatures and minimum VPD. Our results suggest that increasing VPD as well as shifts in plant trait composition together have large effects on the carbon and water budgets in Western mountain watersheds. As plant water use influences recharge rates and productivity in the watershed these results suggest that over the past ~70 years but especially in the last decade climate driven shifts in vegetation have impacted ecosystem and watershed functioning.\n\n\n\nRMBL Gothic, CO"
  },
  {
    "objectID": "talk/2020-12-12-agu-chacon-rmbl/index.html#abstract",
    "href": "talk/2020-12-12-agu-chacon-rmbl/index.html#abstract",
    "title": "Assessing how long-term shifts in species and functional composition have impacted carbon and water cycling the southern Rocky Mountains",
    "section": "",
    "text": "Alpine communities of the Rocky Mountains show a directional shift in temperature and vapor pressure deficit with increasingly hotter and drier growing seasons. In much of the Western and Southwestern United States, the growing season is characterized by an early drought that occurs after snowmelt and lasts until the start of the summer monsoon season. Climate change models predict an increase in the length and severity of this dry period, due to a reduction in precipitations, rise of temperatures, earlier snowmelt dates and shifts in the North American monsoon. Here we integrated several efforts to characterize ~70 years of changes in plant community diversity, functional traits, and ecosystem functioning. First, to investigate the patterns and causes of temporal changes in montane plant communities, we used a resampled dataset of 121 transects surveyed from 1948 to 1952 in the East River Basin near Crested Butte, Colorado. We combined these data, long-term climate data and another biodiversity dataset of ~ 15 years of community composition monitoring across a 1,000m elevational gradient. To understand how variability in the timing and strength of the early season drought, temperature, and VPD affects ecosystem carbon exchange through potential shifts in biodiversity and traits, we used a long-term ( ~ 17 year) dataset of peak carbon and water flux measures of vegetation along the same elevational gradient. We test the hypothesis that (i) changing climate, in particular temperature, and (ii) shifts in snowmelt date drive shifts in diversity of plant traits linked to temperature. We next assessed the hypothesis that (iii) shifts in these traits would influence ecosystem carbon and water exchange. Our results show strong shifts in species and trait composition with time and elevation. In the 17 years of monitoring ecosystem functioning we find a strong signal of increased temperatures and minimum VPD. Our results suggest that increasing VPD as well as shifts in plant trait composition together have large effects on the carbon and water budgets in Western mountain watersheds. As plant water use influences recharge rates and productivity in the watershed these results suggest that over the past ~70 years but especially in the last decade climate driven shifts in vegetation have impacted ecosystem and watershed functioning.\n\n\n\nRMBL Gothic, CO"
  },
  {
    "objectID": "publication/2022-05-18-bauman-tree-mortality/index.html",
    "href": "publication/2022-05-18-bauman-tree-mortality/index.html",
    "title": "Tropical tree mortality has increased with rising atmospheric water stress",
    "section": "",
    "text": "Evidence exists that tree mortality is accelerating in some regions of the tropics1,2, with profound consequences for the future of the tropical carbon sink and the global anthropogenic carbon budget left to limit peak global warming below 2 °C. However, the mechanisms that may be driving such mortality changes and whether particular species are especially vulnerable remain unclear3,4,5,6,7,8. Here we analyse a 49-year record of tree dynamics from 24 old-growth forest plots encompassing a broad climatic gradient across the Australian moist tropics and find that annual tree mortality risk has, on average, doubled across all plots and species over the last 35 years, indicating a potential halving in life expectancy and carbon residence time. Associated losses in biomass were not offset by gains from growth and recruitment. Plots in less moist local climates presented higher average mortality risk, but local mean climate did not predict the pace of temporal increase in mortality risk. Species varied in the trajectories of their mortality risk, with the highest average risk found nearer to the upper end of the atmospheric vapour pressure deficit niches of species. A long-term increase in vapour pressure deficit was evident across the region, suggesting that thresholds involving atmospheric water stress, driven by global warming, may be a primary cause of increasing tree mortality in moist tropical forests.\n\n\n\nTemporal increase of tree mortality risk"
  },
  {
    "objectID": "publication/2022-05-18-bauman-tree-mortality/index.html#abstract",
    "href": "publication/2022-05-18-bauman-tree-mortality/index.html#abstract",
    "title": "Tropical tree mortality has increased with rising atmospheric water stress",
    "section": "",
    "text": "Evidence exists that tree mortality is accelerating in some regions of the tropics1,2, with profound consequences for the future of the tropical carbon sink and the global anthropogenic carbon budget left to limit peak global warming below 2 °C. However, the mechanisms that may be driving such mortality changes and whether particular species are especially vulnerable remain unclear3,4,5,6,7,8. Here we analyse a 49-year record of tree dynamics from 24 old-growth forest plots encompassing a broad climatic gradient across the Australian moist tropics and find that annual tree mortality risk has, on average, doubled across all plots and species over the last 35 years, indicating a potential halving in life expectancy and carbon residence time. Associated losses in biomass were not offset by gains from growth and recruitment. Plots in less moist local climates presented higher average mortality risk, but local mean climate did not predict the pace of temporal increase in mortality risk. Species varied in the trajectories of their mortality risk, with the highest average risk found nearer to the upper end of the atmospheric vapour pressure deficit niches of species. A long-term increase in vapour pressure deficit was evident across the region, suggesting that thresholds involving atmospheric water stress, driven by global warming, may be a primary cause of increasing tree mortality in moist tropical forests.\n\n\n\nTemporal increase of tree mortality risk"
  },
  {
    "objectID": "publication/2021-11-06-bauman-growth sensitivity/index.html",
    "href": "publication/2021-11-06-bauman-growth sensitivity/index.html",
    "title": "Tropical tree growth sensitivity to climate is driven by species intrinsic growth rate and leaf traits",
    "section": "",
    "text": "A better understanding of how climate affects growth in tree species is essential for improved predictions of forest dynamics under climate change. Long-term climate averages (mean climate) drive spatial variations in species’ baseline growth rates, whereas deviations from these averages over time (anomalies) can create growth variation around the local baseline. However, the rarity of long-term tree census data spanning climatic gradients has so far limited our understanding of their respective role, especially in tropical systems. Furthermore, tree growth sensitivity to climate is likely to vary widely among species, and the ecological strategies underlying these differences remain poorly understood. Here, we utilize an exceptional dataset of 49 years of growth data for 509 tree species across 23 tropical rainforest plots along a climatic gradient to examine how multiannual tree growth responds to both climate means and anomalies, and how species’ functional traits mediate these growth responses to climate. We show that anomalous increases in atmospheric evaporative demand and solar radiation consistently reduced tree growth. Drier forests and fast-growing species were more sensitive to water stress anomalies. In addition, species traits related to water use and photosynthesis partly explained differences in growth sensitivity to both climate means and anomalies. Our study demonstrates that both climate means and anomalies shape tree growth in tropical forests and that species traits can provide insights into understanding these demographic responses to climate change, offering a promising way forward to forecast tropical forest dynamics under different climate trajectories.\n\n\n\nEffect of climate"
  },
  {
    "objectID": "publication/2021-11-06-bauman-growth sensitivity/index.html#abstract",
    "href": "publication/2021-11-06-bauman-growth sensitivity/index.html#abstract",
    "title": "Tropical tree growth sensitivity to climate is driven by species intrinsic growth rate and leaf traits",
    "section": "",
    "text": "A better understanding of how climate affects growth in tree species is essential for improved predictions of forest dynamics under climate change. Long-term climate averages (mean climate) drive spatial variations in species’ baseline growth rates, whereas deviations from these averages over time (anomalies) can create growth variation around the local baseline. However, the rarity of long-term tree census data spanning climatic gradients has so far limited our understanding of their respective role, especially in tropical systems. Furthermore, tree growth sensitivity to climate is likely to vary widely among species, and the ecological strategies underlying these differences remain poorly understood. Here, we utilize an exceptional dataset of 49 years of growth data for 509 tree species across 23 tropical rainforest plots along a climatic gradient to examine how multiannual tree growth responds to both climate means and anomalies, and how species’ functional traits mediate these growth responses to climate. We show that anomalous increases in atmospheric evaporative demand and solar radiation consistently reduced tree growth. Drier forests and fast-growing species were more sensitive to water stress anomalies. In addition, species traits related to water use and photosynthesis partly explained differences in growth sensitivity to both climate means and anomalies. Our study demonstrates that both climate means and anomalies shape tree growth in tropical forests and that species traits can provide insights into understanding these demographic responses to climate change, offering a promising way forward to forecast tropical forest dynamics under different climate trajectories.\n\n\n\nEffect of climate"
  },
  {
    "objectID": "publication/2021-01-15-fadrique-bamboo/index.html",
    "href": "publication/2021-01-15-fadrique-bamboo/index.html",
    "title": "Reduced tree density and basal area in Andean forests are associated with bamboo dominance",
    "section": "",
    "text": "Forest structure and composition play an essential role in determining the carbon storage capacity of tropical forests. Andean forests, with great potential for carbon accumulation, include large expanses of high-density woody bamboo communities. Woody bamboos can potentially alter forest structure, composition and dynamics and thus can affect carbon storage capacity; however, they are commonly excluded from forest monitoring and modelling. With the aim of documenting patterns of bamboo abundance and disentangling its association with forest structure, we carried out a bamboo census in seven 1-ha long-term forest monitoring plots situated across a large elevation gradient (1000–3600 m a.s.l.) in the Peruvian Andes. We determined that bamboo is a dominant plant group in the study area. In every plot, bamboos were the most common genera in terms of number of stems, and in two of the plots bamboo species were among those with the greatest basal area. We used a combination of Generalized linear mixed models (GLMM) and structural equation modelling (SEM) to hypothesize a causal framework and determine the direction and size of the effects of bamboo abundance (basal area) on number of individual trees, total tree basal area, mean tree basal area, mean tree growth rate and tree mortality rate. We found an overall negative association between bamboo abundance and total tree basal area driven mainly by reduced tree density (directly and indirectly mediated by an increase in tree mortality). However, the decrease in tree density and the increase in tree mortality are also associated with a small increase in tree diameter (mean tree basal area). Overall, the negative association between bamboo abundance and tree basal area suggests a lower biomass accumulation and thus a lower carbon storage capacity of trees in Andean forests where bamboo is dominant. Our results, which show the importance of bamboo in determining forest function, highlight the need for including bamboo in monitoring efforts and modeling studies.\n\n\n\nStructural Equation Model (SEM)"
  },
  {
    "objectID": "publication/2021-01-15-fadrique-bamboo/index.html#abstract",
    "href": "publication/2021-01-15-fadrique-bamboo/index.html#abstract",
    "title": "Reduced tree density and basal area in Andean forests are associated with bamboo dominance",
    "section": "",
    "text": "Forest structure and composition play an essential role in determining the carbon storage capacity of tropical forests. Andean forests, with great potential for carbon accumulation, include large expanses of high-density woody bamboo communities. Woody bamboos can potentially alter forest structure, composition and dynamics and thus can affect carbon storage capacity; however, they are commonly excluded from forest monitoring and modelling. With the aim of documenting patterns of bamboo abundance and disentangling its association with forest structure, we carried out a bamboo census in seven 1-ha long-term forest monitoring plots situated across a large elevation gradient (1000–3600 m a.s.l.) in the Peruvian Andes. We determined that bamboo is a dominant plant group in the study area. In every plot, bamboos were the most common genera in terms of number of stems, and in two of the plots bamboo species were among those with the greatest basal area. We used a combination of Generalized linear mixed models (GLMM) and structural equation modelling (SEM) to hypothesize a causal framework and determine the direction and size of the effects of bamboo abundance (basal area) on number of individual trees, total tree basal area, mean tree basal area, mean tree growth rate and tree mortality rate. We found an overall negative association between bamboo abundance and total tree basal area driven mainly by reduced tree density (directly and indirectly mediated by an increase in tree mortality). However, the decrease in tree density and the increase in tree mortality are also associated with a small increase in tree diameter (mean tree basal area). Overall, the negative association between bamboo abundance and tree basal area suggests a lower biomass accumulation and thus a lower carbon storage capacity of trees in Andean forests where bamboo is dominant. Our results, which show the importance of bamboo in determining forest function, highlight the need for including bamboo in monitoring efforts and modeling studies.\n\n\n\nStructural Equation Model (SEM)"
  },
  {
    "objectID": "publication/2018-11-18-leaves-darken/index.html",
    "href": "publication/2018-11-18-leaves-darken/index.html",
    "title": "Tropical forest leaves may darken in response to climate change",
    "section": "",
    "text": "Tropical forest leaf albedo (reflectance) greatly impacts how much energy the planet absorbs; however; little is known about how it might be impacted by climate change. Here, we measure leaf traits and leaf albedo at ten 1-ha plots along a 3,200-m elevation gradient in Peru. Leaf mass per area (LMA) decreased with warmer temperatures along the elevation gradient; the distribution of LMA was positively skewed at all sites indicating a shift in LMA towards a warmer climate and future reduced tropical LMA. Reduced LMA was significantly (P &lt; 0.0001) correlated with reduced leaf near-infrared (NIR) albedo; community-weighted mean NIR albedo significantly (P &lt; 0.01) decreased as temperature increased. A potential future 2 °C increase in tropical temperatures could reduce lowland tropical leaf LMA by 6–7 g m−2 (5–6%) and reduce leaf NIR albedo by 0.0015–0.002 units. Reduced NIR albedo means that leaves are darker and absorb more of the Sun’s energy. Climate simulations indicate this increased absorbed energy will warm tropical forests more at high CO2 conditions with proportionately more energy going towards heating and less towards evapotranspiration and cloud formation.\n\n\n\nClimate simulations"
  },
  {
    "objectID": "publication/2018-11-18-leaves-darken/index.html#abstract",
    "href": "publication/2018-11-18-leaves-darken/index.html#abstract",
    "title": "Tropical forest leaves may darken in response to climate change",
    "section": "",
    "text": "Tropical forest leaf albedo (reflectance) greatly impacts how much energy the planet absorbs; however; little is known about how it might be impacted by climate change. Here, we measure leaf traits and leaf albedo at ten 1-ha plots along a 3,200-m elevation gradient in Peru. Leaf mass per area (LMA) decreased with warmer temperatures along the elevation gradient; the distribution of LMA was positively skewed at all sites indicating a shift in LMA towards a warmer climate and future reduced tropical LMA. Reduced LMA was significantly (P &lt; 0.0001) correlated with reduced leaf near-infrared (NIR) albedo; community-weighted mean NIR albedo significantly (P &lt; 0.01) decreased as temperature increased. A potential future 2 °C increase in tropical temperatures could reduce lowland tropical leaf LMA by 6–7 g m−2 (5–6%) and reduce leaf NIR albedo by 0.0015–0.002 units. Reduced NIR albedo means that leaves are darker and absorb more of the Sun’s energy. Climate simulations indicate this increased absorbed energy will warm tropical forests more at high CO2 conditions with proportionately more energy going towards heating and less towards evapotranspiration and cloud formation.\n\n\n\nClimate simulations"
  },
  {
    "objectID": "publication/2017-10-26-leaves-signatures/index.html",
    "href": "publication/2017-10-26-leaves-signatures/index.html",
    "title": "Can Leaf Spectroscopy Predict Leaf and Forest Traits Along a Peruvian Tropical Forest Elevation Gradient?",
    "section": "",
    "text": "High-resolution spectroscopy can be used to measure leaf chemical and structural traits. Such leaf traits are often highly correlated to other traits, such as photosynthesis, through the leaf economics spectrum. We measured VNIR (visible-near infrared) leaf reflectance (400–1,075 nm) of sunlit and shaded leaves in ~150 dominant species across ten, 1 ha plots along a 3,300 m elevation gradient in Peru (on 4,284 individual leaves). We used partial least squares (PLS) regression to compare leaf reflectance to chemical traits, such as nitrogen and phosphorus, structural traits, including leaf mass per area (LMA), branch wood density and leaf venation, and “higher-level” traits such as leaf photosynthetic capacity, leaf water repellency, and woody growth rates. Empirical models using leaf reflectance predicted leaf N and LMA (r2 &gt; 30% and %RMSE &lt; 30%), weakly predicted leaf venation, photosynthesis, and branch density (r2 between 10 and 35% and %RMSE between 10% and 65%), and did not predict leaf water repellency or woody growth rates (r2&lt;5%). Prediction of higher-level traits such as photosynthesis and branch density is likely due to these traits correlations with LMA, a trait readily predicted with leaf spectroscopy."
  },
  {
    "objectID": "publication/2017-10-26-leaves-signatures/index.html#abstract",
    "href": "publication/2017-10-26-leaves-signatures/index.html#abstract",
    "title": "Can Leaf Spectroscopy Predict Leaf and Forest Traits Along a Peruvian Tropical Forest Elevation Gradient?",
    "section": "",
    "text": "High-resolution spectroscopy can be used to measure leaf chemical and structural traits. Such leaf traits are often highly correlated to other traits, such as photosynthesis, through the leaf economics spectrum. We measured VNIR (visible-near infrared) leaf reflectance (400–1,075 nm) of sunlit and shaded leaves in ~150 dominant species across ten, 1 ha plots along a 3,300 m elevation gradient in Peru (on 4,284 individual leaves). We used partial least squares (PLS) regression to compare leaf reflectance to chemical traits, such as nitrogen and phosphorus, structural traits, including leaf mass per area (LMA), branch wood density and leaf venation, and “higher-level” traits such as leaf photosynthetic capacity, leaf water repellency, and woody growth rates. Empirical models using leaf reflectance predicted leaf N and LMA (r2 &gt; 30% and %RMSE &lt; 30%), weakly predicted leaf venation, photosynthesis, and branch density (r2 between 10 and 35% and %RMSE between 10% and 65%), and did not predict leaf water repellency or woody growth rates (r2&lt;5%). Prediction of higher-level traits such as photosynthesis and branch density is likely due to these traits correlations with LMA, a trait readily predicted with leaf spectroscopy."
  },
  {
    "objectID": "publication/2017-10-26-leaves-signatures/index.html#key-points",
    "href": "publication/2017-10-26-leaves-signatures/index.html#key-points",
    "title": "Can Leaf Spectroscopy Predict Leaf and Forest Traits Along a Peruvian Tropical Forest Elevation Gradient?",
    "section": "Key Points",
    "text": "Key Points\n\nEmpirical models using leaf reflectance strongly predicted leaf chemical and structural properties along a tropical elevation gradient\nThese models also predicted “higher-level” traits such as photosynthesis and branch density\nInterspecific variation in leaf reflectance properties far exceeds intraspecific variation in both sunlit and shade leaves\n\n\n\n\nLeaf reflectance"
  },
  {
    "objectID": "publication/2023-08-04-code-sharing/index.html",
    "href": "publication/2023-08-04-code-sharing/index.html",
    "title": "Code sharing increases citations, but remains uncommon",
    "section": "",
    "text": "Biologists increasingly rely on computer code, reinforcing the importance of published code for transparency, reproducibility, training, and a basis for further work. Here we conduct a literature review examining temporal trends in code sharing in ecology and evolution publications since 2010, and test for an influence of code sharing on citation rate. We find that scientists are overwhelmingly (95%) failing to publish their code and that there has been no significant improvement over time, but we also find evidence that code sharing can considerably improve citations, particularly when combined with open access publication.\n\n\n\nCode sharing trend"
  },
  {
    "objectID": "publication/2023-08-04-code-sharing/index.html#abstract",
    "href": "publication/2023-08-04-code-sharing/index.html#abstract",
    "title": "Code sharing increases citations, but remains uncommon",
    "section": "",
    "text": "Biologists increasingly rely on computer code, reinforcing the importance of published code for transparency, reproducibility, training, and a basis for further work. Here we conduct a literature review examining temporal trends in code sharing in ecology and evolution publications since 2010, and test for an influence of code sharing on citation rate. We find that scientists are overwhelmingly (95%) failing to publish their code and that there has been no significant improvement over time, but we also find evidence that code sharing can considerably improve citations, particularly when combined with open access publication.\n\n\n\nCode sharing trend"
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html",
    "href": "blog/2023-10-22-rango-abundancia/index.html",
    "title": "Curvas Rango-Abundancia",
    "section": "",
    "text": "El estudio de las comunidades vegetales es esencial para desentrañar la complejidad de los ecosistemas terrestres, contribuyendo a la comprensión de la biodiversidad, la ecología y la conservación. En esta revisión, nos sumergiremos más profundamente en el uso de la técnica de rango-abundancia como una herramienta valiosa para analizar la estructura de estas comunidades. Además de explorar sus aplicaciones, ventajas y limitaciones, presentaremos ejemplos concretos y estudios de caso para ilustrar su aplicabilidad en diferentes contextos."
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html#introducción",
    "href": "blog/2023-10-22-rango-abundancia/index.html#introducción",
    "title": "Curvas Rango-Abundancia",
    "section": "",
    "text": "El estudio de las comunidades vegetales es esencial para desentrañar la complejidad de los ecosistemas terrestres, contribuyendo a la comprensión de la biodiversidad, la ecología y la conservación. En esta revisión, nos sumergiremos más profundamente en el uso de la técnica de rango-abundancia como una herramienta valiosa para analizar la estructura de estas comunidades. Además de explorar sus aplicaciones, ventajas y limitaciones, presentaremos ejemplos concretos y estudios de caso para ilustrar su aplicabilidad en diferentes contextos."
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html#concepto-de-la-técnica-de-rango-abundancia",
    "href": "blog/2023-10-22-rango-abundancia/index.html#concepto-de-la-técnica-de-rango-abundancia",
    "title": "Curvas Rango-Abundancia",
    "section": "Concepto de la Técnica de Rango-Abundancia",
    "text": "Concepto de la Técnica de Rango-Abundancia\nLa técnica de rango-abundancia se basa en la clasificación de las especies dentro de una comunidad vegetal según su abundancia relativa en el ecosistema. En su forma más simple, las especies se ordenan según su frecuencia o abundancia, de mayor a menor, creando un gráfico de rango-abundancia. Este gráfico muestra las especies en un eje y su abundancia relativa en el otro. La forma de la curva de rango-abundancia proporciona información valiosa sobre la diversidad y estructura de la comunidad."
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html#aplicaciones-de-la-técnica-de-rango-abundancia",
    "href": "blog/2023-10-22-rango-abundancia/index.html#aplicaciones-de-la-técnica-de-rango-abundancia",
    "title": "Curvas Rango-Abundancia",
    "section": "Aplicaciones de la Técnica de Rango-Abundancia",
    "text": "Aplicaciones de la Técnica de Rango-Abundancia\n\nEvaluación de la Diversidad: La técnica de rango-abundancia permite una evaluación rápida de la diversidad de especies en una comunidad vegetal. La pendiente de la curva y su forma general pueden indicar la equitabilidad y la presencia de especies dominantes.\nEvaluación de Impacto Ambiental: Esta técnica es útil para detectar cambios en la estructura de la comunidad vegetal debido a impactos ambientales, como la degradación del suelo, el cambio climático o la invasión de especies exóticas. Por ejemplo, un estudio en [mencionar lugar] detectó cambios significativos en la curva de rango-abundancia tras un período de sequía prolongada.\nComparación de Comunidades: Se utiliza para comparar la estructura de las comunidades vegetales en diferentes lugares o a lo largo del tiempo. Esto es especialmente valioso en estudios de ecología de restauración y monitoreo a largo plazo. [Ejemplo: Un estudio en [mencionar lugar] comparó la riqueza de especies de dos áreas de restauración y encontró diferencias notables en las curvas de rango-abundancia].\nPriorización de Especies para Conservación: La identificación de especies raras pero críticas en la comunidad vegetal se simplifica con esta técnica, lo que puede ayudar a priorizar esfuerzos de conservación."
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html#ventajas-de-la-técnica-de-rango-abundancia",
    "href": "blog/2023-10-22-rango-abundancia/index.html#ventajas-de-la-técnica-de-rango-abundancia",
    "title": "Curvas Rango-Abundancia",
    "section": "Ventajas de la Técnica de Rango-Abundancia",
    "text": "Ventajas de la Técnica de Rango-Abundancia\n\nSencillez: La técnica de rango-abundancia es fácil de implementar y no requiere equipos costosos ni un tiempo de muestreo prolongado.\nSíntesis de Datos: Proporciona una visión general de la estructura de la comunidad vegetal en una sola figura, lo que facilita la comunicación de resultados a un público amplio.\nSensibilidad a Cambios: Es sensible a cambios en la abundancia relativa de especies, lo que la convierte en una herramienta valiosa para el monitoreo ambiental y la detección de perturbaciones.\n\nLimitaciones de la Técnica de Rango-Abundancia\n\nOmisión de Datos Detallados: A pesar de su utilidad, la técnica de rango-abundancia no proporciona información detallada sobre la biología, interacciones o fenología de las especies.\nFalta de Consideración de la Distribución Espacial: No tiene en cuenta la distribución espacial de las especies en la comunidad.\nSesgo por Muestreo: La calidad de los datos puede verse influenciada por la cantidad de muestreo y la selección de sitios de muestreo."
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html#conclusión",
    "href": "blog/2023-10-22-rango-abundancia/index.html#conclusión",
    "title": "Curvas Rango-Abundancia",
    "section": "Conclusión",
    "text": "Conclusión\nLa técnica de rango-abundancia es una herramienta valiosa en el estudio de comunidades vegetales. Esta revisión ha buscado ampliar la comprensión de su aplicabilidad mediante ejemplos y estudios de caso concretos. A pesar de sus limitaciones, su simplicidad y sensibilidad a los cambios la hacen relevante en la conservación y la ecología de comunidades vegetales, especialmente cuando se combina con otras técnicas y métodos para obtener una comprensión completa de estos ecosistemas."
  },
  {
    "objectID": "blog/2023-10-22-rango-abundancia/index.html#acknowledgments",
    "href": "blog/2023-10-22-rango-abundancia/index.html#acknowledgments",
    "title": "Curvas Rango-Abundancia",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nFeatured photo by Pawel Czerwinski on Unsplash."
  },
  {
    "objectID": "project/2020-10-29-nhsrtheme/index.html#ppendemic",
    "href": "project/2020-10-29-nhsrtheme/index.html#ppendemic",
    "title": "ppendemic",
    "section": "",
    "text": "Introducing a novel and updated database showcasing Peru’s endemic plants. This meticulously compiled and revised botanical collection encompasses a remarkable assemblage of over 7249 distinct species."
  },
  {
    "objectID": "project/2023-10-22-ppendemic/index.html",
    "href": "project/2023-10-22-ppendemic/index.html",
    "title": "ppendemic",
    "section": "",
    "text": "Introducing a novel and updated database showcasing Peru’s endemic plants. This meticulously compiled and revised botanical collection encompasses a remarkable assemblage of over 7249 distinct species."
  },
  {
    "objectID": "project/2023-10-22-ppendemic/index.html#ppendemic",
    "href": "project/2023-10-22-ppendemic/index.html#ppendemic",
    "title": "ppendemic",
    "section": "",
    "text": "Introducing a novel and updated database showcasing Peru’s endemic plants. This meticulously compiled and revised botanical collection encompasses a remarkable assemblage of over 7249 distinct species."
  }
]